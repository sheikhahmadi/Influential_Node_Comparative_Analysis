{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fc304b-5d9a-4ba8-96a5-7925a88ac7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kneed\n",
      "  Downloading kneed-0.8.5-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from kneed) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.14.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from kneed) (1.26.4)\n",
      "Installing collected packages: kneed\n",
      "Successfully installed kneed-0.8.5\n"
     ]
    }
   ],
   "source": [
    "!pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2966d-eab7-44bf-a6ca-ebaafe35617d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: facebook_withSIR.xlsx\n",
      "Processing beta column: Beta_0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import kendalltau\n",
    "from kneed import KneeLocator\n",
    "\n",
    "# --- Step 1: Load Data ---\n",
    "def load_graph_from_excel(file_path):\n",
    "    \"\"\"Loads graph from an Excel file, removes self-loops, and filters nodes to those in SIR sheet.\"\"\"\n",
    "    df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(zip(df[\"source_column\"], df[\"target_column\"]))\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    return G\n",
    "\n",
    "# --- Step 2: Feature Computation ---\n",
    "def compute_connectivity_vector(G, node, node_to_index):\n",
    "    \"\"\"Returns the connectivity vector for a given node.\"\"\"\n",
    "    connectivity_vector = np.zeros(len(G.nodes()))\n",
    "    for neighbor in G.neighbors(node):\n",
    "        if neighbor in node_to_index:\n",
    "            connectivity_vector[node_to_index[neighbor]] = 1\n",
    "    return connectivity_vector\n",
    "\n",
    "def compute_degree_vector(G):\n",
    "    \"\"\"Returns the degree vector for all nodes.\"\"\"\n",
    "    return np.array([G.degree[node] for node in G.nodes()])\n",
    "\n",
    "def compute_coreness_vector(G):\n",
    "    \"\"\"Computes the extended coreness score for all nodes.\"\"\"\n",
    "    coreness = nx.core_number(G)\n",
    "    eks = {node: coreness[node] * G.degree[node] + sum(coreness[n] * G.degree[n] for n in G.neighbors(node))\n",
    "           for node in G.nodes()}\n",
    "    return np.array([eks[node] for node in G.nodes()])\n",
    "\n",
    "def compute_feature_vector(G, alpha1=1.0, alpha2=3.0):\n",
    "    \"\"\"Computes feature vectors for all nodes.\"\"\"\n",
    "    node_to_index = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "    degree_vector = compute_degree_vector(G)\n",
    "    coreness_vector = compute_coreness_vector(G)\n",
    "    feature_vectors = {}\n",
    "    for node in G.nodes():\n",
    "        connectivity_vector = compute_connectivity_vector(G, node, node_to_index)\n",
    "        feature_vector = connectivity_vector * (alpha1 * degree_vector + alpha2 * coreness_vector)\n",
    "        feature_vectors[node] = feature_vector\n",
    "    return feature_vectors\n",
    "\n",
    "# --- Step 3: Optimal Clusters ---\n",
    "def optimal_k(features):\n",
    "    distortions = []\n",
    "    K_range = range(1, min(10, len(features)))\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(features)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    knee = KneeLocator(K_range, distortions, curve=\"convex\", direction=\"decreasing\")\n",
    "    return knee.elbow if knee.elbow else 3\n",
    "\n",
    "# --- Step 4: Clustering & Sample Selection ---\n",
    "def cluster_nodes(feature_vectors):\n",
    "    feature_matrix = np.array(list(feature_vectors.values()))\n",
    "    k = optimal_k(feature_matrix)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(feature_matrix)\n",
    "    clusters = {i: [] for i in range(k)}\n",
    "    for node, label in zip(feature_vectors.keys(), kmeans.labels_):\n",
    "        clusters[label].append(node)\n",
    "    return clusters, k\n",
    "\n",
    "def select_training_samples(clusters, s, beta_values):\n",
    "    \"\"\"\n",
    "    For each cluster, sample nodes that are present in beta_values.\n",
    "    This ensures that the returned sampled_x and sampled_y have equal length.\n",
    "    \"\"\"\n",
    "    sampled_x, sampled_y = [], []\n",
    "    total_nodes = sum(len(cluster) for cluster in clusters.values())\n",
    "    sample_size_per_cluster = max(1, int(s * total_nodes / len(clusters)))\n",
    "    for nodes in clusters.values():\n",
    "        valid_nodes = [node for node in nodes if node in beta_values.index]\n",
    "        if not valid_nodes:\n",
    "            continue\n",
    "        sampled_nodes = np.random.choice(valid_nodes, min(sample_size_per_cluster, len(valid_nodes)), replace=False)\n",
    "        sampled_x.extend(sampled_nodes)\n",
    "        sampled_y.extend(beta_values.loc[sampled_nodes].values)\n",
    "    return sampled_x, sampled_y\n",
    "\n",
    "# --- Step 5: Train SVR Model ---\n",
    "def train_svr(X_train, y_train):\n",
    "    svr = SVR(kernel=\"rbf\", gamma=\"scale\")\n",
    "    svr.fit(X_train, y_train)\n",
    "    return svr\n",
    "\n",
    "# --- Step 6: Prediction & Evaluation ---\n",
    "def predict_vitality(G, svr, feature_vectors, alpha=0.25):\n",
    "    EML = {}\n",
    "    for node in G.nodes():\n",
    "        predicted = svr.predict([feature_vectors[node]])[0]\n",
    "        neighbor_sum = sum(svr.predict([feature_vectors[neighbor]])[0] for neighbor in G.neighbors(node))\n",
    "        EML[node] = predicted + alpha * neighbor_sum\n",
    "    return sorted(EML.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def measure_execution_time(method, G):\n",
    "    start_time = time.time()\n",
    "    ranked_nodes = method(G)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return ranked_nodes, execution_time\n",
    "\n",
    "def compute_monotonicity(G, ranked_nodes):\n",
    "    ranks = [score for node, score in ranked_nodes]\n",
    "    unique_ranks = list(set(ranks))\n",
    "    n = G.number_of_nodes()\n",
    "    nr_dict = {rank: ranks.count(rank) for rank in unique_ranks}\n",
    "    nr_sum = sum(nr * (nr - 1) for nr in nr_dict.values())\n",
    "    return (1 - nr_sum / (n * (n - 1))) ** 2\n",
    "\n",
    "def compute_rbo(sigma, R, alpha=0.9):\n",
    "    \"\"\"\n",
    "    Compute the Rank-Biased Overlap (RBO) between two rankings sigma and R.\n",
    "    \"\"\"\n",
    "    def A(sigma, R, f):\n",
    "        sigma_f = set(sigma[:f])\n",
    "        R_f = set(R[:f])\n",
    "        union = sigma_f | R_f\n",
    "        return len(sigma_f & R_f) / len(union) if union else 1\n",
    "    n = max(len(sigma), len(R))\n",
    "    return (1 - alpha) * sum(alpha**(f - 1) * A(sigma, R, f) for f in range(1, n + 1))\n",
    "\n",
    "def compute_spread_impact(G, ranked_nodes, spread_power, f_values):\n",
    "    si_scores = {}\n",
    "    n = len(G.nodes())\n",
    "    for f in f_values:\n",
    "        top_f_nodes = [node for node, _ in ranked_nodes[:int(f * n)]]\n",
    "        # Use get() to safely retrieve spread_power value (defaulting to 0 if missing)\n",
    "        si_scores[f'SI {f:.2f}'] = sum(spread_power.get(node, 0) for node in top_f_nodes) / (f * n)\n",
    "    return si_scores\n",
    "\n",
    "def compute_mrr(ground_truth, ranked_list, top_ratios):\n",
    "    mrr_scores = {}\n",
    "    num_nodes = len(ground_truth)\n",
    "    for ratio in top_ratios:\n",
    "        k = max(1, int(ratio * num_nodes))\n",
    "        reciprocal_ranks = []\n",
    "        for node in ground_truth[:k]:\n",
    "            if node in ranked_list:\n",
    "                rank = ranked_list.index(node) + 1\n",
    "                reciprocal_ranks.append(1 / rank)\n",
    "        mrr_scores[f'MRR {ratio:.2f}'] = sum(reciprocal_ranks) / k if reciprocal_ranks else 0\n",
    "    return mrr_scores\n",
    "\n",
    "def compute_jaccard_similarity(set1, set2):\n",
    "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0\n",
    "\n",
    "# --- Main Execution for a Single File ---\n",
    "def main(file_path, beta_cols):\n",
    "    print(f\"\\nProcessing dataset: {os.path.basename(file_path)}\")\n",
    "    G = load_graph_from_excel(file_path)\n",
    "    feature_vectors = compute_feature_vector(G)\n",
    "    sir_data = pd.read_excel(file_path, sheet_name=\"SIR\").set_index(\"Node\")\n",
    "    # Convert sir_data index to same type as graph nodes\n",
    "    node_type = type(next(iter(G.nodes())))\n",
    "    sir_data.index = sir_data.index.astype(node_type)\n",
    "    # Optionally, filter graph to nodes in SIR data\n",
    "    valid_nodes = set(sir_data.index)\n",
    "    G = G.subgraph(valid_nodes).copy()\n",
    "\n",
    "    # Evaluation parameters.\n",
    "    k_factors = [0.01, 0.03, 0.05, 0.07, 0.085, 0.10]\n",
    "    f_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10]\n",
    "    top_ratios = k_factors  # For MRR computation.\n",
    "\n",
    "    dataset_results = []\n",
    "    dataset_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    num_nodes = len(G.nodes())\n",
    "\n",
    "    for beta_col in beta_cols:\n",
    "        print(f\"Processing beta column: {beta_col}\")\n",
    "        beta_values = sir_data[beta_col]\n",
    "\n",
    "        clusters, k = cluster_nodes(feature_vectors)\n",
    "        sampled_x, sampled_y = select_training_samples(clusters, 0.30, beta_values)\n",
    "\n",
    "        X_train = np.array([feature_vectors[node] for node in sampled_x])\n",
    "        y_train = np.array(sampled_y)\n",
    "        if len(X_train) != len(y_train):\n",
    "            print(f\"Warning: X_train has {len(X_train)} samples but y_train has {len(y_train)} samples. Skipping beta {beta_col}.\")\n",
    "            continue\n",
    "\n",
    "        svr = train_svr(X_train, y_train)\n",
    "        ranked_nodes, exec_time = measure_execution_time(lambda G: predict_vitality(G, svr, feature_vectors), G)\n",
    "\n",
    "        monotonicity = compute_monotonicity(G, ranked_nodes)\n",
    "\n",
    "        # Ground truth ranking (sorted nodes by spread power in descending order).\n",
    "        true_sir_ranking = beta_values.sort_values(ascending=False).index.tolist()\n",
    "        predicted_ranking = [node for node, _ in ranked_nodes]\n",
    "\n",
    "        common_nodes = set(true_sir_ranking) & set(predicted_ranking)\n",
    "        filtered_sigma = [node for node in true_sir_ranking if node in common_nodes]\n",
    "        filtered_predicted = [node for node in predicted_ranking if node in common_nodes]\n",
    "\n",
    "        true_rank_dict = {node: rank for rank, node in enumerate(filtered_sigma)}\n",
    "        predicted_rank_list = [true_rank_dict[node] for node in filtered_predicted]\n",
    "        kendall_tau, p_value = kendalltau(predicted_rank_list, list(range(len(predicted_rank_list))))\n",
    "\n",
    "        jaccard_scores = {}\n",
    "        for k_factor in k_factors:\n",
    "            k_val = max(1, int(k_factor * num_nodes))\n",
    "            top_sigma = set(filtered_sigma[:k_val])\n",
    "            top_predicted = set(filtered_predicted[:k_val])\n",
    "            jaccard_scores[f'Jaccard k={k_val}'] = compute_jaccard_similarity(top_sigma, top_predicted)\n",
    "\n",
    "        spread_power = beta_values.to_dict()\n",
    "        si_scores = compute_spread_impact(G, ranked_nodes, spread_power, f_values)\n",
    "\n",
    "        mrr_scores = compute_mrr(filtered_sigma, filtered_predicted, top_ratios)\n",
    "\n",
    "        # Compute RBO scores using the compute_rbo function.\n",
    "        rbo_scores = {}\n",
    "        for f in f_values:\n",
    "            top = max(1, int(f * len(filtered_sigma)))\n",
    "            sigma_top = filtered_sigma[:top]\n",
    "            predicted_top = filtered_predicted[:top]\n",
    "            rbo_scores[f'RBO {f:.2f}'] = compute_rbo(sigma_top, predicted_top)\n",
    "\n",
    "        result_row = [\n",
    "            dataset_name,\n",
    "            'EML_SVR',\n",
    "            exec_time,\n",
    "            monotonicity,\n",
    "            beta_col,\n",
    "            kendall_tau,\n",
    "            p_value\n",
    "        ]\n",
    "        result_row += [jaccard_scores[f'Jaccard k={max(1, int(k_factor * num_nodes))}'] for k_factor in k_factors]\n",
    "        result_row += [si_scores[f'SI {f:.2f}'] for f in f_values]\n",
    "        result_row += [rbo_scores[f'RBO {f:.2f}'] for f in f_values]\n",
    "        result_row += [mrr_scores[f'MRR {k_factor:.2f}'] for k_factor in top_ratios]\n",
    "\n",
    "        dataset_results.append(result_row)\n",
    "\n",
    "    columns = (['Dataset', 'Method', 'Execution Time', 'Monotonicity', 'Beta', 'Kendall Tau', 'P-Value'] +\n",
    "               [f'Jaccard k={max(1, int(k_factor * num_nodes))}' for k_factor in k_factors] +\n",
    "               [f'SI {f:.2f}' for f in f_values] +\n",
    "               [f'RBO {f:.2f}' for f in f_values] +\n",
    "               [f'MRR {k_factor:.2f}' for k_factor in top_ratios])\n",
    "\n",
    "    result_df = pd.DataFrame(dataset_results, columns=columns)\n",
    "    output_file = f'{dataset_name}_results.xlsx'\n",
    "    result_df.to_excel(output_file, index=False)\n",
    "    print(f\"Processed {dataset_name}, results saved in {output_file}\")\n",
    "\n",
    "# --- Process All Excel Files in the Dataset Folder ---\n",
    "def process_dataset_folder(folder_path, beta_cols):\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.xlsx'):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                main(file_path, beta_cols)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# --- Run the Program ---\n",
    "dataset_folder = 'dataset'\n",
    "beta_columns = ['Beta_0', 'Beta_1', 'Beta_2', 'Beta_3']\n",
    "process_dataset_folder(dataset_folder, beta_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550963c2-aab4-4704-8fc8-adb84bbbf366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
