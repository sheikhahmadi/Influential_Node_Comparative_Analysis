{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfc301-d4a3-49dc-9a57-ecba4fd8dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def sir(G, s, maxitr, beta, gamma):\n",
    "    sum_sp = 0  # sum of all recovered nodes in all iterations\n",
    "    \n",
    "    neighbor_sets = {node: set(G[node]) for node in G.nodes()}\n",
    "    for i in range(maxitr):\n",
    "        infected = set(s)\n",
    "        recovered = set()\n",
    "        while infected:\n",
    "            new_infected = set()\n",
    "            for node in infected:\n",
    "                # Retrieve precomputed neighbor set\n",
    "                neighbors = neighbor_sets[node]\n",
    "                for neighbor in neighbors:\n",
    "                    if random.random() < beta:\n",
    "                        if neighbor not in infected and neighbor not in recovered:\n",
    "                            new_infected.add(neighbor)\n",
    "                if random.random() < gamma:\n",
    "                    recovered.add(node)\n",
    "            infected |= new_infected\n",
    "            infected -= recovered\n",
    "        sum_sp += len(recovered)\n",
    "    \n",
    "    avg_sp = sum_sp / maxitr\n",
    "    return avg_sp\n",
    "\n",
    "def measure_execution_time(method, G):\n",
    "    start_time = time.time()\n",
    "    result = method(G)\n",
    "    ranked_nodes = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return ranked_nodes, execution_time\n",
    "\n",
    "def compute_monotonicity(G, ranked_nodes):\n",
    "    ranks = [score for node, score in ranked_nodes]\n",
    "    unique_ranks = list(set(ranks))\n",
    "    n = G.number_of_nodes()\n",
    "    nr_dict = {rank: ranks.count(rank) for rank in unique_ranks}\n",
    "    nr_sum = sum(nr * (nr - 1) for nr in nr_dict.values())\n",
    "    M = (1 - nr_sum / (n * (n - 1))) ** 2\n",
    "    return M\n",
    "\n",
    "def compute_rbo(sigma, R, alpha=0.9):\n",
    "    \"\"\"Compute the Rank-Biased Overlap (RBO) between two ranking lists.\"\"\"\n",
    "    def A(sigma, R, f):\n",
    "        sigma_f = set(sigma[:f])\n",
    "        R_f = set(R[:f])\n",
    "        if len(sigma_f) == 0 and len(R_f) == 0:\n",
    "            return 1\n",
    "        return len(sigma_f & R_f) / len(sigma_f | R_f)\n",
    "\n",
    "    n = max(len(sigma), len(R))\n",
    "    rbo = (1 - alpha) * sum(alpha**(f - 1) * A(sigma, R, f) for f in range(1, n + 1))\n",
    "    return rbo\n",
    "\n",
    "\n",
    "# Define methods\n",
    "def method1(G):\n",
    "    return nx.degree_centrality(G)\n",
    "\n",
    "def method2(G):\n",
    "    return nx.betweenness_centrality(G)\n",
    "\n",
    "def method3(G):\n",
    "    return nx.closeness_centrality(G)\n",
    "\n",
    "def method4(G):\n",
    "    return nx.core_number(G)\n",
    "\n",
    "def method5(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = {vi: sum(ks[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc\n",
    "\n",
    "def method6(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = method5(G)\n",
    "    cnc_plus = {vi: sum(cnc[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc_plus\n",
    "\n",
    "def method7(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_max_set = {node for node, k in ks.items() if k == ks_max}\n",
    "    teta = {}\n",
    "    for vi in G.nodes():\n",
    "        teta_vi = (ks_max - ks[vi] + 1)\n",
    "        total_dis = 0\n",
    "        for vj in ks_max_set:\n",
    "            if nx.has_path(G, vi, vj):\n",
    "                total_dis += nx.shortest_path_length(G, vi, vj)\n",
    "        teta[vi] = teta_vi * total_dis\n",
    "    return teta\n",
    "\n",
    "def method8(G):\n",
    "    ks = nx.core_number(G)\n",
    "    deg = nx.degree_centrality(G)\n",
    "    bet = nx.betweenness_centrality(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_norm = {node: ks[node] / ks_max for node in G.nodes()}\n",
    "    all_around = {v: math.sqrt(ks_norm[v]**2 + bet[v]**2 + deg[v]**2) for v in G.nodes()}\n",
    "    return all_around\n",
    "\n",
    "def h_index_centrality(G):\n",
    "    h_index = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degrees = sorted([G.degree(neighbor) for neighbor in neighbors], reverse=True)\n",
    "        h = 0\n",
    "        for i, degree in enumerate(degrees):\n",
    "            if degree >= i + 1:\n",
    "                h = i + 1\n",
    "            else:\n",
    "                break\n",
    "        h_index[node] = h\n",
    "    return h_index\n",
    "\n",
    "def method10(G):\n",
    "    HI = h_index_centrality(G)\n",
    "    LHI = {vi: HI[vi] + sum(HI[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return LHI\n",
    "\n",
    "def method11(G):\n",
    "    nv = {}\n",
    "    for vi in G.nodes():\n",
    "        set1 = set()\n",
    "        for vj in G.neighbors(vi):\n",
    "            set1.add(vj)\n",
    "            for vk in G.neighbors(vj):\n",
    "                set1.add(vk)\n",
    "        nv[vi] = len(set1) - 1\n",
    "    \n",
    "    c_local = {}\n",
    "    for vi in G.nodes():\n",
    "        total1 = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            total2 = 0\n",
    "            for vk in G.neighbors(vj):\n",
    "                total2 += nv[vk]\n",
    "            total1 += total2\n",
    "        c_local[vi] = total1\n",
    "    return c_local\n",
    "\n",
    "def method12(G):\n",
    "    SLC = {}\n",
    "    k = dict(nx.degree(G))\n",
    "    C = nx.clustering(G)\n",
    "    for i in G.nodes():\n",
    "        SLC[i] = sum(C[j] for j in G.neighbors(i))\n",
    "    \n",
    "    DNC = {}\n",
    "    alpha = 1\n",
    "    for i in G.nodes():\n",
    "        DNC[i] = k[i] + alpha * SLC[i]\n",
    "    return DNC\n",
    "\n",
    "def method13(G):\n",
    "    CLC = nx.clustering(G)\n",
    "    CLGC = {}\n",
    "    for i in G.nodes():\n",
    "        CLGC[i] = sum(\n",
    "            math.sqrt(CLC[j]) / nx.shortest_path_length(G, i, j)\n",
    "            for j in G.nodes()\n",
    "            if i != j and nx.has_path(G, i, j)\n",
    "        ) * CLC[i]\n",
    "    \n",
    "    ECLGC = {}\n",
    "    for v in G.nodes():\n",
    "        sum1 = sum2 = sum3 = 0\n",
    "        neighbors_v = list(G.neighbors(v))\n",
    "        for u in G.nodes():\n",
    "            if u != v and nx.has_path(G, v, u):\n",
    "                neighbors_u = list(G.neighbors(u))\n",
    "                lu = len(neighbors_u)\n",
    "                sum1 += CLC[u] / len(neighbors_v)\n",
    "                sum2 = sum(CLC[j] / lu for j in neighbors_u)\n",
    "                sum3 += math.sqrt(CLC[u] + sum2) / nx.shortest_path_length(G, v, u)\n",
    "        ECLGC[v] = sum1 * sum3\n",
    "    return ECLGC\n",
    "\n",
    "def compute_alpha(graph):\n",
    "    size_of_V = graph.number_of_nodes()\n",
    "    order_of_magnitude = int(math.log10(size_of_V))\n",
    "    alpha = size_of_V / order_of_magnitude\n",
    "    return alpha\n",
    "\n",
    "def method14(G, m=1):\n",
    "    alpha = compute_alpha(G)\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    kshell = nx.core_number(G)\n",
    "    \n",
    "    centrality = {}\n",
    "    \n",
    "    for i in G.nodes:\n",
    "        cenC_i = 0\n",
    "        \n",
    "        # Get m-neighborhood of node i\n",
    "        m_neighborhood = set(nx.single_source_shortest_path_length(G, i, cutoff=m).keys())\n",
    "        m_neighborhood.remove(i)  # Exclude the node itself\n",
    "        \n",
    "        for j in m_neighborhood:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij == 0:\n",
    "                continue\n",
    "            DC_i = degree_centrality[i]\n",
    "            C_i = clustering_coeff[i]\n",
    "            k_s_i = kshell[i]\n",
    "            k_s_j = kshell[j]\n",
    "            \n",
    "            term1 = (math.exp(alpha) * DC_i + (1 / C_i if C_i != 0 else 0))\n",
    "            term2 = (k_s_j / (abs(k_s_i - k_s_j) + 1))\n",
    "            \n",
    "            cenC_i += (4 * math.pi**2) * (term1 / (d_ij**2)) * term2\n",
    "        \n",
    "        centrality[i] = cenC_i\n",
    "    \n",
    "    return centrality\n",
    "\n",
    "def method15(G):\n",
    "    clustering_coeffs = nx.clustering(G)\n",
    "    cluster_rank_scores = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    degrees = dict(G.degree())\n",
    "\n",
    "    # Calculate ClusterRank scores\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(degrees[neighbor] + 1 for neighbor in neighbors)\n",
    "        cluster_rank_scores[node] = 10 ** (-clustering_coeffs[node]) * sum_term\n",
    "\n",
    "    return cluster_rank_scores\n",
    "\n",
    "# Optimized methods\n",
    "def get_neighborhood(G, node, radius):\n",
    "    neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "    neighborhood.pop(node, None)\n",
    "    return list(neighborhood.keys())\n",
    "\n",
    "# Improved Gravity Centrality\n",
    "def method16(G):\n",
    "    IGC = {}\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        IGC[i] = 0\n",
    "        for j in G.neighbors(i):\n",
    "            neighborhood = get_neighborhood(G, j, 2)\n",
    "            for p in neighborhood:\n",
    "                IGC[i] += (ks[j] * k[p]) / (nx.shortest_path_length(G, j, p)**2)\n",
    "    return IGC\n",
    "\n",
    "def k_shell(graph):\n",
    "    return nx.core_number(graph)\n",
    "\n",
    "def evaluate_power_k_shell(k_si, k_sj):\n",
    "    return math.sqrt(k_si + k_sj)\n",
    "\n",
    "def compute_P_ji(graph, node_i, node_j):\n",
    "    try:\n",
    "        a_ji = graph[node_j][node_i]['weight'] if 'weight' in graph[node_j][node_i] else 1\n",
    "        total_a_ji = sum(graph[node_j][nbr]['weight'] if 'weight' in graph[node_j][nbr] else 1 for nbr in graph.neighbors(node_j))\n",
    "        return a_ji / total_a_ji\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def evaluate_ED(graph, node_i, node_j):\n",
    "    P_ji = compute_P_ji(graph, node_i, node_j)\n",
    "    if P_ji == 0:\n",
    "        return 1  # Prevent log(0)\n",
    "    return max(1 - math.log(P_ji), 1)\n",
    "\n",
    "def evaluate_influence(graph, node_i, node_j, k_shell_values):\n",
    "    alpha_ij = evaluate_power_k_shell(k_shell_values[node_i], k_shell_values[node_j])\n",
    "    ED_ij = evaluate_ED(graph, node_i, node_j)\n",
    "    degree_vi = graph.degree(node_i)\n",
    "    return alpha_ij * degree_vi / ED_ij\n",
    "\n",
    "# EDBC\n",
    "def method17(G):\n",
    "    k_shell_values = k_shell(G)\n",
    "    EDBC = {u: 0 for u in G.nodes()}\n",
    "    \n",
    "    for u in G.nodes():\n",
    "        for v in G.neighbors(u):\n",
    "            for neighbor_u in G.neighbors(u):\n",
    "                EDBC[v] += evaluate_influence(G, v, neighbor_u, k_shell_values)\n",
    "    \n",
    "    return EDBC\n",
    "\n",
    "def method18(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    entropy_centrality = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if len(neighbors) == 0:\n",
    "            entropy_centrality[node] = 0\n",
    "        else:\n",
    "            sum_degrees = sum(G.degree(neighbor) for neighbor in neighbors)\n",
    "            sum_centrality = sum(degree_centrality[neighbor] for neighbor in neighbors)\n",
    "            if sum_centrality == 0:\n",
    "                entropy_centrality[node] = 0\n",
    "            else:\n",
    "                entropy_centrality[node] = closeness_centrality[node] * (sum_degrees / sum_centrality)\n",
    "    \n",
    "    return entropy_centrality\n",
    "\n",
    "# KNC\n",
    "def method19(G):\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    degree_values = dict(G.degree())\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    \n",
    "    alpha = compute_alpha(G)\n",
    "    KNC = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(k_shell_values[neighbor] * degree_values[neighbor] * clustering_coeff[neighbor] for neighbor in neighbors)\n",
    "        KNC[node] = alpha * sum_term\n",
    "    \n",
    "    return KNC\n",
    "\n",
    "# NEDC\n",
    "def method20(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    \n",
    "    nodes = G.nodes()\n",
    "    NEDC = {}\n",
    "    \n",
    "    for node in nodes:\n",
    "        term1 = degree_centrality[node] * betweenness_centrality[node]\n",
    "        term2 = closeness_centrality[node] * k_shell_values[node]\n",
    "        NEDC[node] = term1 + term2\n",
    "    \n",
    "    return NEDC\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "# Method 21 (LGC)\n",
    "def method21(G):\n",
    "    LC = {}\n",
    "    K = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        LC[i] = sum(K[j] for j in nx.neighbors(G, i)) * 2 + K[i] ** 2 + K[i]\n",
    "\n",
    "    LGC = {}\n",
    "    radius = 3\n",
    "    for vi in G.nodes():\n",
    "        LGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            try:\n",
    "                d_ij = nx.shortest_path_length(G, vi, vj)\n",
    "                if d_ij != 0:\n",
    "                    LGC[vi] += LC[vi] * LC[vj] / d_ij ** 2\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    return LGC\n",
    "\n",
    "# Method 22 (SEGM)\n",
    "def method22(G):\n",
    "    k = dict(G.degree())\n",
    "    I = dict()\n",
    "    E = dict()\n",
    "    SE = dict()\n",
    "\n",
    "    for node in G.nodes:\n",
    "        I[node] = 0\n",
    "\n",
    "    for i in G.nodes:\n",
    "        sum_kj = 0\n",
    "        for j in G.neighbors(i):\n",
    "            sum_kj += k[j]\n",
    "        if sum_kj != 0:\n",
    "            I[i] = k[i] / sum_kj\n",
    "        sum_e = 0\n",
    "        for j in G.neighbors(i):\n",
    "            if I[j] != 0:\n",
    "                sum_e += I[j] * np.log(I[j])\n",
    "        E[i] = -1 * sum_e\n",
    "        SE[i] = np.exp(E[i]) * k[i]\n",
    "\n",
    "    SEGM = dict()\n",
    "    R = 3\n",
    "    for i in G.nodes:\n",
    "        SEGM[i] = 0\n",
    "        neighbors = get_neighborhood(G, i, R)\n",
    "        for j in neighbors:\n",
    "            if nx.shortest_path_length(G, i, j) != 0:\n",
    "                SEGM[i] += (SE[i] * SE[j]) / (nx.shortest_path_length(G, i, j) ** 2)\n",
    "\n",
    "    return SEGM\n",
    "\n",
    "# Method 23 (MCGM)\n",
    "def method23(G):\n",
    "    k = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    x = nx.eigenvector_centrality(G)\n",
    "\n",
    "    k_values = np.array(list(k.values()))\n",
    "    ks_values = np.array(list(ks.values()))\n",
    "    x_values = np.array(list(x.values()))\n",
    "\n",
    "    k_mid = np.median(k_values)\n",
    "    ks_mid = np.median(ks_values)\n",
    "    x_mid = np.median(x_values)\n",
    "\n",
    "    k_max = max(k_values)\n",
    "    ks_max = max(ks_values)\n",
    "    x_max = max(x_values)\n",
    "\n",
    "    alpha = max(k_mid / k_max, x_mid / x_max) / (ks_mid / ks_max)\n",
    "\n",
    "    MCGM = dict()\n",
    "    R = 3\n",
    "\n",
    "    for i in G.nodes():\n",
    "        MCGM[i] = 0\n",
    "        for j in G.nodes():\n",
    "            if i != j and nx.shortest_path_length(G, source=i, target=j) <= R:\n",
    "                MCGM[i] += ((k[i] / k_max + alpha * ks[i] / ks_max + x[i] / x_max) *\n",
    "                            (k[j] / k_max + alpha * ks[j] / ks_max + x[j] / x_max) /\n",
    "                            (nx.shortest_path_length(G, source=i, target=j) ** 2))\n",
    "\n",
    "    return MCGM\n",
    "\n",
    "# Method 24 (HVGC)\n",
    "def compute_H_index(G, i):\n",
    "    degrees = [G.degree(neighbor) for neighbor in G.neighbors(i)]\n",
    "    degrees.sort(reverse=True)\n",
    "    H_index = 0\n",
    "    for idx, degree in enumerate(degrees):\n",
    "        if degree >= idx + 1:\n",
    "            H_index = idx + 1\n",
    "        else:\n",
    "            break\n",
    "    return H_index\n",
    "\n",
    "def compute_H_v(G, i):\n",
    "    H_i = compute_H_index(G, i)\n",
    "    H_v_i = sum(G.degree(j) for j in G.neighbors(i) if G.degree(j) >= H_i)\n",
    "    return H_v_i\n",
    "\n",
    "def compute_c_i(G, i):\n",
    "    def compute_p_ij(G, i, j):\n",
    "        neighbors_i = list(G.neighbors(i))\n",
    "        sum_z_iw = sum(G[i][w].get('weight', 1) for w in neighbors_i)\n",
    "        p_ij = G[i][j].get('weight', 1) / sum_z_iw if sum_z_iw != 0 else 0\n",
    "        return p_ij\n",
    "\n",
    "    c_i = 0\n",
    "    neighbors_i = list(G.neighbors(i))\n",
    "    for j in neighbors_i:\n",
    "        p_ij = compute_p_ij(G, i, j)\n",
    "        neighbors_j = list(G.neighbors(j))\n",
    "        common_neighbors = set(neighbors_i).intersection(neighbors_j)\n",
    "        sum_piw_pwj = sum(compute_p_ij(G, i, w) * compute_p_ij(G, w, j) for w in common_neighbors)\n",
    "        term = p_ij + sum_piw_pwj\n",
    "        c_i += term ** 2\n",
    "    \n",
    "    return c_i\n",
    "\n",
    "def compute_HVGC(G, i, H_v, R):\n",
    "    c_i = compute_c_i(G, i)\n",
    "    HVGC_i = 0\n",
    "    for j in G.nodes():\n",
    "        if j != i:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij <= R:\n",
    "                HVGC_i += (math.exp(-c_i) * H_v[i] * H_v[j]) / (d_ij ** 2)\n",
    "    return HVGC_i\n",
    "\n",
    "#HVGC\n",
    "def method24(G):\n",
    "    H_v = {i: compute_H_v(G, i) for i in G.nodes()}\n",
    "    R = 2\n",
    "    HVGC = {node: compute_HVGC(G, node, H_v, R) for node in G.nodes()}\n",
    "    return HVGC\n",
    "\n",
    "# Method 25 (MDD)\n",
    "def method25(G):\n",
    "    def compute_mixed_degrees(H, landa):\n",
    "        km = {}\n",
    "        for v in H.nodes():\n",
    "            kr = H.degree(v)\n",
    "            ke = G.degree(v) - kr\n",
    "            km[v] = kr + landa * ke\n",
    "        return km\n",
    "\n",
    "    H = G.copy()\n",
    "    landa = 0.7\n",
    "    rank = {}\n",
    "    km = {v: 1 for v in G.nodes()}  # Initialize km values\n",
    "\n",
    "    while H.number_of_nodes() > 0:\n",
    "        km = compute_mixed_degrees(H, landa)\n",
    "        min_km_value = min(km.values())\n",
    "\n",
    "        # Find and remove nodes with mixed degree <= min_km_value\n",
    "        node_set = [v for v in H.nodes() if km[v] == min_km_value]\n",
    "        for each in node_set:\n",
    "            rank[each] = km[each]\n",
    "            H.remove_node(each)\n",
    "\n",
    "    return rank\n",
    "\n",
    "# Method 26 (BaseGM)\n",
    "def method26(G):\n",
    "    def get_neighborhood(G, node, radius):\n",
    "        neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "        neighborhood.pop(node, None)\n",
    "        return list(neighborhood.keys())\n",
    "\n",
    "    gravity = dict()\n",
    "    kshell = nx.core_number(G)\n",
    "    for vi in G.nodes():\n",
    "        gravity[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, 3)\n",
    "        for vj in neighbors:\n",
    "            gravity[vi] += ((kshell[vi] * kshell[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    gravity_plus = dict()\n",
    "    for vi in G.nodes():\n",
    "        gravity_plus[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            gravity_plus[vi] += gravity[vj]\n",
    "\n",
    "    return gravity_plus\n",
    "\n",
    "# Method 27 (GlobalGM)\n",
    "def method27(G):\n",
    "    gm = dict()\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        gm[vi] = 0\n",
    "        for vj in G.nodes():\n",
    "            if vj != vi:\n",
    "                gm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gm\n",
    "\n",
    "# Method 28 (LocalGM)\n",
    "def method28(G):\n",
    "    lgm = dict()\n",
    "    radius = 3\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        lgm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                lgm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return lgm\n",
    "\n",
    "# Method 29 (WGravity)\n",
    "def method29(G):\n",
    "    gmm = dict()\n",
    "    ev = nx.eigenvector_centrality(G)\n",
    "    k = dict(nx.degree(G))\n",
    "\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        gmm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                gmm[vi] += (ev[vi] * (k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gmm\n",
    "\n",
    "\n",
    "# Method 30 (HKS)\n",
    "def method30(G):\n",
    "    def set_b_values(G):\n",
    "        b, Shell, b_values = 1, 1, {}\n",
    "        while G.number_of_nodes() > 0:\n",
    "            flag = False\n",
    "            for v in list(G.nodes):\n",
    "                if G.degree[v] <= Shell:\n",
    "                    b_values[v] = b\n",
    "                    flag = True\n",
    "            \n",
    "            if flag:\n",
    "                G.remove_nodes_from([v for v in list(G.nodes) if b_values.get(v) == b])\n",
    "                b += 1\n",
    "            else:\n",
    "                Shell += 1\n",
    "        \n",
    "        return b_values\n",
    "\n",
    "    def set_f(G, b_values):\n",
    "        V, f = list(G.nodes), max(b_values.values())\n",
    "        fi = {v: b_values[v] if all(b_values[v] >= b_values[vj] for vj in G.neighbors(v)) else 0 for v in V}\n",
    "\n",
    "        while V:\n",
    "            for vi in V:\n",
    "                if fi[vi] == f:\n",
    "                    for vj in G.neighbors(vi):\n",
    "                        if fi[vi] - 1 > fi[vj]:\n",
    "                            fi[vj] = fi[vi] - 1\n",
    "            V.remove(vi)\n",
    "\n",
    "        return fi\n",
    "\n",
    "    b_values = set_b_values(G.copy())\n",
    "    fi_values = set_f(G.copy(), b_values)\n",
    "\n",
    "    S = {vi: sum(G.degree[vj] * (b_values[vj] + fi_values[vj]) for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    HKS = {vi: sum(S[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return HKS\n",
    "\n",
    "# Method 31 (SHKS)\n",
    "def method31(G):\n",
    "    def compute_efficiency(G):\n",
    "        n = len(G)\n",
    "        if n <= 1:\n",
    "            return 0\n",
    "        inv_distances = (1 / d for u, v_dict in nx.shortest_path_length(G) for v, d in v_dict.items() if d > 0)\n",
    "        return sum(inv_distances) / (n * (n - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_copy = G.copy()\n",
    "            G_copy.remove_node(k)\n",
    "            centrality[k] = (efficiency_G - compute_efficiency(G_copy)) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    def network_constrained_coefficient(G, v):\n",
    "        neighbors = list(G.neighbors(v))\n",
    "        coefficient = sum(calculate_pvu(G, v, u) + sum((calculate_pvu(G, v, w) * calculate_pvu(G, w, u)) ** 2 for w in set(G.neighbors(u)) & set(G.neighbors(v))) for u in neighbors)\n",
    "        return coefficient\n",
    "\n",
    "    def calculate_pvu(G, v, u):\n",
    "        return (1 if u in G.neighbors(v) else 0) / G.degree(v)\n",
    "\n",
    "    efficiency_centrality = compute_efficiency_centrality(G)\n",
    "    Constraint_Coef = {node: network_constrained_coefficient(G, node) for node in G.nodes()}\n",
    "    sh = {node: 1 / Constraint_Coef[node] for node in G.nodes()}\n",
    "    ks = nx.core_number(G)\n",
    "    alpha = 0.2\n",
    "    I = {node: alpha * sh[node] + ks[node] for node in G.nodes()}\n",
    "    C = {v: sum(I[v] + I[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    IS = {v: sum(C[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    SHKS = {v: sum(IS[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    return SHKS\n",
    "\n",
    "# Method 32 (KSGC)\n",
    "def method32(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_min = min(ks.values())\n",
    "    k = dict(G.degree())\n",
    "    n = G.number_of_nodes()\n",
    "    C = np.zeros((n, n))\n",
    "    F = np.zeros((n, n))\n",
    "    nodes = list(G.nodes())\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                vi, vj = nodes[i], nodes[j]\n",
    "                try:\n",
    "                    C[i, j] = np.exp((ks[vi] - ks[vj]) / (ks_max - ks_min))\n",
    "                    F[i, j] = C[i, j] * (k[vi] * k[vj] / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    F[i, j] = 0\n",
    "\n",
    "    KSGC = dict()\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        vi_index = node_index[vi]\n",
    "        KSGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            vj_index = node_index[vj]\n",
    "            KSGC[vi] += F[vi_index, vj_index]\n",
    "    \n",
    "    return KSGC\n",
    "\n",
    "# Method 33 (EFFC)\n",
    "def method33(G):\n",
    "    def compute_efficiency(G):\n",
    "        N = len(G.nodes)\n",
    "        if N < 2:\n",
    "            return 0\n",
    "        efficiency_sum = 0\n",
    "        for i in G.nodes:\n",
    "            for j in G.nodes:\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        shortest_path_length = nx.shortest_path_length(G, source=i, target=j)\n",
    "                        efficiency_sum += 1 / shortest_path_length\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        efficiency_sum += 0  # Adding 0 for disconnected pairs\n",
    "        return efficiency_sum / (N * (N - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_k_prime = G.copy()\n",
    "            G_k_prime.remove_node(k)\n",
    "            efficiency_G_k_prime = compute_efficiency(G_k_prime)\n",
    "            centrality[k] = (efficiency_G - efficiency_G_k_prime) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    return compute_efficiency_centrality(G)\n",
    "\n",
    "# Method 34 (Local Relative ASP)\n",
    "def method34(G):\n",
    "    def find_diameter(G):\n",
    "        \"\"\"\n",
    "        Find the diameter (longest shortest path) of the graph G.\n",
    "        \"\"\"\n",
    "        if nx.is_connected(G):\n",
    "            return nx.diameter(G)\n",
    "        else:\n",
    "            lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "            diameter = 0\n",
    "            for u in lengths:\n",
    "                for v in lengths[u]:\n",
    "                    if lengths[u][v] > diameter:\n",
    "                        diameter = lengths[u][v]\n",
    "            return diameter\n",
    "\n",
    "    def calculate_asp(G):\n",
    "        \"\"\"\n",
    "        Calculate the Average Shortest Path (ASP) for the graph G.\n",
    "        \"\"\"\n",
    "        n = len(G.nodes)\n",
    "        if n < 2:\n",
    "            return 0\n",
    "        \n",
    "        if nx.is_connected(G):\n",
    "            return nx.average_shortest_path_length(G)\n",
    "        \n",
    "        diameter = find_diameter(G)\n",
    "        asp_sum = 0\n",
    "        \n",
    "        lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "        for u in G.nodes:\n",
    "            for v in G.nodes:\n",
    "                if u != v:\n",
    "                    asp_sum += lengths[u].get(v, diameter)\n",
    "        \n",
    "        return asp_sum / (n * (n - 1))\n",
    "\n",
    "    asp = calculate_asp(G)\n",
    "    AC = {}\n",
    "\n",
    "    for k in G.nodes():\n",
    "        G_removed = G.copy()\n",
    "        G_removed.remove_node(k)\n",
    "        asp_removed = calculate_asp(G_removed)\n",
    "        AC[k] = abs(asp_removed - asp) / asp if asp > 0 else 0\n",
    "\n",
    "    return AC\n",
    "\n",
    "def k_neighbors(G, node, radius):\n",
    "        neighbors = set(nx.single_source_shortest_path_length(G, node, cutoff=radius).keys())\n",
    "        neighbors.discard(node)  # Remove the node itself if present\n",
    "        return neighbors\n",
    "    \n",
    "# Method 35 (InformationRank)\n",
    "def method35(G):\n",
    "    \n",
    "    PROPA = {}\n",
    "    Score = {}\n",
    "    L = 2\n",
    "    Miu = 0.2\n",
    "\n",
    "    # Initialize PROPA and Score dictionaries\n",
    "    for v in G.nodes():\n",
    "        PROPA[v] = {}\n",
    "        Score[v] = 0\n",
    "\n",
    "    # Calculate PROPA and Score\n",
    "    for v in G.nodes():\n",
    "        neighbors = k_neighbors(G, v, L)\n",
    "        for w in neighbors:\n",
    "            PROPA[v][w] = 1\n",
    "            path_length_counts = {}\n",
    "            for path in nx.all_simple_paths(G, v, w, cutoff=L):\n",
    "                length = len(path) - 1\n",
    "                path_length_counts[length] = path_length_counts.get(length, 0) + 1\n",
    "\n",
    "            for l in range(1, L + 1):\n",
    "                pow_val = path_length_counts.get(l, 0)\n",
    "                PROPA[v][w] *= (1 - Miu ** l) ** pow_val\n",
    "            PROPA[v][w] = 1 - PROPA[v][w]\n",
    "            Score[v] += PROPA[v][w]\n",
    "\n",
    "    return Score\n",
    "\n",
    "\n",
    "# Method 36 (Weighted K-shell)\n",
    "def method36(G):\n",
    "    c1 = 0.1\n",
    "    c2 = 0.4\n",
    "    deg = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    ksdw = dict()\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksdw[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            ksdw[vi] += (c1 * deg[vi] + c2 * ks[vi]) * (c1 * deg[vj] + c2 * ks[vj])\n",
    "\n",
    "    return ksdw\n",
    "\n",
    "# Method 37 (HybridKshell)\n",
    "def method37(G):\n",
    "    radius = 2\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(G.degree())\n",
    "    ksh = dict()\n",
    "    landa = 0.4\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksh[vi] = 0\n",
    "        neighbors = k_neighbors(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            ksh[vi] += (math.sqrt(ks[vi] + ks[vj]) + landa * k[vj]) / (nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    return ksh\n",
    "\n",
    "# Optimized method 38 (Social Capital)\n",
    "def method38(G):\n",
    "    SC = {}\n",
    "    K = dict(G.degree())\n",
    "    for i in G.nodes:\n",
    "        SC[i] = K[i] + sum(K[j] for j in G.neighbors(i))\n",
    "    return SC\n",
    "\n",
    "# Optimized method 39 (Potential Edge Weight)\n",
    "def method39(G):\n",
    "    KW = {}\n",
    "    k = dict(G.degree())\n",
    "    Landa = 0.5\n",
    "\n",
    "    for vi in G.nodes:\n",
    "        KW[vi] = Landa * k[vi] + sum((1 - Landa) * (k[vi] + k[vj]) for vj in G.neighbors(vi))\n",
    "        KW[vi] = int(KW[vi])\n",
    "\n",
    "    def find_and_remove_nodes(H, ks):\n",
    "        nodes_to_remove = [node for node in list(H.nodes) if KW[node] <= ks]\n",
    "        H.remove_nodes_from(nodes_to_remove)\n",
    "        return nodes_to_remove\n",
    "\n",
    "    H = G.copy()\n",
    "    ks = min(KW.values())\n",
    "    kshell = {}\n",
    "    tmp = []\n",
    "\n",
    "    while H.nodes:\n",
    "        nodes_to_remove = find_and_remove_nodes(H, ks)\n",
    "        if not nodes_to_remove:\n",
    "            if tmp:\n",
    "                kshell[ks] = tmp\n",
    "            ks += 1\n",
    "            tmp = []\n",
    "        else:\n",
    "            tmp.extend(nodes_to_remove)\n",
    "        if not H.nodes:\n",
    "            kshell[ks] = tmp\n",
    "            break\n",
    "\n",
    "    weightedks = {}\n",
    "    wks = 1\n",
    "    for ks, value in kshell.items():\n",
    "        if value:\n",
    "            weightedks[wks] = value\n",
    "            wks += 1\n",
    "            \n",
    "    output_wks = {}\n",
    "    for weight, nodes in weightedks.items():\n",
    "        for node in nodes:\n",
    "            output_wks[node] = weight\n",
    "\n",
    "    return output_wks\n",
    "\n",
    "# method calculate effg centrality\n",
    "def method40(G):\n",
    "    def calculate_effective_distance(G):\n",
    "        degree = dict(G.degree())\n",
    "        \n",
    "        # Step 1: Calculate the probability P n|m\n",
    "        probability = {(m, n): (1 / degree[m]) if G.has_edge(m, n) else 0 \n",
    "                       for m in G.nodes for n in G.nodes if m != n}\n",
    "\n",
    "        # Step 2: Calculate the effective distance D_{n|m} for directly connected nodes\n",
    "        effective_distance = {(m, n): 1 - np.log2(p) if p > 0 else float('inf') \n",
    "                              for (m, n), p in probability.items()}\n",
    "\n",
    "        # Step 3: Calculate the effective distance for indirectly connected nodes using the shortest path\n",
    "        all_pairs_shortest_path_length = dict(nx.all_pairs_dijkstra_path_length(G))\n",
    "        for m in G.nodes:\n",
    "            for n in G.nodes:\n",
    "                if m != n:\n",
    "                    shortest_path_length = all_pairs_shortest_path_length[m].get(n, float('inf'))\n",
    "                    if shortest_path_length != float('inf'):\n",
    "                        effective_distance[(m, n)] = shortest_path_length\n",
    "\n",
    "        return effective_distance\n",
    "\n",
    "    def calculate_interaction_scores(G, effective_distance):\n",
    "        interaction_scores = {}\n",
    "        for (i, j), d in effective_distance.items():\n",
    "            k_i = G.degree[i]\n",
    "            k_j = G.degree[j]\n",
    "            interaction_scores[(i, j)] = (k_i * k_j) / (d ** 2)\n",
    "        return interaction_scores\n",
    "\n",
    "    def compute_effg_centrality(G, interaction_scores):\n",
    "        effg_centrality = {node: 0 for node in G.nodes()}\n",
    "        for (i, j), score in interaction_scores.items():\n",
    "            effg_centrality[i] += score\n",
    "            effg_centrality[j] += score\n",
    "        return effg_centrality\n",
    "\n",
    "    # Calculate Effective Distance\n",
    "    effective_distance = calculate_effective_distance(G)\n",
    "\n",
    "    # Calculate Interaction Scores\n",
    "    interaction_scores = calculate_interaction_scores(G, effective_distance)\n",
    "\n",
    "    # Calculate EffG Centrality\n",
    "    effg_centrality = compute_effg_centrality(G, interaction_scores)\n",
    "    \n",
    "    return effg_centrality\n",
    "\n",
    "# method 41 (IS-PEW)\n",
    "def method41(G):\n",
    "    import networkx as nx\n",
    "    import numpy as np\n",
    "\n",
    "    def compute_TP(G, v_A):\n",
    "        # Compute NTC(v_A)\n",
    "        NTC_v_A = nx.triangles(G, v_A)\n",
    "        # Compute TC\n",
    "        TC = sum(nx.triangles(G).values()) // 3\n",
    "        return NTC_v_A / TC if TC != 0 else 0\n",
    "\n",
    "    def compute_InfE(G, v_A, v_B):\n",
    "        common_neighbors = list(nx.common_neighbors(G, v_A, v_B))\n",
    "        DG_v_A = G.degree[v_A]\n",
    "        DG_v_B = G.degree[v_B]\n",
    "        sum_DG_k = sum(G.degree[k] for k in common_neighbors)\n",
    "        return (DG_v_A * DG_v_B) / (1 + sum_DG_k)\n",
    "\n",
    "    def compute_NIP(G, v_A):\n",
    "        neighbors = list(G.neighbors(v_A))\n",
    "        DG_v_a = [G.degree[v_a] for v_a in neighbors]\n",
    "        KS_v_a = [nx.core_number(G)[v_a] for v_a in neighbors]\n",
    "        return sum(np.sqrt(DG * KS) for DG, KS in zip(DG_v_a, KS_v_a))\n",
    "\n",
    "    def compute_EW(G, v_A, v_B):\n",
    "        TP_v_A = compute_TP(G, v_A)\n",
    "        TP_v_B = compute_TP(G, v_B)\n",
    "        KS_v_A = nx.core_number(G)[v_A]\n",
    "        KS_v_B = nx.core_number(G)[v_B]\n",
    "        NIP_v_A = compute_NIP(G, v_A)\n",
    "        NIP_v_B = compute_NIP(G, v_B)\n",
    "        ED_v_A_v_B = 1 / compute_InfE(G, v_A, v_B)\n",
    "        return ((KS_v_A * (1 + TP_v_A) * NIP_v_A) / ED_v_A_v_B) + ((KS_v_B * (1 + TP_v_B) * NIP_v_B) / ED_v_A_v_B)\n",
    "\n",
    "    def compute_IS(G, v_A):\n",
    "        neighbors = list(G.neighbors(v_A))\n",
    "        EW_values = [compute_EW(G, v_A, v_B) for v_B in neighbors]\n",
    "        return sum(EW_values)\n",
    "\n",
    "    # Main Function\n",
    "    def influential_spreaders(G):\n",
    "        # Step 1: Compute potential edge weight for each edge\n",
    "        for u, v in G.edges():\n",
    "            G[u][v]['weight'] = compute_EW(G, u, v)\n",
    "\n",
    "        # Step 2: Identify influential spreaders\n",
    "        IS_values = {v: compute_IS(G, v) for v in G.nodes()}\n",
    "        return IS_values\n",
    "\n",
    "    return influential_spreaders(G)\n",
    "\n",
    "# k-shell iteration Factor (KS-IF)\n",
    "def method42(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    iteration_factors = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        if iteration_nodes:\n",
    "            m = iteration_nodes[-1][1]\n",
    "            for node, n in iteration_nodes:\n",
    "                iteration_factors[node] = k * (1 + n / m)\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    ks_IF = {}\n",
    "    deg = dict(G.degree())\n",
    "    for vi in G.nodes:\n",
    "        ks_IF[vi] = iteration_factors[vi] * deg[vi]\n",
    "        for vj in nx.neighbors(G, vi):\n",
    "            ks_IF[vi] += iteration_factors[vj] * deg[vj]\n",
    "\n",
    "    return ks_IF\n",
    "\n",
    "\n",
    "# Method 43 (DKGM)\n",
    "def method43(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    k_star = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        m = iteration_nodes[-1][1]\n",
    "        for node, n in iteration_nodes:\n",
    "            k_star[node] = k_shell[node] + (n / (m + 1))\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    DK = {}\n",
    "    DKGM = {}\n",
    "\n",
    "    for node in k_shell:\n",
    "        DK[node] = G.degree[node] + k_star[node]\n",
    "\n",
    "    R = 2  # Define the radius for DKGM computation\n",
    "    for node in G.nodes():\n",
    "        DKGM[node] = 0\n",
    "        for neighbor in nx.single_source_shortest_path_length(G, node, cutoff=R):\n",
    "            if node != neighbor:\n",
    "                try:\n",
    "                    d_ij = nx.shortest_path_length(G, source=node, target=neighbor)\n",
    "                    DKGM[node] += DK[node] * DK[neighbor] / d_ij**2\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "    return DKGM\n",
    "\n",
    "\n",
    "\n",
    "methods = [globals()[f'method{i}'] for i in range(1, 44)]\n",
    "\n",
    "#methods = {\n",
    "#    \"Degree Centrality\": method1,\n",
    "#    \"Betweenness Centrality\": method2,\n",
    "#    \"Closeness Centrality\": method3,\n",
    "#    \"K-shell\": method4,\n",
    "#    \"Cnc\": method5,\n",
    "#    \"Cnc Plus\": method6,\n",
    "#    \"Distance to Network Core\": method7,\n",
    "#    \"All Around Node\": method8,\n",
    "#    \"H-index Centrality\": method9,\n",
    "#    \"Local H-index\": method10,\n",
    "#    \"Semi-Local Centrality\": method11,\n",
    "#    \"DNC\": method12,\n",
    "#    \"ECLGC\": method13,\n",
    "#    \"Centripetal Centrality\": method14,\n",
    "#    \"Cluster Rank\": method15,\n",
    "#    \"Improved Gravity Centrality\":method16,\n",
    "#    \"EDBC\" : method17,\n",
    "#    \"Entropy Centrality\": method18,\n",
    "#    \"KNC\" : method19,\n",
    "#    \"NEDC\" : method20,\n",
    "#    \"LGC\" : method21,\n",
    "#    \"SEGM\" : method22,\n",
    "#    \"MCGM\" : method23,\n",
    "#    \"HVGC\" : method24,\n",
    "#    \"MDD\" : method25,\n",
    "#    \"BaseGM\" : method26,\n",
    "#    \"GlobalGM\" : method27,\n",
    "#    \"LocalGM\" : method28,\n",
    "#    \"WGravity\" : method29,\n",
    "#    \"HKS\" : method30,\n",
    "#    \"SHKS\" : method31,\n",
    "#    \"KSGC\" : method32,\n",
    "#    \"EFFC\" : method33,\n",
    "#    \"Local Relative ASP\" : method34,\n",
    "#    \"Information Rank\" : method35,\n",
    "#    \"Weighted K-shell\" : method36,\n",
    "#    \"Hybrid K-shell\" : method37,\n",
    "#    \"Social Capital\" : method38,\n",
    "#    \"Potential Edge Weight\" : method39,\n",
    "#    \"effg Gravity\" : method40,\n",
    "#    \"IS-PEW\" : method41,\n",
    "#    \"KS-IF\" : method42,\n",
    "#    \"DKGM\" : method43\n",
    "# }\n",
    "\n",
    "\n",
    "# Main Code\n",
    "dataset_folder = 'dataset'\n",
    "beta_values = [0.02, 0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "gamma = 1  # Adjust gamma as needed\n",
    "alpha_values = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for dataset_filename in os.listdir(dataset_folder):\n",
    "    if dataset_filename.endswith('.csv'):\n",
    "        dataset_path = os.path.join(dataset_folder, dataset_filename)\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        G = nx.from_pandas_edgelist(df, source='source_column', target='target_column')\n",
    "\n",
    "        # Remove self-loops\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "        for beta in beta_values:\n",
    "            spread_power = {}\n",
    "            for v in G.nodes():\n",
    "                seed = [v]\n",
    "                spread_power[v] = sir(G, seed, 1000, beta, gamma)\n",
    "\n",
    "            results = {}\n",
    "            sigma = [node for node, _ in sorted(spread_power.items(), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "            for i, method in enumerate(methods, start=1):\n",
    "                method_name = f'method{i}'\n",
    "                try:\n",
    "                    ranked_nodes, exec_time = measure_execution_time(method, G)\n",
    "                    monotonicity = compute_monotonicity(G, ranked_nodes)\n",
    "                    sir_ranks = sorted(spread_power.items(), key=lambda x: x[1], reverse=True)\n",
    "                    tau, p_value = kendalltau(sir_ranks, ranked_nodes)\n",
    "                    R = [node for node, _ in ranked_nodes]\n",
    "\n",
    "                    rbo_scores = {f'RBO {alpha}': compute_rbo(sigma, R, alpha) for alpha in alpha_values}\n",
    "                    \n",
    "                    results[method_name] = {\n",
    "                        'ranked_nodes': ranked_nodes,\n",
    "                        'execution_time': exec_time,\n",
    "                        'monotonicity': monotonicity,\n",
    "                        f'Beta {beta}': beta,\n",
    "                        f'Kendall Tau {beta}': tau,\n",
    "                        f'P-Value {beta}': p_value,\n",
    "                        **rbo_scores\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    results[method_name] = {\n",
    "                        'ranked_nodes': [],\n",
    "                        'execution_time': None,\n",
    "                        'monotonicity': None,\n",
    "                        f'Beta {beta}': beta,\n",
    "                        f'Kendall Tau {beta}': None,\n",
    "                        f'P-Value {beta}': None,\n",
    "                        **{f'RBO {alpha}': None for alpha in alpha_values},\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "\n",
    "            # Create DataFrame for results\n",
    "            data = []\n",
    "            for method_name, result in results.items():\n",
    "                if isinstance(result['ranked_nodes'], list) and all(isinstance(item, tuple) and len(item) == 2 for item in result['ranked_nodes']):\n",
    "                    top_5_nodes = []\n",
    "                    for node, score in result['ranked_nodes'][:5]:\n",
    "                        try:\n",
    "                            if isinstance(score, (int, float)):  # Ensure score is a number\n",
    "                                top_5_nodes.append(f\"{node}:{score:.4f}\")\n",
    "                            else:\n",
    "                                top_5_nodes.append(f\"{node}:N/A\")\n",
    "                                print(f\"Score for node {node} is not a number: {score}\")\n",
    "                        except (ValueError, TypeError) as e:\n",
    "                            top_5_nodes.append(f\"{node}:N/A\")\n",
    "                            print(f\"Error formatting score for node {node}: {e}\")\n",
    "                else:\n",
    "                    top_5_nodes = ['N/A'] * 5\n",
    "\n",
    "                row = [\n",
    "                    method_name, \n",
    "                    result['execution_time'], \n",
    "                    result.get('monotonicity', 'N/A'), \n",
    "                    result.get(f'Beta {beta}'), \n",
    "                    result.get(f'Kendall Tau {beta}'), \n",
    "                    result.get(f'P-Value {beta}')\n",
    "                ] + [result.get(f'RBO {alpha}') for alpha in alpha_values] + top_5_nodes\n",
    "                if 'error' in result:\n",
    "                    row.append(result['error'])\n",
    "                else:\n",
    "                    row.append('')  # Ensure the row has the same number of columns\n",
    "                data.append(row)\n",
    "\n",
    "            # Ensure each row has the correct number of columns\n",
    "            expected_columns = ['Method', 'Execution Time', 'Monotonicity', f'Beta {beta}', f'Kendall Tau {beta}', f'P-Value {beta}'] + [f'RBO {alpha}' for alpha in alpha_values] + [f'Top {i+1}' for i in range(5)] + ['Error']\n",
    "            for row in data:\n",
    "                if len(row) < len(expected_columns):\n",
    "                    row.extend([''] * (len(expected_columns) - len(row)))\n",
    "\n",
    "            result_df = pd.DataFrame(data, columns=expected_columns)\n",
    "\n",
    "            # Write DataFrame to Excel file for each beta and dataset\n",
    "            dataset_name = os.path.splitext(dataset_filename)[0]\n",
    "            output_file = f'{dataset_name}_results_beta_{beta}.xlsx'\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                result_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "\n",
    "                # Save SIR spread power to Excel file\n",
    "                spread_df = pd.DataFrame(list(spread_power.items()), columns=['Node', 'Spread Power'])\n",
    "                spread_df.to_excel(writer, sheet_name=f'SIR_{beta}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417ac10-7624-4a7d-95b5-a81679b8e8fa",
   "metadata": {},
   "source": [
    "# Updated Code to read SIR data from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7fd7e-46e0-4731-8e20-2f4268c0d2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def measure_execution_time(method, G):\n",
    "    start_time = time.time()\n",
    "    result = method(G)\n",
    "    ranked_nodes = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return ranked_nodes, execution_time\n",
    "\n",
    "def compute_monotonicity(G, ranked_nodes):\n",
    "    ranks = [score for node, score in ranked_nodes]\n",
    "    unique_ranks = list(set(ranks))\n",
    "    n = G.number_of_nodes()\n",
    "    nr_dict = {rank: ranks.count(rank) for rank in unique_ranks}\n",
    "    nr_sum = sum(nr * (nr - 1) for nr in nr_dict.values())\n",
    "    M = (1 - nr_sum / (n * (n - 1))) ** 2\n",
    "    return M\n",
    "\n",
    "def compute_rbo(sigma, R, alpha=0.9):\n",
    "    \"\"\"Compute the Rank-Biased Overlap (RBO) between two ranking lists.\"\"\"\n",
    "    def A(sigma, R, f):\n",
    "        sigma_f = set(sigma[:f])\n",
    "        R_f = set(R[:f])\n",
    "        if len(sigma_f) == 0 and len(R_f) == 0:\n",
    "            return 1\n",
    "        return len(sigma_f & R_f) / len(sigma_f | R_f)\n",
    "\n",
    "    n = max(len(sigma), len(R))\n",
    "    rbo = (1 - alpha) * sum(alpha**(f - 1) * A(sigma, R, f) for f in range(1, n + 1))\n",
    "    return rbo\n",
    "\n",
    "\n",
    "# Define methods\n",
    "def method1(G):\n",
    "    return nx.degree_centrality(G)\n",
    "\n",
    "def method2(G):\n",
    "    return nx.betweenness_centrality(G)\n",
    "\n",
    "def method3(G):\n",
    "    return nx.closeness_centrality(G)\n",
    "\n",
    "def method4(G):\n",
    "    return nx.core_number(G)\n",
    "\n",
    "def method5(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = {vi: sum(ks[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc\n",
    "\n",
    "def method6(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = method5(G)\n",
    "    cnc_plus = {vi: sum(cnc[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc_plus\n",
    "\n",
    "def method7(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_max_set = {node for node, k in ks.items() if k == ks_max}\n",
    "    teta = {}\n",
    "    for vi in G.nodes():\n",
    "        teta_vi = (ks_max - ks[vi] + 1)\n",
    "        total_dis = 0\n",
    "        for vj in ks_max_set:\n",
    "            if nx.has_path(G, vi, vj):\n",
    "                total_dis += nx.shortest_path_length(G, vi, vj)\n",
    "        teta[vi] = teta_vi * total_dis\n",
    "    return teta\n",
    "\n",
    "def method8(G):\n",
    "    ks = nx.core_number(G)\n",
    "    deg = nx.degree_centrality(G)\n",
    "    bet = nx.betweenness_centrality(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_norm = {node: ks[node] / ks_max for node in G.nodes()}\n",
    "    all_around = {v: math.sqrt(ks_norm[v]**2 + bet[v]**2 + deg[v]**2) for v in G.nodes()}\n",
    "    return all_around\n",
    "\n",
    "def method9(G):\n",
    "    h_index = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degrees = sorted([G.degree(neighbor) for neighbor in neighbors], reverse=True)\n",
    "        h = 0\n",
    "        for i, degree in enumerate(degrees):\n",
    "            if degree >= i + 1:\n",
    "                h = i + 1\n",
    "            else:\n",
    "                break\n",
    "        h_index[node] = h\n",
    "    return h_index\n",
    "\n",
    "def method10(G):\n",
    "    HI = method9(G)\n",
    "    LHI = {vi: HI[vi] + sum(HI[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return LHI\n",
    "\n",
    "def method11(G):\n",
    "    nv = {}\n",
    "    for vi in G.nodes():\n",
    "        set1 = set()\n",
    "        for vj in G.neighbors(vi):\n",
    "            set1.add(vj)\n",
    "            for vk in G.neighbors(vj):\n",
    "                set1.add(vk)\n",
    "        nv[vi] = len(set1) - 1\n",
    "    \n",
    "    c_local = {}\n",
    "    for vi in G.nodes():\n",
    "        total1 = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            total2 = 0\n",
    "            for vk in G.neighbors(vj):\n",
    "                total2 += nv[vk]\n",
    "            total1 += total2\n",
    "        c_local[vi] = total1\n",
    "    return c_local\n",
    "\n",
    "def method12(G):\n",
    "    SLC = {}\n",
    "    k = dict(nx.degree(G))\n",
    "    C = nx.clustering(G)\n",
    "    for i in G.nodes():\n",
    "        SLC[i] = sum(C[j] for j in G.neighbors(i))\n",
    "    \n",
    "    DNC = {}\n",
    "    alpha = 1\n",
    "    for i in G.nodes():\n",
    "        DNC[i] = k[i] + alpha * SLC[i]\n",
    "    return DNC\n",
    "\n",
    "def method13(G):\n",
    "    CLC = nx.clustering(G)\n",
    "    CLGC = {}\n",
    "    for i in G.nodes():\n",
    "        CLGC[i] = sum(\n",
    "            math.sqrt(CLC[j]) / nx.shortest_path_length(G, i, j)\n",
    "            for j in G.nodes()\n",
    "            if i != j and nx.has_path(G, i, j)\n",
    "        ) * CLC[i]\n",
    "    \n",
    "    ECLGC = {}\n",
    "    for v in G.nodes():\n",
    "        sum1 = sum2 = sum3 = 0\n",
    "        neighbors_v = list(G.neighbors(v))\n",
    "        for u in G.nodes():\n",
    "            if u != v and nx.has_path(G, v, u):\n",
    "                neighbors_u = list(G.neighbors(u))\n",
    "                lu = len(neighbors_u)\n",
    "                sum1 += CLC[u] / len(neighbors_v)\n",
    "                sum2 = sum(CLC[j] / lu for j in neighbors_u)\n",
    "                sum3 += math.sqrt(CLC[u] + sum2) / nx.shortest_path_length(G, v, u)\n",
    "        ECLGC[v] = sum1 * sum3\n",
    "    return ECLGC\n",
    "\n",
    "def compute_alpha(graph):\n",
    "    size_of_V = graph.number_of_nodes()\n",
    "    order_of_magnitude = int(math.log10(size_of_V))\n",
    "    alpha = size_of_V / order_of_magnitude\n",
    "    return alpha\n",
    "\n",
    "def method14(G, m=1):\n",
    "    alpha = compute_alpha(G)\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    kshell = nx.core_number(G)\n",
    "    \n",
    "    centrality = {}\n",
    "    \n",
    "    for i in G.nodes:\n",
    "        cenC_i = 0\n",
    "        \n",
    "        # Get m-neighborhood of node i\n",
    "        m_neighborhood = set(nx.single_source_shortest_path_length(G, i, cutoff=m).keys())\n",
    "        m_neighborhood.remove(i)  # Exclude the node itself\n",
    "        \n",
    "        for j in m_neighborhood:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij == 0:\n",
    "                continue\n",
    "            DC_i = degree_centrality[i]\n",
    "            C_i = clustering_coeff[i]\n",
    "            k_s_i = kshell[i]\n",
    "            k_s_j = kshell[j]\n",
    "            \n",
    "            term1 = (math.exp(alpha) * DC_i + (1 / C_i if C_i != 0 else 0))\n",
    "            term2 = (k_s_j / (abs(k_s_i - k_s_j) + 1))\n",
    "            \n",
    "            cenC_i += (4 * math.pi**2) * (term1 / (d_ij**2)) * term2\n",
    "        \n",
    "        centrality[i] = cenC_i\n",
    "    \n",
    "    return centrality\n",
    "\n",
    "def method15(G):\n",
    "    clustering_coeffs = nx.clustering(G)\n",
    "    cluster_rank_scores = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    degrees = dict(G.degree())\n",
    "\n",
    "    # Calculate ClusterRank scores\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(degrees[neighbor] + 1 for neighbor in neighbors)\n",
    "        cluster_rank_scores[node] = 10 ** (-clustering_coeffs[node]) * sum_term\n",
    "\n",
    "    return cluster_rank_scores\n",
    "\n",
    "# Optimized methods\n",
    "def get_neighborhood(G, node, radius):\n",
    "    neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "    neighborhood.pop(node, None)\n",
    "    return list(neighborhood.keys())\n",
    "\n",
    "# Improved Gravity Centrality\n",
    "def method16(G):\n",
    "    IGC = {}\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        IGC[i] = 0\n",
    "        for j in G.neighbors(i):\n",
    "            neighborhood = get_neighborhood(G, j, 2)\n",
    "            for p in neighborhood:\n",
    "                IGC[i] += (ks[j] * k[p]) / (nx.shortest_path_length(G, j, p)**2)\n",
    "    return IGC\n",
    "\n",
    "def k_shell(graph):\n",
    "    return nx.core_number(graph)\n",
    "\n",
    "def evaluate_power_k_shell(k_si, k_sj):\n",
    "    return math.sqrt(k_si + k_sj)\n",
    "\n",
    "def compute_P_ji(graph, node_i, node_j):\n",
    "    try:\n",
    "        a_ji = graph[node_j][node_i]['weight'] if 'weight' in graph[node_j][node_i] else 1\n",
    "        total_a_ji = sum(graph[node_j][nbr]['weight'] if 'weight' in graph[node_j][nbr] else 1 for nbr in graph.neighbors(node_j))\n",
    "        return a_ji / total_a_ji\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def evaluate_ED(graph, node_i, node_j):\n",
    "    P_ji = compute_P_ji(graph, node_i, node_j)\n",
    "    if P_ji == 0:\n",
    "        return 1  # Prevent log(0)\n",
    "    return max(1 - math.log(P_ji), 1)\n",
    "\n",
    "def evaluate_influence(graph, node_i, node_j, k_shell_values):\n",
    "    alpha_ij = evaluate_power_k_shell(k_shell_values[node_i], k_shell_values[node_j])\n",
    "    ED_ij = evaluate_ED(graph, node_i, node_j)\n",
    "    degree_vi = graph.degree(node_i)\n",
    "    return alpha_ij * degree_vi / ED_ij\n",
    "\n",
    "# EDBC\n",
    "def method17(G):\n",
    "    k_shell_values = k_shell(G)\n",
    "    EDBC = {u: 0 for u in G.nodes()}\n",
    "    \n",
    "    for u in G.nodes():\n",
    "        for v in G.neighbors(u):\n",
    "            for neighbor_u in G.neighbors(u):\n",
    "                EDBC[v] += evaluate_influence(G, v, neighbor_u, k_shell_values)\n",
    "    \n",
    "    return EDBC\n",
    "\n",
    "def method18(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    entropy_centrality = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if len(neighbors) == 0:\n",
    "            entropy_centrality[node] = 0\n",
    "        else:\n",
    "            sum_degrees = sum(G.degree(neighbor) for neighbor in neighbors)\n",
    "            sum_centrality = sum(degree_centrality[neighbor] for neighbor in neighbors)\n",
    "            if sum_centrality == 0:\n",
    "                entropy_centrality[node] = 0\n",
    "            else:\n",
    "                entropy_centrality[node] = closeness_centrality[node] * (sum_degrees / sum_centrality)\n",
    "    \n",
    "    return entropy_centrality\n",
    "\n",
    "# KNC\n",
    "def method19(G):\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    degree_values = dict(G.degree())\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    \n",
    "    alpha = compute_alpha(G)\n",
    "    KNC = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(k_shell_values[neighbor] * degree_values[neighbor] * clustering_coeff[neighbor] for neighbor in neighbors)\n",
    "        KNC[node] = alpha * sum_term\n",
    "    \n",
    "    return KNC\n",
    "\n",
    "# NEDC\n",
    "def method20(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    \n",
    "    nodes = G.nodes()\n",
    "    NEDC = {}\n",
    "    \n",
    "    for node in nodes:\n",
    "        term1 = degree_centrality[node] * betweenness_centrality[node]\n",
    "        term2 = closeness_centrality[node] * k_shell_values[node]\n",
    "        NEDC[node] = term1 + term2\n",
    "    \n",
    "    return NEDC\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "# Method 21 (LGC)\n",
    "def method21(G):\n",
    "    LC = {}\n",
    "    K = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        LC[i] = sum(K[j] for j in nx.neighbors(G, i)) * 2 + K[i] ** 2 + K[i]\n",
    "\n",
    "    LGC = {}\n",
    "    radius = 3\n",
    "    for vi in G.nodes():\n",
    "        LGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            try:\n",
    "                d_ij = nx.shortest_path_length(G, vi, vj)\n",
    "                if d_ij != 0:\n",
    "                    LGC[vi] += LC[vi] * LC[vj] / d_ij ** 2\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    return LGC\n",
    "\n",
    "# Method 22 (SEGM)\n",
    "def method22(G):\n",
    "    k = dict(G.degree())\n",
    "    I = dict()\n",
    "    E = dict()\n",
    "    SE = dict()\n",
    "\n",
    "    for node in G.nodes:\n",
    "        I[node] = 0\n",
    "\n",
    "    for i in G.nodes:\n",
    "        sum_kj = 0\n",
    "        for j in G.neighbors(i):\n",
    "            sum_kj += k[j]\n",
    "        if sum_kj != 0:\n",
    "            I[i] = k[i] / sum_kj\n",
    "        sum_e = 0\n",
    "        for j in G.neighbors(i):\n",
    "            if I[j] != 0:\n",
    "                sum_e += I[j] * np.log(I[j])\n",
    "        E[i] = -1 * sum_e\n",
    "        SE[i] = np.exp(E[i]) * k[i]\n",
    "\n",
    "    SEGM = dict()\n",
    "    R = 3\n",
    "    for i in G.nodes:\n",
    "        SEGM[i] = 0\n",
    "        neighbors = get_neighborhood(G, i, R)\n",
    "        for j in neighbors:\n",
    "            if nx.shortest_path_length(G, i, j) != 0:\n",
    "                SEGM[i] += (SE[i] * SE[j]) / (nx.shortest_path_length(G, i, j) ** 2)\n",
    "\n",
    "    return SEGM\n",
    "\n",
    "# Method 23 (MCGM)\n",
    "def method23(G):\n",
    "    k = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    x = nx.eigenvector_centrality(G)\n",
    "\n",
    "    k_values = np.array(list(k.values()))\n",
    "    ks_values = np.array(list(ks.values()))\n",
    "    x_values = np.array(list(x.values()))\n",
    "\n",
    "    k_mid = np.median(k_values)\n",
    "    ks_mid = np.median(ks_values)\n",
    "    x_mid = np.median(x_values)\n",
    "\n",
    "    k_max = max(k_values)\n",
    "    ks_max = max(ks_values)\n",
    "    x_max = max(x_values)\n",
    "\n",
    "    alpha = max(k_mid / k_max, x_mid / x_max) / (ks_mid / ks_max)\n",
    "\n",
    "    MCGM = dict()\n",
    "    R = 3\n",
    "\n",
    "    for i in G.nodes():\n",
    "        MCGM[i] = 0\n",
    "        for j in G.nodes():\n",
    "            if i != j and nx.shortest_path_length(G, source=i, target=j) <= R:\n",
    "                MCGM[i] += ((k[i] / k_max + alpha * ks[i] / ks_max + x[i] / x_max) *\n",
    "                            (k[j] / k_max + alpha * ks[j] / ks_max + x[j] / x_max) /\n",
    "                            (nx.shortest_path_length(G, source=i, target=j) ** 2))\n",
    "\n",
    "    return MCGM\n",
    "\n",
    "# Method 24 (HVGC)\n",
    "def compute_H_index(G, i):\n",
    "    degrees = [G.degree(neighbor) for neighbor in G.neighbors(i)]\n",
    "    degrees.sort(reverse=True)\n",
    "    H_index = 0\n",
    "    for idx, degree in enumerate(degrees):\n",
    "        if degree >= idx + 1:\n",
    "            H_index = idx + 1\n",
    "        else:\n",
    "            break\n",
    "    return H_index\n",
    "\n",
    "def compute_H_v(G, i):\n",
    "    H_i = compute_H_index(G, i)\n",
    "    H_v_i = sum(G.degree(j) for j in G.neighbors(i) if G.degree(j) >= H_i)\n",
    "    return H_v_i\n",
    "\n",
    "def compute_c_i(G, i):\n",
    "    def compute_p_ij(G, i, j):\n",
    "        neighbors_i = list(G.neighbors(i))\n",
    "        sum_z_iw = sum(G[i][w].get('weight', 1) for w in neighbors_i)\n",
    "        p_ij = G[i][j].get('weight', 1) / sum_z_iw if sum_z_iw != 0 else 0\n",
    "        return p_ij\n",
    "\n",
    "    c_i = 0\n",
    "    neighbors_i = list(G.neighbors(i))\n",
    "    for j in neighbors_i:\n",
    "        p_ij = compute_p_ij(G, i, j)\n",
    "        neighbors_j = list(G.neighbors(j))\n",
    "        common_neighbors = set(neighbors_i).intersection(neighbors_j)\n",
    "        sum_piw_pwj = sum(compute_p_ij(G, i, w) * compute_p_ij(G, w, j) for w in common_neighbors)\n",
    "        term = p_ij + sum_piw_pwj\n",
    "        c_i += term ** 2\n",
    "    \n",
    "    return c_i\n",
    "\n",
    "def compute_HVGC(G, i, H_v, R):\n",
    "    c_i = compute_c_i(G, i)\n",
    "    HVGC_i = 0\n",
    "    for j in G.nodes():\n",
    "        if j != i:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij <= R:\n",
    "                HVGC_i += (math.exp(-c_i) * H_v[i] * H_v[j]) / (d_ij ** 2)\n",
    "    return HVGC_i\n",
    "\n",
    "#HVGC\n",
    "def method24(G):\n",
    "    H_v = {i: compute_H_v(G, i) for i in G.nodes()}\n",
    "    R = 2\n",
    "    HVGC = {node: compute_HVGC(G, node, H_v, R) for node in G.nodes()}\n",
    "    return HVGC\n",
    "\n",
    "# Method 25 (MDD)\n",
    "def method25(G):\n",
    "    def compute_mixed_degrees(H, landa):\n",
    "        km = {}\n",
    "        for v in H.nodes():\n",
    "            kr = H.degree(v)\n",
    "            ke = G.degree(v) - kr\n",
    "            km[v] = kr + landa * ke\n",
    "        return km\n",
    "\n",
    "    H = G.copy()\n",
    "    landa = 0.7\n",
    "    rank = {}\n",
    "    km = {v: 1 for v in G.nodes()}  # Initialize km values\n",
    "\n",
    "    while H.number_of_nodes() > 0:\n",
    "        km = compute_mixed_degrees(H, landa)\n",
    "        min_km_value = min(km.values())\n",
    "\n",
    "        # Find and remove nodes with mixed degree <= min_km_value\n",
    "        node_set = [v for v in H.nodes() if km[v] == min_km_value]\n",
    "        for each in node_set:\n",
    "            rank[each] = km[each]\n",
    "            H.remove_node(each)\n",
    "\n",
    "    return rank\n",
    "\n",
    "# Method 26 (BaseGM)\n",
    "def method26(G):\n",
    "    def get_neighborhood(G, node, radius):\n",
    "        neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "        neighborhood.pop(node, None)\n",
    "        return list(neighborhood.keys())\n",
    "\n",
    "    gravity = dict()\n",
    "    kshell = nx.core_number(G)\n",
    "    for vi in G.nodes():\n",
    "        gravity[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, 3)\n",
    "        for vj in neighbors:\n",
    "            gravity[vi] += ((kshell[vi] * kshell[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    gravity_plus = dict()\n",
    "    for vi in G.nodes():\n",
    "        gravity_plus[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            gravity_plus[vi] += gravity[vj]\n",
    "\n",
    "    return gravity_plus\n",
    "\n",
    "# Method 27 (GlobalGM)\n",
    "def method27(G):\n",
    "    gm = dict()\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        gm[vi] = 0\n",
    "        for vj in G.nodes():\n",
    "            if vj != vi:\n",
    "                gm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gm\n",
    "\n",
    "# Method 28 (LocalGM)\n",
    "def method28(G):\n",
    "    lgm = dict()\n",
    "    radius = 3\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        lgm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                lgm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return lgm\n",
    "\n",
    "# Method 29 (WGravity)\n",
    "def method29(G):\n",
    "    gmm = dict()\n",
    "    ev = nx.eigenvector_centrality(G)\n",
    "    k = dict(nx.degree(G))\n",
    "\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        gmm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                gmm[vi] += (ev[vi] * (k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gmm\n",
    "\n",
    "\n",
    "# Method 30 (HKS)\n",
    "def method30(G):\n",
    "    def set_b_values(G):\n",
    "        b, Shell, b_values = 1, 1, {}\n",
    "        while G.number_of_nodes() > 0:\n",
    "            flag = False\n",
    "            for v in list(G.nodes):\n",
    "                if G.degree[v] <= Shell:\n",
    "                    b_values[v] = b\n",
    "                    flag = True\n",
    "            \n",
    "            if flag:\n",
    "                G.remove_nodes_from([v for v in list(G.nodes) if b_values.get(v) == b])\n",
    "                b += 1\n",
    "            else:\n",
    "                Shell += 1\n",
    "        \n",
    "        return b_values\n",
    "\n",
    "    def set_f(G, b_values):\n",
    "        V, f = list(G.nodes), max(b_values.values())\n",
    "        fi = {v: b_values[v] if all(b_values[v] >= b_values[vj] for vj in G.neighbors(v)) else 0 for v in V}\n",
    "\n",
    "        while V:\n",
    "            for vi in V:\n",
    "                if fi[vi] == f:\n",
    "                    for vj in G.neighbors(vi):\n",
    "                        if fi[vi] - 1 > fi[vj]:\n",
    "                            fi[vj] = fi[vi] - 1\n",
    "            V.remove(vi)\n",
    "\n",
    "        return fi\n",
    "\n",
    "    b_values = set_b_values(G.copy())\n",
    "    fi_values = set_f(G.copy(), b_values)\n",
    "\n",
    "    S = {vi: sum(G.degree[vj] * (b_values[vj] + fi_values[vj]) for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    HKS = {vi: sum(S[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return HKS\n",
    "\n",
    "# Method 31 (SHKS)\n",
    "def method31(G):\n",
    "    def compute_efficiency(G):\n",
    "        n = len(G)\n",
    "        if n <= 1:\n",
    "            return 0\n",
    "        inv_distances = (1 / d for u, v_dict in nx.shortest_path_length(G) for v, d in v_dict.items() if d > 0)\n",
    "        return sum(inv_distances) / (n * (n - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_copy = G.copy()\n",
    "            G_copy.remove_node(k)\n",
    "            centrality[k] = (efficiency_G - compute_efficiency(G_copy)) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    def network_constrained_coefficient(G, v):\n",
    "        neighbors = list(G.neighbors(v))\n",
    "        coefficient = sum(calculate_pvu(G, v, u) + sum((calculate_pvu(G, v, w) * calculate_pvu(G, w, u)) ** 2 for w in set(G.neighbors(u)) & set(G.neighbors(v))) for u in neighbors)\n",
    "        return coefficient\n",
    "\n",
    "    def calculate_pvu(G, v, u):\n",
    "        return (1 if u in G.neighbors(v) else 0) / G.degree(v)\n",
    "\n",
    "    efficiency_centrality = compute_efficiency_centrality(G)\n",
    "    Constraint_Coef = {node: network_constrained_coefficient(G, node) for node in G.nodes()}\n",
    "    sh = {node: 1 / Constraint_Coef[node] for node in G.nodes()}\n",
    "    ks = nx.core_number(G)\n",
    "    alpha = 0.2\n",
    "    I = {node: alpha * sh[node] + ks[node] for node in G.nodes()}\n",
    "    C = {v: sum(I[v] + I[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    IS = {v: sum(C[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    SHKS = {v: sum(IS[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    return SHKS\n",
    "\n",
    "# Method 32 (KSGC)\n",
    "def method32(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_min = min(ks.values())\n",
    "    k = dict(G.degree())\n",
    "    n = G.number_of_nodes()\n",
    "    C = np.zeros((n, n))\n",
    "    F = np.zeros((n, n))\n",
    "    nodes = list(G.nodes())\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                vi, vj = nodes[i], nodes[j]\n",
    "                try:\n",
    "                    C[i, j] = np.exp((ks[vi] - ks[vj]) / (ks_max - ks_min))\n",
    "                    F[i, j] = C[i, j] * (k[vi] * k[vj] / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    F[i, j] = 0\n",
    "\n",
    "    KSGC = dict()\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        vi_index = node_index[vi]\n",
    "        KSGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            vj_index = node_index[vj]\n",
    "            KSGC[vi] += F[vi_index, vj_index]\n",
    "    \n",
    "    return KSGC\n",
    "\n",
    "# Method 33 (EFFC)\n",
    "def method33(G):\n",
    "    def compute_efficiency(G):\n",
    "        N = len(G.nodes)\n",
    "        if N < 2:\n",
    "            return 0\n",
    "        efficiency_sum = 0\n",
    "        for i in G.nodes:\n",
    "            for j in G.nodes:\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        shortest_path_length = nx.shortest_path_length(G, source=i, target=j)\n",
    "                        efficiency_sum += 1 / shortest_path_length\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        efficiency_sum += 0  # Adding 0 for disconnected pairs\n",
    "        return efficiency_sum / (N * (N - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_k_prime = G.copy()\n",
    "            G_k_prime.remove_node(k)\n",
    "            efficiency_G_k_prime = compute_efficiency(G_k_prime)\n",
    "            centrality[k] = (efficiency_G - efficiency_G_k_prime) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    return compute_efficiency_centrality(G)\n",
    "\n",
    "# Method 34 (Local Relative ASP)\n",
    "def method34(G):\n",
    "    def find_diameter(G):\n",
    "        \"\"\"\n",
    "        Find the diameter (longest shortest path) of the graph G.\n",
    "        \"\"\"\n",
    "        if nx.is_connected(G):\n",
    "            return nx.diameter(G)\n",
    "        else:\n",
    "            lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "            diameter = 0\n",
    "            for u in lengths:\n",
    "                for v in lengths[u]:\n",
    "                    if lengths[u][v] > diameter:\n",
    "                        diameter = lengths[u][v]\n",
    "            return diameter\n",
    "\n",
    "    def calculate_asp(G):\n",
    "        \"\"\"\n",
    "        Calculate the Average Shortest Path (ASP) for the graph G.\n",
    "        \"\"\"\n",
    "        n = len(G.nodes)\n",
    "        if n < 2:\n",
    "            return 0\n",
    "        \n",
    "        if nx.is_connected(G):\n",
    "            return nx.average_shortest_path_length(G)\n",
    "        \n",
    "        diameter = find_diameter(G)\n",
    "        asp_sum = 0\n",
    "        \n",
    "        lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "        for u in G.nodes:\n",
    "            for v in G.nodes:\n",
    "                if u != v:\n",
    "                    asp_sum += lengths[u].get(v, diameter)\n",
    "        \n",
    "        return asp_sum / (n * (n - 1))\n",
    "\n",
    "    asp = calculate_asp(G)\n",
    "    AC = {}\n",
    "\n",
    "    for k in G.nodes():\n",
    "        G_removed = G.copy()\n",
    "        G_removed.remove_node(k)\n",
    "        asp_removed = calculate_asp(G_removed)\n",
    "        AC[k] = abs(asp_removed - asp) / asp if asp > 0 else 0\n",
    "\n",
    "    return AC\n",
    "\n",
    "def k_neighbors(G, node, radius):\n",
    "        neighbors = set(nx.single_source_shortest_path_length(G, node, cutoff=radius).keys())\n",
    "        neighbors.discard(node)  # Remove the node itself if present\n",
    "        return neighbors\n",
    "    \n",
    "# Method 35 (InformationRank)\n",
    "def method35(G):\n",
    "    \n",
    "    PROPA = {}\n",
    "    Score = {}\n",
    "    L = 2\n",
    "    Miu = 0.2\n",
    "\n",
    "    # Initialize PROPA and Score dictionaries\n",
    "    for v in G.nodes():\n",
    "        PROPA[v] = {}\n",
    "        Score[v] = 0\n",
    "\n",
    "    # Calculate PROPA and Score\n",
    "    for v in G.nodes():\n",
    "        neighbors = k_neighbors(G, v, L)\n",
    "        for w in neighbors:\n",
    "            PROPA[v][w] = 1\n",
    "            path_length_counts = {}\n",
    "            for path in nx.all_simple_paths(G, v, w, cutoff=L):\n",
    "                length = len(path) - 1\n",
    "                path_length_counts[length] = path_length_counts.get(length, 0) + 1\n",
    "\n",
    "            for l in range(1, L + 1):\n",
    "                pow_val = path_length_counts.get(l, 0)\n",
    "                PROPA[v][w] *= (1 - Miu ** l) ** pow_val\n",
    "            PROPA[v][w] = 1 - PROPA[v][w]\n",
    "            Score[v] += PROPA[v][w]\n",
    "\n",
    "    return Score\n",
    "\n",
    "\n",
    "# Method 36 (Weighted K-shell)\n",
    "def method36(G):\n",
    "    c1 = 0.1\n",
    "    c2 = 0.4\n",
    "    deg = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    ksdw = dict()\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksdw[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            ksdw[vi] += (c1 * deg[vi] + c2 * ks[vi]) * (c1 * deg[vj] + c2 * ks[vj])\n",
    "\n",
    "    return ksdw\n",
    "\n",
    "# Method 37 (HybridKshell)\n",
    "def method37(G):\n",
    "    radius = 2\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(G.degree())\n",
    "    ksh = dict()\n",
    "    landa = 0.4\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksh[vi] = 0\n",
    "        neighbors = k_neighbors(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            ksh[vi] += (math.sqrt(ks[vi] + ks[vj]) + landa * k[vj]) / (nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    return ksh\n",
    "\n",
    "# Optimized method 38 (Social Capital)\n",
    "def method38(G):\n",
    "    SC = {}\n",
    "    K = dict(G.degree())\n",
    "    for i in G.nodes:\n",
    "        SC[i] = K[i] + sum(K[j] for j in G.neighbors(i))\n",
    "    return SC\n",
    "\n",
    "# Optimized method 39 (Potential Edge Weight)\n",
    "def method39(G):\n",
    "    KW = {}\n",
    "    k = dict(G.degree())\n",
    "    Landa = 0.5\n",
    "\n",
    "    for vi in G.nodes:\n",
    "        KW[vi] = Landa * k[vi] + sum((1 - Landa) * (k[vi] + k[vj]) for vj in G.neighbors(vi))\n",
    "        KW[vi] = int(KW[vi])\n",
    "\n",
    "    def find_and_remove_nodes(H, ks):\n",
    "        nodes_to_remove = [node for node in list(H.nodes) if KW[node] <= ks]\n",
    "        H.remove_nodes_from(nodes_to_remove)\n",
    "        return nodes_to_remove\n",
    "\n",
    "    H = G.copy()\n",
    "    ks = min(KW.values())\n",
    "    kshell = {}\n",
    "    tmp = []\n",
    "\n",
    "    while H.nodes:\n",
    "        nodes_to_remove = find_and_remove_nodes(H, ks)\n",
    "        if not nodes_to_remove:\n",
    "            if tmp:\n",
    "                kshell[ks] = tmp\n",
    "            ks += 1\n",
    "            tmp = []\n",
    "        else:\n",
    "            tmp.extend(nodes_to_remove)\n",
    "        if not H.nodes:\n",
    "            kshell[ks] = tmp\n",
    "            break\n",
    "\n",
    "    weightedks = {}\n",
    "    wks = 1\n",
    "    for ks, value in kshell.items():\n",
    "        if value:\n",
    "            weightedks[wks] = value\n",
    "            wks += 1\n",
    "            \n",
    "    output_wks = {}\n",
    "    for weight, nodes in weightedks.items():\n",
    "        for node in nodes:\n",
    "            output_wks[node] = weight\n",
    "\n",
    "    return output_wks\n",
    "\n",
    "# method calculate effg centrality\n",
    "def method40(G):\n",
    "    def calculate_effective_distance(G):\n",
    "        degree = dict(G.degree())\n",
    "        \n",
    "        # Step 1: Calculate the probability P n|m\n",
    "        probability = {(m, n): (1 / degree[m]) if G.has_edge(m, n) else 0 \n",
    "                       for m in G.nodes for n in G.nodes if m != n}\n",
    "\n",
    "        # Step 2: Calculate the effective distance D_{n|m} for directly connected nodes\n",
    "        effective_distance = {(m, n): 1 - np.log2(p) if p > 0 else float('inf') \n",
    "                              for (m, n), p in probability.items()}\n",
    "\n",
    "        # Step 3: Calculate the effective distance for indirectly connected nodes using the shortest path\n",
    "        all_pairs_shortest_path_length = dict(nx.all_pairs_dijkstra_path_length(G))\n",
    "        for m in G.nodes:\n",
    "            for n in G.nodes:\n",
    "                if m != n:\n",
    "                    shortest_path_length = all_pairs_shortest_path_length[m].get(n, float('inf'))\n",
    "                    if shortest_path_length != float('inf'):\n",
    "                        effective_distance[(m, n)] = shortest_path_length\n",
    "\n",
    "        return effective_distance\n",
    "\n",
    "    def calculate_interaction_scores(G, effective_distance):\n",
    "        interaction_scores = {}\n",
    "        for (i, j), d in effective_distance.items():\n",
    "            k_i = G.degree[i]\n",
    "            k_j = G.degree[j]\n",
    "            interaction_scores[(i, j)] = (k_i * k_j) / (d ** 2)\n",
    "        return interaction_scores\n",
    "\n",
    "    def compute_effg_centrality(G, interaction_scores):\n",
    "        effg_centrality = {node: 0 for node in G.nodes()}\n",
    "        for (i, j), score in interaction_scores.items():\n",
    "            effg_centrality[i] += score\n",
    "            effg_centrality[j] += score\n",
    "        return effg_centrality\n",
    "\n",
    "    # Calculate Effective Distance\n",
    "    effective_distance = calculate_effective_distance(G)\n",
    "\n",
    "    # Calculate Interaction Scores\n",
    "    interaction_scores = calculate_interaction_scores(G, effective_distance)\n",
    "\n",
    "    # Calculate EffG Centrality\n",
    "    effg_centrality = compute_effg_centrality(G, interaction_scores)\n",
    "    \n",
    "    return effg_centrality\n",
    "\n",
    "# method 41 (IS-PEW)\n",
    "def method41(G):\n",
    "    # Precompute expensive metrics\n",
    "    triangles = nx.triangles(G)  # Dictionary {node: triangle_count}\n",
    "    core_numbers = nx.core_number(G)  # Dictionary {node: core_number}\n",
    "    degrees = dict(G.degree())  # Dictionary {node: degree}\n",
    "    \n",
    "    # Compute total triangles once\n",
    "    TC = sum(triangles.values()) // 3 if sum(triangles.values()) > 0 else 1  # Avoid division by zero\n",
    "    \n",
    "    def compute_TP(v_A):\n",
    "        return triangles[v_A] / TC\n",
    "    \n",
    "    def compute_InfE(v_A, v_B):\n",
    "        common_neighbors = set(G[v_A]) & set(G[v_B])\n",
    "        sum_DG_k = sum(degrees[k] for k in common_neighbors)\n",
    "        return (degrees[v_A] * degrees[v_B]) / (1 + sum_DG_k)\n",
    "\n",
    "    def compute_NIP(v_A):\n",
    "        return sum(np.sqrt(degrees[v_a] * core_numbers[v_a]) for v_a in G[v_A])\n",
    "\n",
    "    # Store edge weights in a dictionary to avoid recomputation\n",
    "    edge_weights = {}\n",
    "\n",
    "    def compute_EW(v_A, v_B):\n",
    "        if (v_A, v_B) in edge_weights:\n",
    "            return edge_weights[(v_A, v_B)]\n",
    "        \n",
    "        TP_v_A, TP_v_B = compute_TP(v_A), compute_TP(v_B)\n",
    "        KS_v_A, KS_v_B = core_numbers[v_A], core_numbers[v_B]\n",
    "        NIP_v_A, NIP_v_B = compute_NIP(v_A), compute_NIP(v_B)\n",
    "        ED_v_A_v_B = 1 / compute_InfE(v_A, v_B)\n",
    "        \n",
    "        EW_value = ((KS_v_A * (1 + TP_v_A) * NIP_v_A) / ED_v_A_v_B) + ((KS_v_B * (1 + TP_v_B) * NIP_v_B) / ED_v_A_v_B)\n",
    "        edge_weights[(v_A, v_B)] = EW_value  # Store result for future use\n",
    "        edge_weights[(v_B, v_A)] = EW_value  # Store symmetric value\n",
    "        return EW_value\n",
    "\n",
    "    def compute_IS(v_A):\n",
    "        return sum(compute_EW(v_A, v_B) for v_B in G[v_A])\n",
    "\n",
    "    # Compute potential edge weights once\n",
    "    for u, v in G.edges():\n",
    "        G[u][v]['weight'] = compute_EW(u, v)\n",
    "\n",
    "    # Compute IS values for all nodes\n",
    "    IS_values = {v: compute_IS(v) for v in G.nodes()}\n",
    "    \n",
    "    return IS_values\n",
    "\n",
    "# k-shell iteration Factor (KS-IF)\n",
    "def method42(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    iteration_factors = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        if iteration_nodes:\n",
    "            m = iteration_nodes[-1][1]\n",
    "            for node, n in iteration_nodes:\n",
    "                iteration_factors[node] = k * (1 + n / m)\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    ks_IF = {}\n",
    "    deg = dict(G.degree())\n",
    "    for vi in G.nodes:\n",
    "        ks_IF[vi] = iteration_factors[vi] * deg[vi]\n",
    "        for vj in nx.neighbors(G, vi):\n",
    "            ks_IF[vi] += iteration_factors[vj] * deg[vj]\n",
    "\n",
    "    return ks_IF\n",
    "\n",
    "\n",
    "# Method 43 (DKGM)\n",
    "def method43(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    k_star = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        m = iteration_nodes[-1][1]\n",
    "        for node, n in iteration_nodes:\n",
    "            k_star[node] = k_shell[node] + (n / (m + 1))\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    DK = {}\n",
    "    DKGM = {}\n",
    "\n",
    "    for node in k_shell:\n",
    "        DK[node] = G.degree[node] + k_star[node]\n",
    "\n",
    "    R = 2  # Define the radius for DKGM computation\n",
    "    for node in G.nodes():\n",
    "        DKGM[node] = 0\n",
    "        for neighbor in nx.single_source_shortest_path_length(G, node, cutoff=R):\n",
    "            if node != neighbor:\n",
    "                try:\n",
    "                    d_ij = nx.shortest_path_length(G, source=node, target=neighbor)\n",
    "                    DKGM[node] += DK[node] * DK[neighbor] / d_ij**2\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "    return DKGM\n",
    "\n",
    "\n",
    "\n",
    "methods = [globals()[f'method{i}'] for i in range(1, 44)]\n",
    "\n",
    "#methods = {\n",
    "#    \"Degree Centrality\": method1,\n",
    "#    \"Betweenness Centrality\": method2,\n",
    "#    \"Closeness Centrality\": method3,\n",
    "#    \"K-shell\": method4,\n",
    "#    \"Cnc\": method5,\n",
    "#    \"Cnc Plus\": method6,\n",
    "#    \"Distance to Network Core\": method7,\n",
    "#    \"All Around Node\": method8,\n",
    "#    \"H-index Centrality\": method9,\n",
    "#    \"Local H-index\": method10,\n",
    "#    \"Semi-Local Centrality\": method11,\n",
    "#    \"DNC\": method12,\n",
    "#    \"ECLGC\": method13,\n",
    "#    \"Centripetal Centrality\": method14,\n",
    "#    \"Cluster Rank\": method15,\n",
    "#    \"Improved Gravity Centrality\":method16,\n",
    "#    \"EDBC\" : method17,\n",
    "#    \"Entropy Centrality\": method18,\n",
    "#    \"KNC\" : method19,\n",
    "#    \"NEDC\" : method20,\n",
    "#    \"LGC\" : method21,\n",
    "#    \"SEGM\" : method22,\n",
    "#    \"MCGM\" : method23,\n",
    "#    \"HVGC\" : method24,\n",
    "#    \"MDD\" : method25,\n",
    "#    \"BaseGM\" : method26,\n",
    "#    \"GlobalGM\" : method27,\n",
    "#    \"LocalGM\" : method28,\n",
    "#    \"WGravity\" : method29,\n",
    "#    \"HKS\" : method30,\n",
    "#    \"SHKS\" : method31,\n",
    "#    \"KSGC\" : method32,\n",
    "#    \"EFFC\" : method33,\n",
    "#    \"Local Relative ASP\" : method34,\n",
    "#    \"Information Rank\" : method35,\n",
    "#    \"Weighted K-shell\" : method36,\n",
    "#    \"Hybrid K-shell\" : method37,\n",
    "#    \"Social Capital\" : method38,\n",
    "#    \"Potential Edge Weight\" : method39,\n",
    "#    \"effg Gravity\" : method40,\n",
    "#    \"IS-PEW\" : method41,\n",
    "#    \"KS-IF\" : method42,\n",
    "#    \"DKGM\" : method43\n",
    "# }\n",
    "\n",
    "# Main Code\n",
    "dataset_folder = 'dataset'\n",
    "beta_values = [0.02, 0.05, 0.08, 0.10, 0.13, 0.15, 0.18, 0.20, 0.22, 0.25, 0.30]\n",
    "alpha_values = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "f_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]  # Percentage of nodes for SI calculation\n",
    "\n",
    "for dataset_filename in os.listdir(dataset_folder):\n",
    "    if dataset_filename.endswith('.xlsx'):\n",
    "        dataset_path = os.path.join(dataset_folder, dataset_filename)\n",
    "\n",
    "        # Load edge list from \"Sheet1\"\n",
    "        df_edges = pd.read_excel(dataset_path, sheet_name='Sheet1')\n",
    "        G = nx.from_pandas_edgelist(df_edges, source='source_column', target='target_column')\n",
    "\n",
    "        # Remove self-loops\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "        # Load precomputed SIR values from the \"SIR\" sheet\n",
    "        df_sir = pd.read_excel(dataset_path, sheet_name='SIR')\n",
    "        df_sir.set_index('Node', inplace=True)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for beta in beta_values:\n",
    "            beta_column = f'Beta_{beta:.2f}'\n",
    "            if beta_column not in df_sir.columns:\n",
    "                print(f\"Warning: {beta_column} not found in {dataset_filename}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Get SIR spread power rankings\n",
    "            spread_power = df_sir[beta_column].to_dict()\n",
    "            sigma = sorted(spread_power.keys(), key=lambda x: spread_power[x], reverse=True)\n",
    "\n",
    "            for i, method in enumerate(methods, start=1):\n",
    "                method_name = f'method{i}'\n",
    "                try:\n",
    "                    ranked_nodes, exec_time = measure_execution_time(method, G)\n",
    "                    monotonicity = compute_monotonicity(G, ranked_nodes)\n",
    "                    tau, p_value = kendalltau(\n",
    "                        [spread_power[node] for node in sigma],\n",
    "                        [spread_power[node] for node, _ in ranked_nodes]\n",
    "                    )\n",
    "                    R = [node for node, _ in ranked_nodes]\n",
    "\n",
    "                    rbo_scores = {f'RBO {alpha}': compute_rbo(sigma, R, alpha) for alpha in alpha_values}\n",
    "\n",
    "                    # Compute Spread Impact (SI) for f = [0.01, ..., 0.1]\n",
    "                    n = len(G.nodes())  # Total number of nodes\n",
    "                    si_scores = {}\n",
    "                    for f in f_values:\n",
    "                        top_f_nodes = [node for node, _ in ranked_nodes[:int(f * n)]]  # Top f  n nodes\n",
    "                        si_scores[f'SI {f:.2f}'] = sum(spread_power[node] for node in top_f_nodes) / (f * n)\n",
    "\n",
    "                    results[(method_name, beta)] = {\n",
    "                        'ranked_nodes': ranked_nodes,\n",
    "                        'execution_time': exec_time,\n",
    "                        'monotonicity': monotonicity,\n",
    "                        f'Beta {beta}': beta,\n",
    "                        f'Kendall Tau {beta}': tau,\n",
    "                        f'P-Value {beta}': p_value,\n",
    "                        **rbo_scores,\n",
    "                        **si_scores  # Include SI values\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    results[(method_name, beta)] = {\n",
    "                        'ranked_nodes': [],\n",
    "                        'execution_time': None,\n",
    "                        'monotonicity': None,\n",
    "                        f'Beta {beta}': beta,\n",
    "                        f'Kendall Tau {beta}': None,\n",
    "                        f'P-Value {beta}': None,\n",
    "                        **{f'RBO {alpha}': None for alpha in alpha_values},\n",
    "                        **{f'SI {f:.2f}': None for f in f_values},  # Add None for SI values in case of error\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "\n",
    "        # Create DataFrame for results\n",
    "        data = []\n",
    "        for (method_name, beta), result in results.items():\n",
    "            top_5_nodes = [\n",
    "                f\"{node}:{score:.4f}\" if isinstance(score, (int, float)) else f\"{node}:N/A\"\n",
    "                for node, score in result['ranked_nodes'][:5]\n",
    "            ] if result['ranked_nodes'] else ['N/A'] * 5\n",
    "\n",
    "            row = [\n",
    "                method_name,\n",
    "                result['execution_time'],\n",
    "                result.get('monotonicity', 'N/A'),\n",
    "                result.get(f'Beta {beta}'),\n",
    "                result.get(f'Kendall Tau {beta}'),\n",
    "                result.get(f'P-Value {beta}')\n",
    "            ] + [result.get(f'RBO {alpha}') for alpha in alpha_values] + \\\n",
    "                [result.get(f'SI {f:.2f}') for f in f_values] + top_5_nodes  # Include SI values in the output\n",
    "\n",
    "            if 'error' in result:\n",
    "                row.append(result['error'])\n",
    "            else:\n",
    "                row.append('')\n",
    "\n",
    "            data.append(row)\n",
    "\n",
    "        expected_columns = ['Method', 'Execution Time', 'Monotonicity', 'Beta', 'Kendall Tau', 'P-Value'] + \\\n",
    "                           [f'RBO {alpha}' for alpha in alpha_values] + \\\n",
    "                           [f'SI {f:.2f}' for f in f_values] + [f'Top {i+1}' for i in range(5)] + ['Error']\n",
    "\n",
    "        result_df = pd.DataFrame(data, columns=expected_columns)\n",
    "\n",
    "        # Write results to Excel file\n",
    "        dataset_name = os.path.splitext(dataset_filename)[0]\n",
    "        output_file = f'{dataset_name}_results.xlsx'\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df_edges.to_excel(writer, sheet_name='Sheet1', index=False)  # Save edge list\n",
    "            df_sir.to_excel(writer, sheet_name='SIR')  # Keep original SIR values\n",
    "            result_df.to_excel(writer, sheet_name='Results', index=False)  # Save computed results\n",
    "\n",
    "        print(f\"Processed {dataset_filename}, results saved in {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8436b-83aa-4183-8443-a88c0ccce324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e1c13-f318-48d9-85c1-6d32f0412c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70f09cc-6b87-418e-9636-d0c61b31572c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dolphins(SIR).xlsx, results saved in Dolphins(SIR)_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "def measure_execution_time(method, G):\n",
    "    start_time = time.time()\n",
    "    result = method(G)\n",
    "    ranked_nodes = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return ranked_nodes, execution_time\n",
    "\n",
    "def compute_mrr(ranked_nodes, ground_truth, k_values):\n",
    "    mrr_scores = {}\n",
    "    for k in k_values:\n",
    "        top_k = set(ranked_nodes[:k])\n",
    "        reciprocal_ranks = [1 / (ranked_nodes.index(node) + 1) for node in ground_truth if node in top_k]\n",
    "        mrr_scores[f'MRR k={k}'] = sum(reciprocal_ranks) / len(ground_truth) if reciprocal_ranks else 0\n",
    "    return mrr_scores\n",
    "\n",
    "def compute_monotonicity(G, ranked_nodes):\n",
    "    ranks = [score for node, score in ranked_nodes]\n",
    "    unique_ranks = list(set(ranks))\n",
    "    n = G.number_of_nodes()\n",
    "    nr_dict = {rank: ranks.count(rank) for rank in unique_ranks}\n",
    "    nr_sum = sum(nr * (nr - 1) for nr in nr_dict.values())\n",
    "    M = (1 - nr_sum / (n * (n - 1))) ** 2\n",
    "    return M\n",
    "\n",
    "def compute_rbo(sigma, R, alpha=0.9):\n",
    "    \"\"\"Compute the Rank-Biased Overlap (RBO) between two ranking lists.\"\"\"\n",
    "    def A(sigma, R, f):\n",
    "        sigma_f = set(sigma[:f])\n",
    "        R_f = set(R[:f])\n",
    "        if len(sigma_f) == 0 and len(R_f) == 0:\n",
    "            return 1\n",
    "        return len(sigma_f & R_f) / len(sigma_f | R_f)\n",
    "\n",
    "    n = max(len(sigma), len(R))\n",
    "    rbo = (1 - alpha) * sum(alpha**(f - 1) * A(sigma, R, f) for f in range(1, n + 1))\n",
    "    return rbo\n",
    "\n",
    "\n",
    "# Define methods\n",
    "def method1(G):\n",
    "    return nx.degree_centrality(G)\n",
    "\n",
    "def method2(G):\n",
    "    return nx.betweenness_centrality(G)\n",
    "\n",
    "def method3(G):\n",
    "    return nx.closeness_centrality(G)\n",
    "\n",
    "def method4(G):\n",
    "    return nx.core_number(G)\n",
    "\n",
    "def method5(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = {vi: sum(ks[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc\n",
    "\n",
    "def method6(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = method5(G)\n",
    "    cnc_plus = {vi: sum(cnc[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc_plus\n",
    "\n",
    "def method7(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_max_set = {node for node, k in ks.items() if k == ks_max}\n",
    "    teta = {}\n",
    "    for vi in G.nodes():\n",
    "        teta_vi = (ks_max - ks[vi] + 1)\n",
    "        total_dis = 0\n",
    "        for vj in ks_max_set:\n",
    "            if nx.has_path(G, vi, vj):\n",
    "                total_dis += nx.shortest_path_length(G, vi, vj)\n",
    "        teta[vi] = teta_vi * total_dis\n",
    "    return teta\n",
    "\n",
    "def method8(G):\n",
    "    ks = nx.core_number(G)\n",
    "    deg = nx.degree_centrality(G)\n",
    "    bet = nx.betweenness_centrality(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_norm = {node: ks[node] / ks_max for node in G.nodes()}\n",
    "    all_around = {v: math.sqrt(ks_norm[v]**2 + bet[v]**2 + deg[v]**2) for v in G.nodes()}\n",
    "    return all_around\n",
    "\n",
    "def method9(G):\n",
    "    h_index = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degrees = sorted([G.degree(neighbor) for neighbor in neighbors], reverse=True)\n",
    "        h = 0\n",
    "        for i, degree in enumerate(degrees):\n",
    "            if degree >= i + 1:\n",
    "                h = i + 1\n",
    "            else:\n",
    "                break\n",
    "        h_index[node] = h\n",
    "    return h_index\n",
    "\n",
    "def method10(G):\n",
    "    HI = method9(G)\n",
    "    LHI = {vi: HI[vi] + sum(HI[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return LHI\n",
    "\n",
    "def method11(G):\n",
    "    nv = {}\n",
    "    for vi in G.nodes():\n",
    "        set1 = set()\n",
    "        for vj in G.neighbors(vi):\n",
    "            set1.add(vj)\n",
    "            for vk in G.neighbors(vj):\n",
    "                set1.add(vk)\n",
    "        nv[vi] = len(set1) - 1\n",
    "    \n",
    "    c_local = {}\n",
    "    for vi in G.nodes():\n",
    "        total1 = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            total2 = 0\n",
    "            for vk in G.neighbors(vj):\n",
    "                total2 += nv[vk]\n",
    "            total1 += total2\n",
    "        c_local[vi] = total1\n",
    "    return c_local\n",
    "\n",
    "def method12(G):\n",
    "    SLC = {}\n",
    "    k = dict(nx.degree(G))\n",
    "    C = nx.clustering(G)\n",
    "    for i in G.nodes():\n",
    "        SLC[i] = sum(C[j] for j in G.neighbors(i))\n",
    "    \n",
    "    DNC = {}\n",
    "    alpha = 1\n",
    "    for i in G.nodes():\n",
    "        DNC[i] = k[i] + alpha * SLC[i]\n",
    "    return DNC\n",
    "\n",
    "def method13(G):\n",
    "    CLC = nx.clustering(G)\n",
    "    CLGC = {}\n",
    "    for i in G.nodes():\n",
    "        CLGC[i] = sum(\n",
    "            math.sqrt(CLC[j]) / nx.shortest_path_length(G, i, j)\n",
    "            for j in G.nodes()\n",
    "            if i != j and nx.has_path(G, i, j)\n",
    "        ) * CLC[i]\n",
    "    \n",
    "    ECLGC = {}\n",
    "    for v in G.nodes():\n",
    "        sum1 = sum2 = sum3 = 0\n",
    "        neighbors_v = list(G.neighbors(v))\n",
    "        for u in G.nodes():\n",
    "            if u != v and nx.has_path(G, v, u):\n",
    "                neighbors_u = list(G.neighbors(u))\n",
    "                lu = len(neighbors_u)\n",
    "                sum1 += CLC[u] / len(neighbors_v)\n",
    "                sum2 = sum(CLC[j] / lu for j in neighbors_u)\n",
    "                sum3 += math.sqrt(CLC[u] + sum2) / nx.shortest_path_length(G, v, u)\n",
    "        ECLGC[v] = sum1 * sum3\n",
    "    return ECLGC\n",
    "\n",
    "def compute_alpha(graph):\n",
    "    size_of_V = graph.number_of_nodes()\n",
    "    order_of_magnitude = int(math.log10(size_of_V))\n",
    "    alpha = size_of_V / order_of_magnitude\n",
    "    return alpha\n",
    "\n",
    "def method14(G, m=1):\n",
    "    alpha = compute_alpha(G)\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    kshell = nx.core_number(G)\n",
    "    \n",
    "    centrality = {}\n",
    "    \n",
    "    for i in G.nodes:\n",
    "        cenC_i = 0\n",
    "        \n",
    "        # Get m-neighborhood of node i\n",
    "        m_neighborhood = set(nx.single_source_shortest_path_length(G, i, cutoff=m).keys())\n",
    "        m_neighborhood.remove(i)  # Exclude the node itself\n",
    "        \n",
    "        for j in m_neighborhood:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij == 0:\n",
    "                continue\n",
    "            DC_i = degree_centrality[i]\n",
    "            C_i = clustering_coeff[i]\n",
    "            k_s_i = kshell[i]\n",
    "            k_s_j = kshell[j]\n",
    "            \n",
    "            term1 = (math.exp(alpha) * DC_i + (1 / C_i if C_i != 0 else 0))\n",
    "            term2 = (k_s_j / (abs(k_s_i - k_s_j) + 1))\n",
    "            \n",
    "            cenC_i += (4 * math.pi**2) * (term1 / (d_ij**2)) * term2\n",
    "        \n",
    "        centrality[i] = cenC_i\n",
    "    \n",
    "    return centrality\n",
    "\n",
    "def method15(G):\n",
    "    clustering_coeffs = nx.clustering(G)\n",
    "    cluster_rank_scores = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    degrees = dict(G.degree())\n",
    "\n",
    "    # Calculate ClusterRank scores\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(degrees[neighbor] + 1 for neighbor in neighbors)\n",
    "        cluster_rank_scores[node] = 10 ** (-clustering_coeffs[node]) * sum_term\n",
    "\n",
    "    return cluster_rank_scores\n",
    "\n",
    "# Optimized methods\n",
    "def get_neighborhood(G, node, radius):\n",
    "    neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "    neighborhood.pop(node, None)\n",
    "    return list(neighborhood.keys())\n",
    "\n",
    "# Improved Gravity Centrality\n",
    "def method16(G):\n",
    "    IGC = {}\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        IGC[i] = 0\n",
    "        for j in G.neighbors(i):\n",
    "            neighborhood = get_neighborhood(G, j, 2)\n",
    "            for p in neighborhood:\n",
    "                IGC[i] += (ks[j] * k[p]) / (nx.shortest_path_length(G, j, p)**2)\n",
    "    return IGC\n",
    "\n",
    "def k_shell(graph):\n",
    "    return nx.core_number(graph)\n",
    "\n",
    "def evaluate_power_k_shell(k_si, k_sj):\n",
    "    return math.sqrt(k_si + k_sj)\n",
    "\n",
    "def compute_P_ji(graph, node_i, node_j):\n",
    "    try:\n",
    "        a_ji = graph[node_j][node_i]['weight'] if 'weight' in graph[node_j][node_i] else 1\n",
    "        total_a_ji = sum(graph[node_j][nbr]['weight'] if 'weight' in graph[node_j][nbr] else 1 for nbr in graph.neighbors(node_j))\n",
    "        return a_ji / total_a_ji\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def evaluate_ED(graph, node_i, node_j):\n",
    "    P_ji = compute_P_ji(graph, node_i, node_j)\n",
    "    if P_ji == 0:\n",
    "        return 1  # Prevent log(0)\n",
    "    return max(1 - math.log(P_ji), 1)\n",
    "\n",
    "def evaluate_influence(graph, node_i, node_j, k_shell_values):\n",
    "    alpha_ij = evaluate_power_k_shell(k_shell_values[node_i], k_shell_values[node_j])\n",
    "    ED_ij = evaluate_ED(graph, node_i, node_j)\n",
    "    degree_vi = graph.degree(node_i)\n",
    "    return alpha_ij * degree_vi / ED_ij\n",
    "\n",
    "# EDBC\n",
    "def method17(G):\n",
    "    k_shell_values = k_shell(G)\n",
    "    EDBC = {u: 0 for u in G.nodes()}\n",
    "    \n",
    "    for u in G.nodes():\n",
    "        for v in G.neighbors(u):\n",
    "            for neighbor_u in G.neighbors(u):\n",
    "                EDBC[v] += evaluate_influence(G, v, neighbor_u, k_shell_values)\n",
    "    \n",
    "    return EDBC\n",
    "\n",
    "def method18(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    entropy_centrality = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if len(neighbors) == 0:\n",
    "            entropy_centrality[node] = 0\n",
    "        else:\n",
    "            sum_degrees = sum(G.degree(neighbor) for neighbor in neighbors)\n",
    "            sum_centrality = sum(degree_centrality[neighbor] for neighbor in neighbors)\n",
    "            if sum_centrality == 0:\n",
    "                entropy_centrality[node] = 0\n",
    "            else:\n",
    "                entropy_centrality[node] = closeness_centrality[node] * (sum_degrees / sum_centrality)\n",
    "    \n",
    "    return entropy_centrality\n",
    "\n",
    "# KNC\n",
    "def method19(G):\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    degree_values = dict(G.degree())\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    \n",
    "    alpha = compute_alpha(G)\n",
    "    KNC = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(k_shell_values[neighbor] * degree_values[neighbor] * clustering_coeff[neighbor] for neighbor in neighbors)\n",
    "        KNC[node] = alpha * sum_term\n",
    "    \n",
    "    return KNC\n",
    "\n",
    "# NEDC\n",
    "def method20(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    \n",
    "    nodes = G.nodes()\n",
    "    NEDC = {}\n",
    "    \n",
    "    for node in nodes:\n",
    "        term1 = degree_centrality[node] * betweenness_centrality[node]\n",
    "        term2 = closeness_centrality[node] * k_shell_values[node]\n",
    "        NEDC[node] = term1 + term2\n",
    "    \n",
    "    return NEDC\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "# Method 21 (LGC)\n",
    "def method21(G):\n",
    "    LC = {}\n",
    "    K = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        LC[i] = sum(K[j] for j in nx.neighbors(G, i)) * 2 + K[i] ** 2 + K[i]\n",
    "\n",
    "    LGC = {}\n",
    "    radius = 3\n",
    "    for vi in G.nodes():\n",
    "        LGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            try:\n",
    "                d_ij = nx.shortest_path_length(G, vi, vj)\n",
    "                if d_ij != 0:\n",
    "                    LGC[vi] += LC[vi] * LC[vj] / d_ij ** 2\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    return LGC\n",
    "\n",
    "# Method 22 (SEGM)\n",
    "def method22(G):\n",
    "    k = dict(G.degree())\n",
    "    I = dict()\n",
    "    E = dict()\n",
    "    SE = dict()\n",
    "\n",
    "    for node in G.nodes:\n",
    "        I[node] = 0\n",
    "\n",
    "    for i in G.nodes:\n",
    "        sum_kj = 0\n",
    "        for j in G.neighbors(i):\n",
    "            sum_kj += k[j]\n",
    "        if sum_kj != 0:\n",
    "            I[i] = k[i] / sum_kj\n",
    "        sum_e = 0\n",
    "        for j in G.neighbors(i):\n",
    "            if I[j] != 0:\n",
    "                sum_e += I[j] * np.log(I[j])\n",
    "        E[i] = -1 * sum_e\n",
    "        SE[i] = np.exp(E[i]) * k[i]\n",
    "\n",
    "    SEGM = dict()\n",
    "    R = 3\n",
    "    for i in G.nodes:\n",
    "        SEGM[i] = 0\n",
    "        neighbors = get_neighborhood(G, i, R)\n",
    "        for j in neighbors:\n",
    "            if nx.shortest_path_length(G, i, j) != 0:\n",
    "                SEGM[i] += (SE[i] * SE[j]) / (nx.shortest_path_length(G, i, j) ** 2)\n",
    "\n",
    "    return SEGM\n",
    "\n",
    "# Method 23 (MCGM)\n",
    "def method23(G):\n",
    "    k = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    x = nx.eigenvector_centrality(G)\n",
    "\n",
    "    k_values = np.array(list(k.values()))\n",
    "    ks_values = np.array(list(ks.values()))\n",
    "    x_values = np.array(list(x.values()))\n",
    "\n",
    "    k_mid = np.median(k_values)\n",
    "    ks_mid = np.median(ks_values)\n",
    "    x_mid = np.median(x_values)\n",
    "\n",
    "    k_max = max(k_values)\n",
    "    ks_max = max(ks_values)\n",
    "    x_max = max(x_values)\n",
    "\n",
    "    alpha = max(k_mid / k_max, x_mid / x_max) / (ks_mid / ks_max)\n",
    "\n",
    "    MCGM = dict()\n",
    "    R = 3\n",
    "\n",
    "    for i in G.nodes():\n",
    "        MCGM[i] = 0\n",
    "        for j in G.nodes():\n",
    "            if i != j and nx.shortest_path_length(G, source=i, target=j) <= R:\n",
    "                MCGM[i] += ((k[i] / k_max + alpha * ks[i] / ks_max + x[i] / x_max) *\n",
    "                            (k[j] / k_max + alpha * ks[j] / ks_max + x[j] / x_max) /\n",
    "                            (nx.shortest_path_length(G, source=i, target=j) ** 2))\n",
    "\n",
    "    return MCGM\n",
    "\n",
    "# Method 24 (HVGC)\n",
    "def compute_H_index(G, i):\n",
    "    degrees = [G.degree(neighbor) for neighbor in G.neighbors(i)]\n",
    "    degrees.sort(reverse=True)\n",
    "    H_index = 0\n",
    "    for idx, degree in enumerate(degrees):\n",
    "        if degree >= idx + 1:\n",
    "            H_index = idx + 1\n",
    "        else:\n",
    "            break\n",
    "    return H_index\n",
    "\n",
    "def compute_H_v(G, i):\n",
    "    H_i = compute_H_index(G, i)\n",
    "    H_v_i = sum(G.degree(j) for j in G.neighbors(i) if G.degree(j) >= H_i)\n",
    "    return H_v_i\n",
    "\n",
    "def compute_c_i(G, i):\n",
    "    def compute_p_ij(G, i, j):\n",
    "        neighbors_i = list(G.neighbors(i))\n",
    "        sum_z_iw = sum(G[i][w].get('weight', 1) for w in neighbors_i)\n",
    "        p_ij = G[i][j].get('weight', 1) / sum_z_iw if sum_z_iw != 0 else 0\n",
    "        return p_ij\n",
    "\n",
    "    c_i = 0\n",
    "    neighbors_i = list(G.neighbors(i))\n",
    "    for j in neighbors_i:\n",
    "        p_ij = compute_p_ij(G, i, j)\n",
    "        neighbors_j = list(G.neighbors(j))\n",
    "        common_neighbors = set(neighbors_i).intersection(neighbors_j)\n",
    "        sum_piw_pwj = sum(compute_p_ij(G, i, w) * compute_p_ij(G, w, j) for w in common_neighbors)\n",
    "        term = p_ij + sum_piw_pwj\n",
    "        c_i += term ** 2\n",
    "    \n",
    "    return c_i\n",
    "\n",
    "def compute_HVGC(G, i, H_v, R):\n",
    "    c_i = compute_c_i(G, i)\n",
    "    HVGC_i = 0\n",
    "    for j in G.nodes():\n",
    "        if j != i:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij <= R:\n",
    "                HVGC_i += (math.exp(-c_i) * H_v[i] * H_v[j]) / (d_ij ** 2)\n",
    "    return HVGC_i\n",
    "\n",
    "#HVGC\n",
    "def method24(G):\n",
    "    H_v = {i: compute_H_v(G, i) for i in G.nodes()}\n",
    "    R = 2\n",
    "    HVGC = {node: compute_HVGC(G, node, H_v, R) for node in G.nodes()}\n",
    "    return HVGC\n",
    "\n",
    "# Method 25 (MDD)\n",
    "def method25(G):\n",
    "    def compute_mixed_degrees(H, landa):\n",
    "        km = {}\n",
    "        for v in H.nodes():\n",
    "            kr = H.degree(v)\n",
    "            ke = G.degree(v) - kr\n",
    "            km[v] = kr + landa * ke\n",
    "        return km\n",
    "\n",
    "    H = G.copy()\n",
    "    landa = 0.7\n",
    "    rank = {}\n",
    "    km = {v: 1 for v in G.nodes()}  # Initialize km values\n",
    "\n",
    "    while H.number_of_nodes() > 0:\n",
    "        km = compute_mixed_degrees(H, landa)\n",
    "        min_km_value = min(km.values())\n",
    "\n",
    "        # Find and remove nodes with mixed degree <= min_km_value\n",
    "        node_set = [v for v in H.nodes() if km[v] == min_km_value]\n",
    "        for each in node_set:\n",
    "            rank[each] = km[each]\n",
    "            H.remove_node(each)\n",
    "\n",
    "    return rank\n",
    "\n",
    "# Method 26 (BaseGM)\n",
    "def method26(G):\n",
    "    def get_neighborhood(G, node, radius):\n",
    "        neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "        neighborhood.pop(node, None)\n",
    "        return list(neighborhood.keys())\n",
    "\n",
    "    gravity = dict()\n",
    "    kshell = nx.core_number(G)\n",
    "    for vi in G.nodes():\n",
    "        gravity[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, 3)\n",
    "        for vj in neighbors:\n",
    "            gravity[vi] += ((kshell[vi] * kshell[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    gravity_plus = dict()\n",
    "    for vi in G.nodes():\n",
    "        gravity_plus[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            gravity_plus[vi] += gravity[vj]\n",
    "\n",
    "    return gravity_plus\n",
    "\n",
    "# Method 27 (GlobalGM)\n",
    "def method27(G):\n",
    "    gm = dict()\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        gm[vi] = 0\n",
    "        for vj in G.nodes():\n",
    "            if vj != vi:\n",
    "                gm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gm\n",
    "\n",
    "# Method 28 (LocalGM)\n",
    "def method28(G):\n",
    "    lgm = dict()\n",
    "    radius = 3\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        lgm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                lgm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return lgm\n",
    "\n",
    "# Method 29 (WGravity)\n",
    "def method29(G):\n",
    "    gmm = dict()\n",
    "    ev = nx.eigenvector_centrality(G)\n",
    "    k = dict(nx.degree(G))\n",
    "\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        gmm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                gmm[vi] += (ev[vi] * (k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gmm\n",
    "\n",
    "\n",
    "# Method 30 (HKS)\n",
    "def method30(G):\n",
    "    def set_b_values(G):\n",
    "        b, Shell, b_values = 1, 1, {}\n",
    "        while G.number_of_nodes() > 0:\n",
    "            flag = False\n",
    "            for v in list(G.nodes):\n",
    "                if G.degree[v] <= Shell:\n",
    "                    b_values[v] = b\n",
    "                    flag = True\n",
    "            \n",
    "            if flag:\n",
    "                G.remove_nodes_from([v for v in list(G.nodes) if b_values.get(v) == b])\n",
    "                b += 1\n",
    "            else:\n",
    "                Shell += 1\n",
    "        \n",
    "        return b_values\n",
    "\n",
    "    def set_f(G, b_values):\n",
    "        V, f = list(G.nodes), max(b_values.values())\n",
    "        fi = {v: b_values[v] if all(b_values[v] >= b_values[vj] for vj in G.neighbors(v)) else 0 for v in V}\n",
    "\n",
    "        while V:\n",
    "            for vi in V:\n",
    "                if fi[vi] == f:\n",
    "                    for vj in G.neighbors(vi):\n",
    "                        if fi[vi] - 1 > fi[vj]:\n",
    "                            fi[vj] = fi[vi] - 1\n",
    "            V.remove(vi)\n",
    "\n",
    "        return fi\n",
    "\n",
    "    b_values = set_b_values(G.copy())\n",
    "    fi_values = set_f(G.copy(), b_values)\n",
    "\n",
    "    S = {vi: sum(G.degree[vj] * (b_values[vj] + fi_values[vj]) for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    HKS = {vi: sum(S[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return HKS\n",
    "\n",
    "# Method 31 (SHKS)\n",
    "def method31(G):\n",
    "    def compute_efficiency(G):\n",
    "        n = len(G)\n",
    "        if n <= 1:\n",
    "            return 0\n",
    "        inv_distances = (1 / d for u, v_dict in nx.shortest_path_length(G) for v, d in v_dict.items() if d > 0)\n",
    "        return sum(inv_distances) / (n * (n - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_copy = G.copy()\n",
    "            G_copy.remove_node(k)\n",
    "            centrality[k] = (efficiency_G - compute_efficiency(G_copy)) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    def network_constrained_coefficient(G, v):\n",
    "        neighbors = list(G.neighbors(v))\n",
    "        coefficient = sum(calculate_pvu(G, v, u) + sum((calculate_pvu(G, v, w) * calculate_pvu(G, w, u)) ** 2 for w in set(G.neighbors(u)) & set(G.neighbors(v))) for u in neighbors)\n",
    "        return coefficient\n",
    "\n",
    "    def calculate_pvu(G, v, u):\n",
    "        return (1 if u in G.neighbors(v) else 0) / G.degree(v)\n",
    "\n",
    "    efficiency_centrality = compute_efficiency_centrality(G)\n",
    "    Constraint_Coef = {node: network_constrained_coefficient(G, node) for node in G.nodes()}\n",
    "    sh = {node: 1 / Constraint_Coef[node] for node in G.nodes()}\n",
    "    ks = nx.core_number(G)\n",
    "    alpha = 0.2\n",
    "    I = {node: alpha * sh[node] + ks[node] for node in G.nodes()}\n",
    "    C = {v: sum(I[v] + I[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    IS = {v: sum(C[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    SHKS = {v: sum(IS[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    return SHKS\n",
    "\n",
    "# Method 32 (KSGC)\n",
    "def method32(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_min = min(ks.values())\n",
    "    k = dict(G.degree())\n",
    "    n = G.number_of_nodes()\n",
    "    C = np.zeros((n, n))\n",
    "    F = np.zeros((n, n))\n",
    "    nodes = list(G.nodes())\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                vi, vj = nodes[i], nodes[j]\n",
    "                try:\n",
    "                    C[i, j] = np.exp((ks[vi] - ks[vj]) / (ks_max - ks_min))\n",
    "                    F[i, j] = C[i, j] * (k[vi] * k[vj] / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    F[i, j] = 0\n",
    "\n",
    "    KSGC = dict()\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        vi_index = node_index[vi]\n",
    "        KSGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            vj_index = node_index[vj]\n",
    "            KSGC[vi] += F[vi_index, vj_index]\n",
    "    \n",
    "    return KSGC\n",
    "\n",
    "# Method 33 (EFFC)\n",
    "def method33(G):\n",
    "    def compute_efficiency(G):\n",
    "        N = len(G.nodes)\n",
    "        if N < 2:\n",
    "            return 0\n",
    "        efficiency_sum = 0\n",
    "        for i in G.nodes:\n",
    "            for j in G.nodes:\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        shortest_path_length = nx.shortest_path_length(G, source=i, target=j)\n",
    "                        efficiency_sum += 1 / shortest_path_length\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        efficiency_sum += 0  # Adding 0 for disconnected pairs\n",
    "        return efficiency_sum / (N * (N - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_k_prime = G.copy()\n",
    "            G_k_prime.remove_node(k)\n",
    "            efficiency_G_k_prime = compute_efficiency(G_k_prime)\n",
    "            centrality[k] = (efficiency_G - efficiency_G_k_prime) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    return compute_efficiency_centrality(G)\n",
    "\n",
    "# Method 34 (Local Relative ASP)\n",
    "def method34(G):\n",
    "    def find_diameter(G):\n",
    "        \"\"\"\n",
    "        Find the diameter (longest shortest path) of the graph G.\n",
    "        \"\"\"\n",
    "        if nx.is_connected(G):\n",
    "            return nx.diameter(G)\n",
    "        else:\n",
    "            lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "            diameter = 0\n",
    "            for u in lengths:\n",
    "                for v in lengths[u]:\n",
    "                    if lengths[u][v] > diameter:\n",
    "                        diameter = lengths[u][v]\n",
    "            return diameter\n",
    "\n",
    "    def calculate_asp(G):\n",
    "        \"\"\"\n",
    "        Calculate the Average Shortest Path (ASP) for the graph G.\n",
    "        \"\"\"\n",
    "        n = len(G.nodes)\n",
    "        if n < 2:\n",
    "            return 0\n",
    "        \n",
    "        if nx.is_connected(G):\n",
    "            return nx.average_shortest_path_length(G)\n",
    "        \n",
    "        diameter = find_diameter(G)\n",
    "        asp_sum = 0\n",
    "        \n",
    "        lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "        for u in G.nodes:\n",
    "            for v in G.nodes:\n",
    "                if u != v:\n",
    "                    asp_sum += lengths[u].get(v, diameter)\n",
    "        \n",
    "        return asp_sum / (n * (n - 1))\n",
    "\n",
    "    asp = calculate_asp(G)\n",
    "    AC = {}\n",
    "\n",
    "    for k in G.nodes():\n",
    "        G_removed = G.copy()\n",
    "        G_removed.remove_node(k)\n",
    "        asp_removed = calculate_asp(G_removed)\n",
    "        AC[k] = abs(asp_removed - asp) / asp if asp > 0 else 0\n",
    "\n",
    "    return AC\n",
    "\n",
    "def k_neighbors(G, node, radius):\n",
    "        neighbors = set(nx.single_source_shortest_path_length(G, node, cutoff=radius).keys())\n",
    "        neighbors.discard(node)  # Remove the node itself if present\n",
    "        return neighbors\n",
    "    \n",
    "# Method 35 (InformationRank)\n",
    "def method35(G):\n",
    "    \n",
    "    PROPA = {}\n",
    "    Score = {}\n",
    "    L = 2\n",
    "    Miu = 0.2\n",
    "\n",
    "    # Initialize PROPA and Score dictionaries\n",
    "    for v in G.nodes():\n",
    "        PROPA[v] = {}\n",
    "        Score[v] = 0\n",
    "\n",
    "    # Calculate PROPA and Score\n",
    "    for v in G.nodes():\n",
    "        neighbors = k_neighbors(G, v, L)\n",
    "        for w in neighbors:\n",
    "            PROPA[v][w] = 1\n",
    "            path_length_counts = {}\n",
    "            for path in nx.all_simple_paths(G, v, w, cutoff=L):\n",
    "                length = len(path) - 1\n",
    "                path_length_counts[length] = path_length_counts.get(length, 0) + 1\n",
    "\n",
    "            for l in range(1, L + 1):\n",
    "                pow_val = path_length_counts.get(l, 0)\n",
    "                PROPA[v][w] *= (1 - Miu ** l) ** pow_val\n",
    "            PROPA[v][w] = 1 - PROPA[v][w]\n",
    "            Score[v] += PROPA[v][w]\n",
    "\n",
    "    return Score\n",
    "\n",
    "\n",
    "# Method 36 (Weighted K-shell)\n",
    "def method36(G):\n",
    "    c1 = 0.1\n",
    "    c2 = 0.4\n",
    "    deg = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    ksdw = dict()\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksdw[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            ksdw[vi] += (c1 * deg[vi] + c2 * ks[vi]) * (c1 * deg[vj] + c2 * ks[vj])\n",
    "\n",
    "    return ksdw\n",
    "\n",
    "# Method 37 (HybridKshell)\n",
    "def method37(G):\n",
    "    radius = 2\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(G.degree())\n",
    "    ksh = dict()\n",
    "    landa = 0.4\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksh[vi] = 0\n",
    "        neighbors = k_neighbors(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            ksh[vi] += (math.sqrt(ks[vi] + ks[vj]) + landa * k[vj]) / (nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    return ksh\n",
    "\n",
    "# Optimized method 38 (Social Capital)\n",
    "def method38(G):\n",
    "    SC = {}\n",
    "    K = dict(G.degree())\n",
    "    for i in G.nodes:\n",
    "        SC[i] = K[i] + sum(K[j] for j in G.neighbors(i))\n",
    "    return SC\n",
    "\n",
    "# Optimized method 39 (Potential Edge Weight)\n",
    "def method39(G):\n",
    "    KW = {}\n",
    "    k = dict(G.degree())\n",
    "    Landa = 0.5\n",
    "\n",
    "    for vi in G.nodes:\n",
    "        KW[vi] = Landa * k[vi] + sum((1 - Landa) * (k[vi] + k[vj]) for vj in G.neighbors(vi))\n",
    "        KW[vi] = int(KW[vi])\n",
    "\n",
    "    def find_and_remove_nodes(H, ks):\n",
    "        nodes_to_remove = [node for node in list(H.nodes) if KW[node] <= ks]\n",
    "        H.remove_nodes_from(nodes_to_remove)\n",
    "        return nodes_to_remove\n",
    "\n",
    "    H = G.copy()\n",
    "    ks = min(KW.values())\n",
    "    kshell = {}\n",
    "    tmp = []\n",
    "\n",
    "    while H.nodes:\n",
    "        nodes_to_remove = find_and_remove_nodes(H, ks)\n",
    "        if not nodes_to_remove:\n",
    "            if tmp:\n",
    "                kshell[ks] = tmp\n",
    "            ks += 1\n",
    "            tmp = []\n",
    "        else:\n",
    "            tmp.extend(nodes_to_remove)\n",
    "        if not H.nodes:\n",
    "            kshell[ks] = tmp\n",
    "            break\n",
    "\n",
    "    weightedks = {}\n",
    "    wks = 1\n",
    "    for ks, value in kshell.items():\n",
    "        if value:\n",
    "            weightedks[wks] = value\n",
    "            wks += 1\n",
    "            \n",
    "    output_wks = {}\n",
    "    for weight, nodes in weightedks.items():\n",
    "        for node in nodes:\n",
    "            output_wks[node] = weight\n",
    "\n",
    "    return output_wks\n",
    "\n",
    "# method calculate effg centrality\n",
    "def method40(G):\n",
    "    def calculate_effective_distance(G):\n",
    "        degree = dict(G.degree())\n",
    "        \n",
    "        # Step 1: Calculate the probability P n|m\n",
    "        probability = {(m, n): (1 / degree[m]) if G.has_edge(m, n) else 0 \n",
    "                       for m in G.nodes for n in G.nodes if m != n}\n",
    "\n",
    "        # Step 2: Calculate the effective distance D_{n|m} for directly connected nodes\n",
    "        effective_distance = {(m, n): 1 - np.log2(p) if p > 0 else float('inf') \n",
    "                              for (m, n), p in probability.items()}\n",
    "\n",
    "        # Step 3: Calculate the effective distance for indirectly connected nodes using the shortest path\n",
    "        all_pairs_shortest_path_length = dict(nx.all_pairs_dijkstra_path_length(G))\n",
    "        for m in G.nodes:\n",
    "            for n in G.nodes:\n",
    "                if m != n:\n",
    "                    shortest_path_length = all_pairs_shortest_path_length[m].get(n, float('inf'))\n",
    "                    if shortest_path_length != float('inf'):\n",
    "                        effective_distance[(m, n)] = shortest_path_length\n",
    "\n",
    "        return effective_distance\n",
    "\n",
    "    def calculate_interaction_scores(G, effective_distance):\n",
    "        interaction_scores = {}\n",
    "        for (i, j), d in effective_distance.items():\n",
    "            k_i = G.degree[i]\n",
    "            k_j = G.degree[j]\n",
    "            interaction_scores[(i, j)] = (k_i * k_j) / (d ** 2)\n",
    "        return interaction_scores\n",
    "\n",
    "    def compute_effg_centrality(G, interaction_scores):\n",
    "        effg_centrality = {node: 0 for node in G.nodes()}\n",
    "        for (i, j), score in interaction_scores.items():\n",
    "            effg_centrality[i] += score\n",
    "            effg_centrality[j] += score\n",
    "        return effg_centrality\n",
    "\n",
    "    # Calculate Effective Distance\n",
    "    effective_distance = calculate_effective_distance(G)\n",
    "\n",
    "    # Calculate Interaction Scores\n",
    "    interaction_scores = calculate_interaction_scores(G, effective_distance)\n",
    "\n",
    "    # Calculate EffG Centrality\n",
    "    effg_centrality = compute_effg_centrality(G, interaction_scores)\n",
    "    \n",
    "    return effg_centrality\n",
    "\n",
    "# method 41 (IS-PEW)\n",
    "def method41(G):\n",
    "    # Precompute expensive metrics\n",
    "    triangles = nx.triangles(G)  # Dictionary {node: triangle_count}\n",
    "    core_numbers = nx.core_number(G)  # Dictionary {node: core_number}\n",
    "    degrees = dict(G.degree())  # Dictionary {node: degree}\n",
    "    \n",
    "    # Compute total triangles once\n",
    "    TC = sum(triangles.values()) // 3 if sum(triangles.values()) > 0 else 1  # Avoid division by zero\n",
    "    \n",
    "    def compute_TP(v_A):\n",
    "        return triangles[v_A] / TC\n",
    "    \n",
    "    def compute_InfE(v_A, v_B):\n",
    "        common_neighbors = set(G[v_A]) & set(G[v_B])\n",
    "        sum_DG_k = sum(degrees[k] for k in common_neighbors)\n",
    "        return (degrees[v_A] * degrees[v_B]) / (1 + sum_DG_k)\n",
    "\n",
    "    def compute_NIP(v_A):\n",
    "        return sum(np.sqrt(degrees[v_a] * core_numbers[v_a]) for v_a in G[v_A])\n",
    "\n",
    "    # Store edge weights in a dictionary to avoid recomputation\n",
    "    edge_weights = {}\n",
    "\n",
    "    def compute_EW(v_A, v_B):\n",
    "        if (v_A, v_B) in edge_weights:\n",
    "            return edge_weights[(v_A, v_B)]\n",
    "        \n",
    "        TP_v_A, TP_v_B = compute_TP(v_A), compute_TP(v_B)\n",
    "        KS_v_A, KS_v_B = core_numbers[v_A], core_numbers[v_B]\n",
    "        NIP_v_A, NIP_v_B = compute_NIP(v_A), compute_NIP(v_B)\n",
    "        ED_v_A_v_B = 1 / compute_InfE(v_A, v_B)\n",
    "        \n",
    "        EW_value = ((KS_v_A * (1 + TP_v_A) * NIP_v_A) / ED_v_A_v_B) + ((KS_v_B * (1 + TP_v_B) * NIP_v_B) / ED_v_A_v_B)\n",
    "        edge_weights[(v_A, v_B)] = EW_value  # Store result for future use\n",
    "        edge_weights[(v_B, v_A)] = EW_value  # Store symmetric value\n",
    "        return EW_value\n",
    "\n",
    "    def compute_IS(v_A):\n",
    "        return sum(compute_EW(v_A, v_B) for v_B in G[v_A])\n",
    "\n",
    "    # Compute potential edge weights once\n",
    "    for u, v in G.edges():\n",
    "        G[u][v]['weight'] = compute_EW(u, v)\n",
    "\n",
    "    # Compute IS values for all nodes\n",
    "    IS_values = {v: compute_IS(v) for v in G.nodes()}\n",
    "    \n",
    "    return IS_values\n",
    "\n",
    "# k-shell iteration Factor (KS-IF)\n",
    "def method42(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    iteration_factors = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        if iteration_nodes:\n",
    "            m = iteration_nodes[-1][1]\n",
    "            for node, n in iteration_nodes:\n",
    "                iteration_factors[node] = k * (1 + n / m)\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    ks_IF = {}\n",
    "    deg = dict(G.degree())\n",
    "    for vi in G.nodes:\n",
    "        ks_IF[vi] = iteration_factors[vi] * deg[vi]\n",
    "        for vj in nx.neighbors(G, vi):\n",
    "            ks_IF[vi] += iteration_factors[vj] * deg[vj]\n",
    "\n",
    "    return ks_IF\n",
    "\n",
    "\n",
    "# Method 43 (DKGM)\n",
    "def method43(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    k_star = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        m = iteration_nodes[-1][1]\n",
    "        for node, n in iteration_nodes:\n",
    "            k_star[node] = k_shell[node] + (n / (m + 1))\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    DK = {}\n",
    "    DKGM = {}\n",
    "\n",
    "    for node in k_shell:\n",
    "        DK[node] = G.degree[node] + k_star[node]\n",
    "\n",
    "    R = 2  # Define the radius for DKGM computation\n",
    "    for node in G.nodes():\n",
    "        DKGM[node] = 0\n",
    "        for neighbor in nx.single_source_shortest_path_length(G, node, cutoff=R):\n",
    "            if node != neighbor:\n",
    "                try:\n",
    "                    d_ij = nx.shortest_path_length(G, source=node, target=neighbor)\n",
    "                    DKGM[node] += DK[node] * DK[neighbor] / d_ij**2\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "    return DKGM\n",
    "\n",
    "#modularity_vitality\n",
    "def method44(G):\n",
    "    \"\"\"Detect communities using Leiden algorithm and compute modularity vitality for each node.\"\"\"\n",
    "    \n",
    "    def detect_communities_leiden(G):\n",
    "        \"\"\"Detect communities using the Leiden algorithm.\n",
    "        \n",
    "        This function converts the NetworkX graph to an iGraph graph while preserving node names,\n",
    "        runs the Leiden algorithm, and returns a dictionary mapping node names to community indices.\n",
    "        \"\"\"\n",
    "        # Convert NetworkX graph to iGraph while preserving node names.\n",
    "        ig_G = ig.Graph.TupleList(G.edges(), directed=False, vertex_name_attr='name')\n",
    "        partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "        communities = {}\n",
    "        # Map each iGraph vertex back to the original NetworkX node name.\n",
    "        for idx, comm in enumerate(partition):\n",
    "            for v in comm:\n",
    "                communities[ig_G.vs[v][\"name\"]] = idx\n",
    "        return communities\n",
    "\n",
    "    def modularity(G, communities):\n",
    "        \"\"\"Compute the modularity of graph G with the given community partition.\"\"\"\n",
    "        M = G.number_of_edges()\n",
    "        Q = 0\n",
    "        for c in set(communities.values()):\n",
    "            nodes_c = [n for n in G.nodes() if communities[n] == c]\n",
    "            subgraph = G.subgraph(nodes_c)\n",
    "            L_c = subgraph.number_of_edges()\n",
    "            k_c = sum(G.degree(n) for n in nodes_c)\n",
    "            Q += (L_c / M) - (k_c / (2 * M)) ** 2\n",
    "        return Q\n",
    "\n",
    "    def compute_hi_c(G, node, communities):\n",
    "        \"\"\"Compute h_i,c values for a node as per Equation (10).\"\"\"\n",
    "        k_i = G.degree(node)\n",
    "        hi_c = {}\n",
    "        for c in set(communities.values()):\n",
    "            # Count links from node to neighbors in community c.\n",
    "            k_i_c = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == c)\n",
    "            hi_c[c] = k_i_c + (k_i if communities[node] == c else 0)\n",
    "        return hi_c\n",
    "\n",
    "    def modularity_vitality(G, communities, node):\n",
    "        \"\"\"Compute the modularity vitality of a node using Equation (9).\"\"\"\n",
    "        Q_initial = modularity(G, communities)\n",
    "        if not G.has_node(node):\n",
    "            return 0  \n",
    "\n",
    "        M = G.number_of_edges()\n",
    "        k_i = G.degree(node)\n",
    "        c_i = communities[node]\n",
    "        \n",
    "        # Compute h_i,c for the given node.\n",
    "        hi_c = compute_hi_c(G, node, communities)\n",
    "        \n",
    "        # Compute the total degree for each community.\n",
    "        d_c = {c: sum(G.degree(n) for n in G.nodes() if communities[n] == c)\n",
    "               for c in set(communities.values())}\n",
    "\n",
    "        # Get nodes in the same community as node.\n",
    "        community_nodes = [n for n in G.nodes() if communities[n] == c_i]\n",
    "        M_internal = G.subgraph(community_nodes).number_of_edges()\n",
    "        k_i_internal = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == c_i)\n",
    "        \n",
    "        # Compute the sum term over all communities.\n",
    "        sum_term = sum((d_c[c] - hi_c[c]) ** 2 for c in hi_c)\n",
    "        \n",
    "        # Compute updated modularity based on Equation (9).\n",
    "        Q_updated = (M_internal - k_i_internal) / (M - k_i) - (1 / (4 * (M - k_i) ** 2)) * sum_term\n",
    "\n",
    "        return Q_initial - Q_updated\n",
    "\n",
    "    # Detect communities using the Leiden algorithm.\n",
    "    communities = detect_communities_leiden(G)\n",
    "\n",
    "    # Compute modularity vitality for each node.\n",
    "    mod_vit = {node: modularity_vitality(G, communities, node) for node in G.nodes()}\n",
    "    \n",
    "    return mod_vit\n",
    "\n",
    "def method45(G, lam=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Global-and-Local Centrality (GLC) of each node in graph G.\n",
    "    \n",
    "    The method first clusters the network using a potential-based approach,\n",
    "    then selects global critical nodes from each cluster, computes local influence\n",
    "    based on k-shell values, and finally calculates the overall GLC value.\n",
    "    \n",
    "    Parameters:\n",
    "        G   : networkx.Graph\n",
    "              The input graph.\n",
    "        lam : float (default=1.0)\n",
    "              The fraction (lambda) of nodes that must be covered by clusters.\n",
    "    \n",
    "    Returns:\n",
    "        GLC : dict\n",
    "              A dictionary mapping each node in G to its GLC centrality value.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Clustering and Global Critical Node Selection ---\n",
    "    clusters = []       # List to hold all clusters\n",
    "    assigned = set()    # Set of nodes that have been assigned or marked\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Helper function: Compute the potential of a node as defined in Eq.(10)\n",
    "    def compute_potential(node):\n",
    "        # Potential of node = degree(node) * (sum over j in N(node) of kin(j))\n",
    "        # where kin(j) = number of neighbors of j that are in {node} U N(node)\n",
    "        nbrs = set(G.neighbors(node))\n",
    "        target_set = nbrs.union({node})\n",
    "        s = 0\n",
    "        for j in nbrs:\n",
    "            s += sum(1 for nbr in G.neighbors(j) if nbr in target_set)\n",
    "        return G.degree(node) * s\n",
    "\n",
    "    # Continue clustering until the number of assigned nodes reaches lam * total_nodes\n",
    "    while len(assigned) < total_nodes * lam:\n",
    "        # Compute potentials for all unassigned nodes.\n",
    "        potentials = {}\n",
    "        for node in G.nodes():\n",
    "            if node not in assigned:\n",
    "                potentials[node] = compute_potential(node)\n",
    "        if not potentials:\n",
    "            break  # All nodes have been assigned\n",
    "        \n",
    "        # Select seed node with maximum potential.\n",
    "        seed = max(potentials, key=potentials.get)\n",
    "        pcmax = potentials[seed]\n",
    "        new_cluster = set([seed])\n",
    "        assigned.add(seed)\n",
    "        \n",
    "        # Include neighbors of the seed with potential >= pcmax/2.\n",
    "        for neighbor in G.neighbors(seed):\n",
    "            if neighbor not in assigned and compute_potential(neighbor) >= pcmax / 2:\n",
    "                new_cluster.add(neighbor)\n",
    "                assigned.add(neighbor)\n",
    "                \n",
    "        # Expand the cluster iteratively (three degrees of influence).\n",
    "        for _ in range(3):\n",
    "            # Gather neighbors of the current cluster (excluding nodes already in the cluster)\n",
    "            cluster_neighbors = set()\n",
    "            for node in new_cluster:\n",
    "                for nbr in G.neighbors(node):\n",
    "                    if nbr not in new_cluster:\n",
    "                        cluster_neighbors.add(nbr)\n",
    "            # Process neighbors in order of increasing degree.\n",
    "            cluster_neighbors = sorted(cluster_neighbors, key=lambda x: G.degree(x))\n",
    "            \n",
    "            added_flag = False\n",
    "            for candidate in cluster_neighbors:\n",
    "                # Count the number of edges from candidate to nodes in new_cluster (kin)\n",
    "                kin = sum(1 for nbr in G.neighbors(candidate) if nbr in new_cluster)\n",
    "                # Count the remaining edges from candidate to nodes outside new_cluster (kout)\n",
    "                kout = sum(1 for nbr in G.neighbors(candidate) if nbr not in new_cluster)\n",
    "                if kin >= kout:\n",
    "                    new_cluster.add(candidate)\n",
    "                    assigned.add(candidate)\n",
    "                    added_flag = True\n",
    "            if not added_flag:\n",
    "                break  # No more nodes can be added in this iteration\n",
    "        \n",
    "        # Mark all neighbors of the new cluster as assigned to avoid re-selection.\n",
    "        for node in new_cluster:\n",
    "            for nbr in G.neighbors(node):\n",
    "                assigned.add(nbr)\n",
    "        clusters.append(new_cluster)\n",
    "    \n",
    "    # From each cluster, select the global critical node (node with highest degree).\n",
    "    global_critical = []\n",
    "    for cluster in clusters:\n",
    "        if cluster:\n",
    "            gc = max(cluster, key=lambda node: G.degree(node))\n",
    "            global_critical.append(gc)\n",
    "    \n",
    "    # --- Step 2: Local Influence Calculation ---\n",
    "    # Compute the k-shell (core) numbers for all nodes.\n",
    "    core_numbers = nx.core_number(G)\n",
    "    \n",
    "    # Compute local influence LI for each node:\n",
    "    # LI(node) = sum_{j in N(node)} k_shell(j)\n",
    "    LI = {}\n",
    "    for node in G.nodes():\n",
    "        li = 0\n",
    "        for nbr in G.neighbors(node):\n",
    "            li += core_numbers[nbr]\n",
    "        LI[node] = li\n",
    "\n",
    "    # --- Step 3: Overall GLC Centrality Calculation ---\n",
    "    # For each node v, compute:\n",
    "    #   GlobalFactor(v) = sum_{u in global_critical} (LI(u) / (2 * max(d(v,u),1)))\n",
    "    #   GLC(v) = LI(v) * GlobalFactor(v)\n",
    "    GLC = {}\n",
    "    for v in G.nodes():\n",
    "        global_factor = 0\n",
    "        for u in global_critical:\n",
    "            try:\n",
    "                d = nx.shortest_path_length(G, source=v, target=u)\n",
    "            except nx.NetworkXNoPath:\n",
    "                d = float('inf')\n",
    "            d = max(d, 1)  # Avoid division by zero\n",
    "            global_factor += LI[u] / (2 * d)\n",
    "        GLC[v] = LI[v] * global_factor\n",
    "\n",
    "    return GLC\n",
    "\n",
    "\n",
    "methods = [globals()[f'method{i}'] for i in range(1, 46)]\n",
    "\n",
    "\n",
    "# Main Code\n",
    "dataset_folder = 'dataset'\n",
    "beta_values = [0.02, 0.05, 0.08, 0.10, 0.13, 0.15, 0.18, 0.20, 0.22, 0.25, 0.30]\n",
    "alpha_values = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "f_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]  # Percentage of nodes for SI calculation\n",
    "\n",
    "for dataset_filename in os.listdir(dataset_folder):\n",
    "    if dataset_filename.endswith('.xlsx'):\n",
    "        dataset_path = os.path.join(dataset_folder, dataset_filename)\n",
    "\n",
    "        # Load edge list from \"Sheet1\"\n",
    "        df_edges = pd.read_excel(dataset_path, sheet_name='Sheet1')\n",
    "        G = nx.from_pandas_edgelist(df_edges, source='source_column', target='target_column')\n",
    "\n",
    "        # Remove self-loops\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "        # Load precomputed SIR values from the \"SIR\" sheet\n",
    "        df_sir = pd.read_excel(dataset_path, sheet_name='SIR')\n",
    "        df_sir.set_index('Node', inplace=True)\n",
    "\n",
    "        results = {}\n",
    "        method_results = {}\n",
    "\n",
    "        # Execute methods once\n",
    "        for i, method in enumerate(methods, start=1):\n",
    "            method_name = f'method{i}'\n",
    "            try:\n",
    "                ranked_nodes, exec_time = measure_execution_time(method, G)\n",
    "                monotonicity = compute_monotonicity(G, ranked_nodes)\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': ranked_nodes,\n",
    "                    'execution_time': exec_time,\n",
    "                    'monotonicity': monotonicity\n",
    "                }\n",
    "            except Exception as e:\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': [],\n",
    "                    'execution_time': None,\n",
    "                    'monotonicity': None,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "\n",
    "        for beta in beta_values:\n",
    "            beta_column = f'Beta_{beta:.2f}'\n",
    "            if beta_column not in df_sir.columns:\n",
    "                print(f\"Warning: {beta_column} not found in {dataset_filename}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Get SIR spread power rankings\n",
    "            spread_power = df_sir[beta_column].to_dict()\n",
    "            sigma = sorted(spread_power.keys(), key=lambda x: spread_power[x], reverse=True)\n",
    "\n",
    "            for method_name, method_data in method_results.items():\n",
    "                if 'error' in method_data:\n",
    "                    results[(method_name, beta)] = {\n",
    "                        'Beta': beta,\n",
    "                        'Kendall Tau': None,\n",
    "                        'P-Value': None,\n",
    "                        **{f'RBO {alpha}': None for alpha in alpha_values},\n",
    "                        **{f'SI {f:.2f}': None for f in f_values},\n",
    "                        'error': method_data['error']\n",
    "                    }\n",
    "                    continue\n",
    "\n",
    "                ranked_nodes = method_data['ranked_nodes']\n",
    "                tau, p_value = kendalltau(\n",
    "                    [spread_power[node] for node in sigma],\n",
    "                    [spread_power[node] for node, _ in ranked_nodes]\n",
    "                )\n",
    "                R = [node for node, _ in ranked_nodes]\n",
    "                \n",
    "                rbo_scores = {f'RBO {alpha}': compute_rbo(sigma, R, alpha) for alpha in alpha_values}\n",
    "\n",
    "                # Compute Spread Impact (SI) for f = [0.01, ..., 0.1]\n",
    "                n = len(G.nodes())  # Total number of nodes\n",
    "                si_scores = {}\n",
    "                for f in f_values:\n",
    "                    top_f_nodes = [node for node, _ in ranked_nodes[:int(f * n)]]  # Top f  n nodes\n",
    "                    si_scores[f'SI {f:.2f}'] = sum(spread_power[node] for node in top_f_nodes) / (f * n)\n",
    "\n",
    "                results[(method_name, beta)] = {\n",
    "                    'Beta': beta,\n",
    "                    'Kendall Tau': tau,\n",
    "                    'P-Value': p_value,\n",
    "                    **rbo_scores,\n",
    "                    **si_scores  # Include SI values\n",
    "                }\n",
    "\n",
    "        # Create DataFrame for results\n",
    "        data = []\n",
    "        for (method_name, beta), result in results.items():\n",
    "            row = [\n",
    "                method_name,\n",
    "                method_results[method_name]['execution_time'],\n",
    "                method_results[method_name].get('monotonicity', 'N/A'),\n",
    "                result.get('Beta'),\n",
    "                result.get('Kendall Tau'),\n",
    "                result.get('P-Value')\n",
    "            ] + [result.get(f'RBO {alpha}') for alpha in alpha_values] + \\\n",
    "                [result.get(f'SI {f:.2f}') for f in f_values]  # Include SI values in the output\n",
    "\n",
    "            if 'error' in result:\n",
    "                row.append(result['error'])\n",
    "            else:\n",
    "                row.append('')\n",
    "\n",
    "            data.append(row)\n",
    "\n",
    "        expected_columns = ['Method', 'Execution Time', 'Monotonicity', 'Beta', 'Kendall Tau', 'P-Value'] + \\\n",
    "                           [f'RBO {alpha}' for alpha in alpha_values] + \\\n",
    "                           [f'SI {f:.2f}' for f in f_values] + ['Error']\n",
    "\n",
    "        result_df = pd.DataFrame(data, columns=expected_columns)\n",
    "\n",
    "        # Write results to Excel file\n",
    "        dataset_name = os.path.splitext(dataset_filename)[0]\n",
    "        output_file = f'{dataset_name}_results.xlsx'\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df_edges.to_excel(writer, sheet_name='Sheet1', index=False)  # Save edge list\n",
    "            df_sir.to_excel(writer, sheet_name='SIR')  # Keep original SIR values\n",
    "            result_df.to_excel(writer, sheet_name='Results', index=False)  # Save computed results\n",
    "\n",
    "        print(f\"Processed {dataset_filename}, results saved in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfcb4e-f2f2-4614-8bd3-990abb6c6446",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7f5885-c59a-4521-8f67-9c4986d6e488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Contiguous(SIR).xlsx, results saved in Contiguous(SIR)_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "def measure_execution_time(method, G):\n",
    "    start_time = time.time()\n",
    "    result = method(G)\n",
    "    ranked_nodes = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return ranked_nodes, execution_time\n",
    "\n",
    "def compute_monotonicity(G, ranked_nodes):\n",
    "    ranks = [score for node, score in ranked_nodes]\n",
    "    unique_ranks = list(set(ranks))\n",
    "    n = G.number_of_nodes()\n",
    "    nr_dict = {rank: ranks.count(rank) for rank in unique_ranks}\n",
    "    nr_sum = sum(nr * (nr - 1) for nr in nr_dict.values())\n",
    "    M = (1 - nr_sum / (n * (n - 1))) ** 2\n",
    "    return M\n",
    "\n",
    "def compute_rbo(sigma, R, alpha=0.9):\n",
    "    \"\"\"Compute the Rank-Biased Overlap (RBO) between two ranking lists.\"\"\"\n",
    "    def A(sigma, R, f):\n",
    "        sigma_f = set(sigma[:f])\n",
    "        R_f = set(R[:f])\n",
    "        if len(sigma_f) == 0 and len(R_f) == 0:\n",
    "            return 1\n",
    "        return len(sigma_f & R_f) / len(sigma_f | R_f)\n",
    "\n",
    "    n = max(len(sigma), len(R))\n",
    "    rbo = (1 - alpha) * sum(alpha**(f - 1) * A(sigma, R, f) for f in range(1, n + 1))\n",
    "    return rbo\n",
    "\n",
    "def compute_jaccard_similarity(set1, set2):\n",
    "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0\n",
    "\n",
    "# Define methods\n",
    "def method1(G):\n",
    "    return nx.degree_centrality(G)\n",
    "\n",
    "def method2(G):\n",
    "    return nx.betweenness_centrality(G)\n",
    "\n",
    "def method3(G):\n",
    "    return nx.closeness_centrality(G)\n",
    "\n",
    "def method4(G):\n",
    "    return nx.core_number(G)\n",
    "\n",
    "def method5(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = {vi: sum(ks[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc\n",
    "\n",
    "def method6(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = method5(G)\n",
    "    cnc_plus = {vi: sum(cnc[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc_plus\n",
    "\n",
    "def method7(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_max_set = {node for node, k in ks.items() if k == ks_max}\n",
    "    teta = {}\n",
    "    for vi in G.nodes():\n",
    "        teta_vi = (ks_max - ks[vi] + 1)\n",
    "        total_dis = 0\n",
    "        for vj in ks_max_set:\n",
    "            if nx.has_path(G, vi, vj):\n",
    "                total_dis += nx.shortest_path_length(G, vi, vj)\n",
    "        teta[vi] = teta_vi * total_dis\n",
    "    return teta\n",
    "\n",
    "def method8(G):\n",
    "    ks = nx.core_number(G)\n",
    "    deg = nx.degree_centrality(G)\n",
    "    bet = nx.betweenness_centrality(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_norm = {node: ks[node] / ks_max for node in G.nodes()}\n",
    "    all_around = {v: math.sqrt(ks_norm[v]**2 + bet[v]**2 + deg[v]**2) for v in G.nodes()}\n",
    "    return all_around\n",
    "\n",
    "def method9(G):\n",
    "    h_index = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degrees = sorted([G.degree(neighbor) for neighbor in neighbors], reverse=True)\n",
    "        h = 0\n",
    "        for i, degree in enumerate(degrees):\n",
    "            if degree >= i + 1:\n",
    "                h = i + 1\n",
    "            else:\n",
    "                break\n",
    "        h_index[node] = h\n",
    "    return h_index\n",
    "\n",
    "def method10(G):\n",
    "    HI = method9(G)\n",
    "    LHI = {vi: HI[vi] + sum(HI[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return LHI\n",
    "\n",
    "def method11(G):\n",
    "    nv = {}\n",
    "    for vi in G.nodes():\n",
    "        set1 = set()\n",
    "        for vj in G.neighbors(vi):\n",
    "            set1.add(vj)\n",
    "            for vk in G.neighbors(vj):\n",
    "                set1.add(vk)\n",
    "        nv[vi] = len(set1) - 1\n",
    "    \n",
    "    c_local = {}\n",
    "    for vi in G.nodes():\n",
    "        total1 = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            total2 = 0\n",
    "            for vk in G.neighbors(vj):\n",
    "                total2 += nv[vk]\n",
    "            total1 += total2\n",
    "        c_local[vi] = total1\n",
    "    return c_local\n",
    "\n",
    "def method12(G):\n",
    "    SLC = {}\n",
    "    k = dict(nx.degree(G))\n",
    "    C = nx.clustering(G)\n",
    "    for i in G.nodes():\n",
    "        SLC[i] = sum(C[j] for j in G.neighbors(i))\n",
    "    \n",
    "    DNC = {}\n",
    "    alpha = 1\n",
    "    for i in G.nodes():\n",
    "        DNC[i] = k[i] + alpha * SLC[i]\n",
    "    return DNC\n",
    "\n",
    "def method13(G):\n",
    "    CLC = nx.clustering(G)\n",
    "    CLGC = {}\n",
    "    for i in G.nodes():\n",
    "        CLGC[i] = sum(\n",
    "            math.sqrt(CLC[j]) / nx.shortest_path_length(G, i, j)\n",
    "            for j in G.nodes()\n",
    "            if i != j and nx.has_path(G, i, j)\n",
    "        ) * CLC[i]\n",
    "    \n",
    "    ECLGC = {}\n",
    "    for v in G.nodes():\n",
    "        sum1 = sum2 = sum3 = 0\n",
    "        neighbors_v = list(G.neighbors(v))\n",
    "        for u in G.nodes():\n",
    "            if u != v and nx.has_path(G, v, u):\n",
    "                neighbors_u = list(G.neighbors(u))\n",
    "                lu = len(neighbors_u)\n",
    "                sum1 += CLC[u] / len(neighbors_v)\n",
    "                sum2 = sum(CLC[j] / lu for j in neighbors_u)\n",
    "                sum3 += math.sqrt(CLC[u] + sum2) / nx.shortest_path_length(G, v, u)\n",
    "        ECLGC[v] = sum1 * sum3\n",
    "    return ECLGC\n",
    "\n",
    "def compute_alpha(graph):\n",
    "    size_of_V = graph.number_of_nodes()\n",
    "    order_of_magnitude = int(math.log10(size_of_V))\n",
    "    alpha = size_of_V / order_of_magnitude\n",
    "    return alpha\n",
    "\n",
    "def method14(G, m=1):\n",
    "    alpha = compute_alpha(G)\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    kshell = nx.core_number(G)\n",
    "    \n",
    "    centrality = {}\n",
    "    \n",
    "    for i in G.nodes:\n",
    "        cenC_i = 0\n",
    "        \n",
    "        # Get m-neighborhood of node i\n",
    "        m_neighborhood = set(nx.single_source_shortest_path_length(G, i, cutoff=m).keys())\n",
    "        m_neighborhood.remove(i)  # Exclude the node itself\n",
    "        \n",
    "        for j in m_neighborhood:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij == 0:\n",
    "                continue\n",
    "            DC_i = degree_centrality[i]\n",
    "            C_i = clustering_coeff[i]\n",
    "            k_s_i = kshell[i]\n",
    "            k_s_j = kshell[j]\n",
    "            \n",
    "            term1 = (math.exp(alpha) * DC_i + (1 / C_i if C_i != 0 else 0))\n",
    "            term2 = (k_s_j / (abs(k_s_i - k_s_j) + 1))\n",
    "            \n",
    "            cenC_i += (4 * math.pi**2) * (term1 / (d_ij**2)) * term2\n",
    "        \n",
    "        centrality[i] = cenC_i\n",
    "    \n",
    "    return centrality\n",
    "\n",
    "def method15(G):\n",
    "    clustering_coeffs = nx.clustering(G)\n",
    "    cluster_rank_scores = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    degrees = dict(G.degree())\n",
    "\n",
    "    # Calculate ClusterRank scores\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(degrees[neighbor] + 1 for neighbor in neighbors)\n",
    "        cluster_rank_scores[node] = 10 ** (-clustering_coeffs[node]) * sum_term\n",
    "\n",
    "    return cluster_rank_scores\n",
    "\n",
    "# Optimized methods\n",
    "def get_neighborhood(G, node, radius):\n",
    "    neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "    neighborhood.pop(node, None)\n",
    "    return list(neighborhood.keys())\n",
    "\n",
    "# Improved Gravity Centrality\n",
    "def method16(G):\n",
    "    IGC = {}\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        IGC[i] = 0\n",
    "        for j in G.neighbors(i):\n",
    "            neighborhood = get_neighborhood(G, j, 2)\n",
    "            for p in neighborhood:\n",
    "                IGC[i] += (ks[j] * k[p]) / (nx.shortest_path_length(G, j, p)**2)\n",
    "    return IGC\n",
    "\n",
    "def k_shell(graph):\n",
    "    return nx.core_number(graph)\n",
    "\n",
    "def evaluate_power_k_shell(k_si, k_sj):\n",
    "    return math.sqrt(k_si + k_sj)\n",
    "\n",
    "def compute_P_ji(graph, node_i, node_j):\n",
    "    try:\n",
    "        a_ji = graph[node_j][node_i]['weight'] if 'weight' in graph[node_j][node_i] else 1\n",
    "        total_a_ji = sum(graph[node_j][nbr]['weight'] if 'weight' in graph[node_j][nbr] else 1 for nbr in graph.neighbors(node_j))\n",
    "        return a_ji / total_a_ji\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def evaluate_ED(graph, node_i, node_j):\n",
    "    P_ji = compute_P_ji(graph, node_i, node_j)\n",
    "    if P_ji == 0:\n",
    "        return 1  # Prevent log(0)\n",
    "    return max(1 - math.log(P_ji), 1)\n",
    "\n",
    "def evaluate_influence(graph, node_i, node_j, k_shell_values):\n",
    "    alpha_ij = evaluate_power_k_shell(k_shell_values[node_i], k_shell_values[node_j])\n",
    "    ED_ij = evaluate_ED(graph, node_i, node_j)\n",
    "    degree_vi = graph.degree(node_i)\n",
    "    return alpha_ij * degree_vi / ED_ij\n",
    "\n",
    "# EDBC\n",
    "def method17(G):\n",
    "    k_shell_values = k_shell(G)\n",
    "    EDBC = {u: 0 for u in G.nodes()}\n",
    "    \n",
    "    for u in G.nodes():\n",
    "        for v in G.neighbors(u):\n",
    "            for neighbor_u in G.neighbors(u):\n",
    "                EDBC[v] += evaluate_influence(G, v, neighbor_u, k_shell_values)\n",
    "    \n",
    "    return EDBC\n",
    "\n",
    "def method18(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    entropy_centrality = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if len(neighbors) == 0:\n",
    "            entropy_centrality[node] = 0\n",
    "        else:\n",
    "            sum_degrees = sum(G.degree(neighbor) for neighbor in neighbors)\n",
    "            sum_centrality = sum(degree_centrality[neighbor] for neighbor in neighbors)\n",
    "            if sum_centrality == 0:\n",
    "                entropy_centrality[node] = 0\n",
    "            else:\n",
    "                entropy_centrality[node] = closeness_centrality[node] * (sum_degrees / sum_centrality)\n",
    "    \n",
    "    return entropy_centrality\n",
    "\n",
    "# KNC\n",
    "def method19(G):\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    degree_values = dict(G.degree())\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    \n",
    "    alpha = compute_alpha(G)\n",
    "    KNC = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(k_shell_values[neighbor] * degree_values[neighbor] * clustering_coeff[neighbor] for neighbor in neighbors)\n",
    "        KNC[node] = alpha * sum_term\n",
    "    \n",
    "    return KNC\n",
    "\n",
    "# NEDC\n",
    "def method20(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    \n",
    "    nodes = G.nodes()\n",
    "    NEDC = {}\n",
    "    \n",
    "    for node in nodes:\n",
    "        term1 = degree_centrality[node] * betweenness_centrality[node]\n",
    "        term2 = closeness_centrality[node] * k_shell_values[node]\n",
    "        NEDC[node] = term1 + term2\n",
    "    \n",
    "    return NEDC\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "# Method 21 (LGC)\n",
    "def method21(G):\n",
    "    LC = {}\n",
    "    K = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        LC[i] = sum(K[j] for j in nx.neighbors(G, i)) * 2 + K[i] ** 2 + K[i]\n",
    "\n",
    "    LGC = {}\n",
    "    radius = 3\n",
    "    for vi in G.nodes():\n",
    "        LGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            try:\n",
    "                d_ij = nx.shortest_path_length(G, vi, vj)\n",
    "                if d_ij != 0:\n",
    "                    LGC[vi] += LC[vi] * LC[vj] / d_ij ** 2\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    return LGC\n",
    "\n",
    "# Method 22 (SEGM)\n",
    "def method22(G):\n",
    "    k = dict(G.degree())\n",
    "    I = dict()\n",
    "    E = dict()\n",
    "    SE = dict()\n",
    "\n",
    "    for node in G.nodes:\n",
    "        I[node] = 0\n",
    "\n",
    "    for i in G.nodes:\n",
    "        sum_kj = 0\n",
    "        for j in G.neighbors(i):\n",
    "            sum_kj += k[j]\n",
    "        if sum_kj != 0:\n",
    "            I[i] = k[i] / sum_kj\n",
    "        sum_e = 0\n",
    "        for j in G.neighbors(i):\n",
    "            if I[j] != 0:\n",
    "                sum_e += I[j] * np.log(I[j])\n",
    "        E[i] = -1 * sum_e\n",
    "        SE[i] = np.exp(E[i]) * k[i]\n",
    "\n",
    "    SEGM = dict()\n",
    "    R = 3\n",
    "    for i in G.nodes:\n",
    "        SEGM[i] = 0\n",
    "        neighbors = get_neighborhood(G, i, R)\n",
    "        for j in neighbors:\n",
    "            if nx.shortest_path_length(G, i, j) != 0:\n",
    "                SEGM[i] += (SE[i] * SE[j]) / (nx.shortest_path_length(G, i, j) ** 2)\n",
    "\n",
    "    return SEGM\n",
    "\n",
    "# Method 23 (MCGM)\n",
    "def method23(G):\n",
    "    k = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    x = nx.eigenvector_centrality(G)\n",
    "\n",
    "    k_values = np.array(list(k.values()))\n",
    "    ks_values = np.array(list(ks.values()))\n",
    "    x_values = np.array(list(x.values()))\n",
    "\n",
    "    k_mid = np.median(k_values)\n",
    "    ks_mid = np.median(ks_values)\n",
    "    x_mid = np.median(x_values)\n",
    "\n",
    "    k_max = max(k_values)\n",
    "    ks_max = max(ks_values)\n",
    "    x_max = max(x_values)\n",
    "\n",
    "    alpha = max(k_mid / k_max, x_mid / x_max) / (ks_mid / ks_max)\n",
    "\n",
    "    MCGM = dict()\n",
    "    R = 3\n",
    "\n",
    "    for i in G.nodes():\n",
    "        MCGM[i] = 0\n",
    "        for j in G.nodes():\n",
    "            if i != j and nx.shortest_path_length(G, source=i, target=j) <= R:\n",
    "                MCGM[i] += ((k[i] / k_max + alpha * ks[i] / ks_max + x[i] / x_max) *\n",
    "                            (k[j] / k_max + alpha * ks[j] / ks_max + x[j] / x_max) /\n",
    "                            (nx.shortest_path_length(G, source=i, target=j) ** 2))\n",
    "\n",
    "    return MCGM\n",
    "\n",
    "# Method 24 (HVGC)\n",
    "def compute_H_index(G, i):\n",
    "    degrees = [G.degree(neighbor) for neighbor in G.neighbors(i)]\n",
    "    degrees.sort(reverse=True)\n",
    "    H_index = 0\n",
    "    for idx, degree in enumerate(degrees):\n",
    "        if degree >= idx + 1:\n",
    "            H_index = idx + 1\n",
    "        else:\n",
    "            break\n",
    "    return H_index\n",
    "\n",
    "def compute_H_v(G, i):\n",
    "    H_i = compute_H_index(G, i)\n",
    "    H_v_i = sum(G.degree(j) for j in G.neighbors(i) if G.degree(j) >= H_i)\n",
    "    return H_v_i\n",
    "\n",
    "def compute_c_i(G, i):\n",
    "    def compute_p_ij(G, i, j):\n",
    "        neighbors_i = list(G.neighbors(i))\n",
    "        sum_z_iw = sum(G[i][w].get('weight', 1) for w in neighbors_i)\n",
    "        p_ij = G[i][j].get('weight', 1) / sum_z_iw if sum_z_iw != 0 else 0\n",
    "        return p_ij\n",
    "\n",
    "    c_i = 0\n",
    "    neighbors_i = list(G.neighbors(i))\n",
    "    for j in neighbors_i:\n",
    "        p_ij = compute_p_ij(G, i, j)\n",
    "        neighbors_j = list(G.neighbors(j))\n",
    "        common_neighbors = set(neighbors_i).intersection(neighbors_j)\n",
    "        sum_piw_pwj = sum(compute_p_ij(G, i, w) * compute_p_ij(G, w, j) for w in common_neighbors)\n",
    "        term = p_ij + sum_piw_pwj\n",
    "        c_i += term ** 2\n",
    "    \n",
    "    return c_i\n",
    "\n",
    "def compute_HVGC(G, i, H_v, R):\n",
    "    c_i = compute_c_i(G, i)\n",
    "    HVGC_i = 0\n",
    "    for j in G.nodes():\n",
    "        if j != i:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij <= R:\n",
    "                HVGC_i += (math.exp(-c_i) * H_v[i] * H_v[j]) / (d_ij ** 2)\n",
    "    return HVGC_i\n",
    "\n",
    "#HVGC\n",
    "def method24(G):\n",
    "    H_v = {i: compute_H_v(G, i) for i in G.nodes()}\n",
    "    R = 2\n",
    "    HVGC = {node: compute_HVGC(G, node, H_v, R) for node in G.nodes()}\n",
    "    return HVGC\n",
    "\n",
    "# Method 25 (MDD)\n",
    "def method25(G):\n",
    "    def compute_mixed_degrees(H, landa):\n",
    "        km = {}\n",
    "        for v in H.nodes():\n",
    "            kr = H.degree(v)\n",
    "            ke = G.degree(v) - kr\n",
    "            km[v] = kr + landa * ke\n",
    "        return km\n",
    "\n",
    "    H = G.copy()\n",
    "    landa = 0.7\n",
    "    rank = {}\n",
    "    km = {v: 1 for v in G.nodes()}  # Initialize km values\n",
    "\n",
    "    while H.number_of_nodes() > 0:\n",
    "        km = compute_mixed_degrees(H, landa)\n",
    "        min_km_value = min(km.values())\n",
    "\n",
    "        # Find and remove nodes with mixed degree <= min_km_value\n",
    "        node_set = [v for v in H.nodes() if km[v] == min_km_value]\n",
    "        for each in node_set:\n",
    "            rank[each] = km[each]\n",
    "            H.remove_node(each)\n",
    "\n",
    "    return rank\n",
    "\n",
    "# Method 26 (BaseGM)\n",
    "def method26(G):\n",
    "    def get_neighborhood(G, node, radius):\n",
    "        neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "        neighborhood.pop(node, None)\n",
    "        return list(neighborhood.keys())\n",
    "\n",
    "    gravity = dict()\n",
    "    kshell = nx.core_number(G)\n",
    "    for vi in G.nodes():\n",
    "        gravity[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, 3)\n",
    "        for vj in neighbors:\n",
    "            gravity[vi] += ((kshell[vi] * kshell[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    gravity_plus = dict()\n",
    "    for vi in G.nodes():\n",
    "        gravity_plus[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            gravity_plus[vi] += gravity[vj]\n",
    "\n",
    "    return gravity_plus\n",
    "\n",
    "# Method 27 (GlobalGM)\n",
    "def method27(G):\n",
    "    gm = dict()\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        gm[vi] = 0\n",
    "        for vj in G.nodes():\n",
    "            if vj != vi:\n",
    "                gm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gm\n",
    "\n",
    "# Method 28 (LocalGM)\n",
    "def method28(G):\n",
    "    lgm = dict()\n",
    "    radius = 3\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        lgm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                lgm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return lgm\n",
    "\n",
    "# Method 29 (WGravity)\n",
    "def method29(G):\n",
    "    gmm = dict()\n",
    "    ev = nx.eigenvector_centrality(G)\n",
    "    k = dict(nx.degree(G))\n",
    "\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        gmm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                gmm[vi] += (ev[vi] * (k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gmm\n",
    "\n",
    "\n",
    "# Method 30 (HKS)\n",
    "def method30(G):\n",
    "    def set_b_values(G):\n",
    "        b, Shell, b_values = 1, 1, {}\n",
    "        while G.number_of_nodes() > 0:\n",
    "            flag = False\n",
    "            for v in list(G.nodes):\n",
    "                if G.degree[v] <= Shell:\n",
    "                    b_values[v] = b\n",
    "                    flag = True\n",
    "            \n",
    "            if flag:\n",
    "                G.remove_nodes_from([v for v in list(G.nodes) if b_values.get(v) == b])\n",
    "                b += 1\n",
    "            else:\n",
    "                Shell += 1\n",
    "        \n",
    "        return b_values\n",
    "\n",
    "    def set_f(G, b_values):\n",
    "        V, f = list(G.nodes), max(b_values.values())\n",
    "        fi = {v: b_values[v] if all(b_values[v] >= b_values[vj] for vj in G.neighbors(v)) else 0 for v in V}\n",
    "\n",
    "        while V:\n",
    "            for vi in V:\n",
    "                if fi[vi] == f:\n",
    "                    for vj in G.neighbors(vi):\n",
    "                        if fi[vi] - 1 > fi[vj]:\n",
    "                            fi[vj] = fi[vi] - 1\n",
    "            V.remove(vi)\n",
    "\n",
    "        return fi\n",
    "\n",
    "    b_values = set_b_values(G.copy())\n",
    "    fi_values = set_f(G.copy(), b_values)\n",
    "\n",
    "    S = {vi: sum(G.degree[vj] * (b_values[vj] + fi_values[vj]) for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    HKS = {vi: sum(S[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return HKS\n",
    "\n",
    "# Method 31 (SHKS)\n",
    "def method31(G):\n",
    "    def compute_efficiency(G):\n",
    "        n = len(G)\n",
    "        if n <= 1:\n",
    "            return 0\n",
    "        inv_distances = (1 / d for u, v_dict in nx.shortest_path_length(G) for v, d in v_dict.items() if d > 0)\n",
    "        return sum(inv_distances) / (n * (n - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_copy = G.copy()\n",
    "            G_copy.remove_node(k)\n",
    "            centrality[k] = (efficiency_G - compute_efficiency(G_copy)) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    def network_constrained_coefficient(G, v):\n",
    "        neighbors = list(G.neighbors(v))\n",
    "        coefficient = sum(calculate_pvu(G, v, u) + sum((calculate_pvu(G, v, w) * calculate_pvu(G, w, u)) ** 2 for w in set(G.neighbors(u)) & set(G.neighbors(v))) for u in neighbors)\n",
    "        return coefficient\n",
    "\n",
    "    def calculate_pvu(G, v, u):\n",
    "        return (1 if u in G.neighbors(v) else 0) / G.degree(v)\n",
    "\n",
    "    efficiency_centrality = compute_efficiency_centrality(G)\n",
    "    Constraint_Coef = {node: network_constrained_coefficient(G, node) for node in G.nodes()}\n",
    "    sh = {node: 1 / Constraint_Coef[node] for node in G.nodes()}\n",
    "    ks = nx.core_number(G)\n",
    "    alpha = 0.2\n",
    "    I = {node: alpha * sh[node] + ks[node] for node in G.nodes()}\n",
    "    C = {v: sum(I[v] + I[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    IS = {v: sum(C[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    SHKS = {v: sum(IS[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    return SHKS\n",
    "\n",
    "# Method 32 (KSGC)\n",
    "def method32(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_min = min(ks.values())\n",
    "    k = dict(G.degree())\n",
    "    n = G.number_of_nodes()\n",
    "    C = np.zeros((n, n))\n",
    "    F = np.zeros((n, n))\n",
    "    nodes = list(G.nodes())\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                vi, vj = nodes[i], nodes[j]\n",
    "                try:\n",
    "                    C[i, j] = np.exp((ks[vi] - ks[vj]) / (ks_max - ks_min))\n",
    "                    F[i, j] = C[i, j] * (k[vi] * k[vj] / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    F[i, j] = 0\n",
    "\n",
    "    KSGC = dict()\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        vi_index = node_index[vi]\n",
    "        KSGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            vj_index = node_index[vj]\n",
    "            KSGC[vi] += F[vi_index, vj_index]\n",
    "    \n",
    "    return KSGC\n",
    "\n",
    "# Method 33 (EFFC)\n",
    "def method33(G):\n",
    "    def compute_efficiency(G):\n",
    "        N = len(G.nodes)\n",
    "        if N < 2:\n",
    "            return 0\n",
    "        efficiency_sum = 0\n",
    "        for i in G.nodes:\n",
    "            for j in G.nodes:\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        shortest_path_length = nx.shortest_path_length(G, source=i, target=j)\n",
    "                        efficiency_sum += 1 / shortest_path_length\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        efficiency_sum += 0  # Adding 0 for disconnected pairs\n",
    "        return efficiency_sum / (N * (N - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_k_prime = G.copy()\n",
    "            G_k_prime.remove_node(k)\n",
    "            efficiency_G_k_prime = compute_efficiency(G_k_prime)\n",
    "            centrality[k] = (efficiency_G - efficiency_G_k_prime) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    return compute_efficiency_centrality(G)\n",
    "\n",
    "# Method 34 (Local Relative ASP)\n",
    "def method34(G):\n",
    "    def find_diameter(G):\n",
    "        \"\"\"\n",
    "        Find the diameter (longest shortest path) of the graph G.\n",
    "        \"\"\"\n",
    "        if nx.is_connected(G):\n",
    "            return nx.diameter(G)\n",
    "        else:\n",
    "            lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "            diameter = 0\n",
    "            for u in lengths:\n",
    "                for v in lengths[u]:\n",
    "                    if lengths[u][v] > diameter:\n",
    "                        diameter = lengths[u][v]\n",
    "            return diameter\n",
    "\n",
    "    def calculate_asp(G):\n",
    "        \"\"\"\n",
    "        Calculate the Average Shortest Path (ASP) for the graph G.\n",
    "        \"\"\"\n",
    "        n = len(G.nodes)\n",
    "        if n < 2:\n",
    "            return 0\n",
    "        \n",
    "        if nx.is_connected(G):\n",
    "            return nx.average_shortest_path_length(G)\n",
    "        \n",
    "        diameter = find_diameter(G)\n",
    "        asp_sum = 0\n",
    "        \n",
    "        lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "        for u in G.nodes:\n",
    "            for v in G.nodes:\n",
    "                if u != v:\n",
    "                    asp_sum += lengths[u].get(v, diameter)\n",
    "        \n",
    "        return asp_sum / (n * (n - 1))\n",
    "\n",
    "    asp = calculate_asp(G)\n",
    "    AC = {}\n",
    "\n",
    "    for k in G.nodes():\n",
    "        G_removed = G.copy()\n",
    "        G_removed.remove_node(k)\n",
    "        asp_removed = calculate_asp(G_removed)\n",
    "        AC[k] = abs(asp_removed - asp) / asp if asp > 0 else 0\n",
    "\n",
    "    return AC\n",
    "\n",
    "def k_neighbors(G, node, radius):\n",
    "        neighbors = set(nx.single_source_shortest_path_length(G, node, cutoff=radius).keys())\n",
    "        neighbors.discard(node)  # Remove the node itself if present\n",
    "        return neighbors\n",
    "    \n",
    "# Method 35 (InformationRank)\n",
    "def method35(G):\n",
    "    \n",
    "    PROPA = {}\n",
    "    Score = {}\n",
    "    L = 2\n",
    "    Miu = 0.2\n",
    "\n",
    "    # Initialize PROPA and Score dictionaries\n",
    "    for v in G.nodes():\n",
    "        PROPA[v] = {}\n",
    "        Score[v] = 0\n",
    "\n",
    "    # Calculate PROPA and Score\n",
    "    for v in G.nodes():\n",
    "        neighbors = k_neighbors(G, v, L)\n",
    "        for w in neighbors:\n",
    "            PROPA[v][w] = 1\n",
    "            path_length_counts = {}\n",
    "            for path in nx.all_simple_paths(G, v, w, cutoff=L):\n",
    "                length = len(path) - 1\n",
    "                path_length_counts[length] = path_length_counts.get(length, 0) + 1\n",
    "\n",
    "            for l in range(1, L + 1):\n",
    "                pow_val = path_length_counts.get(l, 0)\n",
    "                PROPA[v][w] *= (1 - Miu ** l) ** pow_val\n",
    "            PROPA[v][w] = 1 - PROPA[v][w]\n",
    "            Score[v] += PROPA[v][w]\n",
    "\n",
    "    return Score\n",
    "\n",
    "\n",
    "# Method 36 (Weighted K-shell)\n",
    "def method36(G):\n",
    "    c1 = 0.1\n",
    "    c2 = 0.4\n",
    "    deg = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    ksdw = dict()\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksdw[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            ksdw[vi] += (c1 * deg[vi] + c2 * ks[vi]) * (c1 * deg[vj] + c2 * ks[vj])\n",
    "\n",
    "    return ksdw\n",
    "\n",
    "# Method 37 (HybridKshell)\n",
    "def method37(G):\n",
    "    radius = 2\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(G.degree())\n",
    "    ksh = dict()\n",
    "    landa = 0.4\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksh[vi] = 0\n",
    "        neighbors = k_neighbors(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            ksh[vi] += (math.sqrt(ks[vi] + ks[vj]) + landa * k[vj]) / (nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    return ksh\n",
    "\n",
    "# Optimized method 38 (Social Capital)\n",
    "def method38(G):\n",
    "    SC = {}\n",
    "    K = dict(G.degree())\n",
    "    for i in G.nodes:\n",
    "        SC[i] = K[i] + sum(K[j] for j in G.neighbors(i))\n",
    "    return SC\n",
    "\n",
    "# Optimized method 39 (Potential Edge Weight)\n",
    "def method39(G):\n",
    "    KW = {}\n",
    "    k = dict(G.degree())\n",
    "    Landa = 0.5\n",
    "\n",
    "    for vi in G.nodes:\n",
    "        KW[vi] = Landa * k[vi] + sum((1 - Landa) * (k[vi] + k[vj]) for vj in G.neighbors(vi))\n",
    "        KW[vi] = int(KW[vi])\n",
    "\n",
    "    def find_and_remove_nodes(H, ks):\n",
    "        nodes_to_remove = [node for node in list(H.nodes) if KW[node] <= ks]\n",
    "        H.remove_nodes_from(nodes_to_remove)\n",
    "        return nodes_to_remove\n",
    "\n",
    "    H = G.copy()\n",
    "    ks = min(KW.values())\n",
    "    kshell = {}\n",
    "    tmp = []\n",
    "\n",
    "    while H.nodes:\n",
    "        nodes_to_remove = find_and_remove_nodes(H, ks)\n",
    "        if not nodes_to_remove:\n",
    "            if tmp:\n",
    "                kshell[ks] = tmp\n",
    "            ks += 1\n",
    "            tmp = []\n",
    "        else:\n",
    "            tmp.extend(nodes_to_remove)\n",
    "        if not H.nodes:\n",
    "            kshell[ks] = tmp\n",
    "            break\n",
    "\n",
    "    weightedks = {}\n",
    "    wks = 1\n",
    "    for ks, value in kshell.items():\n",
    "        if value:\n",
    "            weightedks[wks] = value\n",
    "            wks += 1\n",
    "            \n",
    "    output_wks = {}\n",
    "    for weight, nodes in weightedks.items():\n",
    "        for node in nodes:\n",
    "            output_wks[node] = weight\n",
    "\n",
    "    return output_wks\n",
    "\n",
    "# method calculate effg centrality\n",
    "def method40(G):\n",
    "    def calculate_effective_distance(G):\n",
    "        degree = dict(G.degree())\n",
    "        \n",
    "        # Step 1: Calculate the probability P n|m\n",
    "        probability = {(m, n): (1 / degree[m]) if G.has_edge(m, n) else 0 \n",
    "                       for m in G.nodes for n in G.nodes if m != n}\n",
    "\n",
    "        # Step 2: Calculate the effective distance D_{n|m} for directly connected nodes\n",
    "        effective_distance = {(m, n): 1 - np.log2(p) if p > 0 else float('inf') \n",
    "                              for (m, n), p in probability.items()}\n",
    "\n",
    "        # Step 3: Calculate the effective distance for indirectly connected nodes using the shortest path\n",
    "        all_pairs_shortest_path_length = dict(nx.all_pairs_dijkstra_path_length(G))\n",
    "        for m in G.nodes:\n",
    "            for n in G.nodes:\n",
    "                if m != n:\n",
    "                    shortest_path_length = all_pairs_shortest_path_length[m].get(n, float('inf'))\n",
    "                    if shortest_path_length != float('inf'):\n",
    "                        effective_distance[(m, n)] = shortest_path_length\n",
    "\n",
    "        return effective_distance\n",
    "\n",
    "    def calculate_interaction_scores(G, effective_distance):\n",
    "        interaction_scores = {}\n",
    "        for (i, j), d in effective_distance.items():\n",
    "            k_i = G.degree[i]\n",
    "            k_j = G.degree[j]\n",
    "            interaction_scores[(i, j)] = (k_i * k_j) / (d ** 2)\n",
    "        return interaction_scores\n",
    "\n",
    "    def compute_effg_centrality(G, interaction_scores):\n",
    "        effg_centrality = {node: 0 for node in G.nodes()}\n",
    "        for (i, j), score in interaction_scores.items():\n",
    "            effg_centrality[i] += score\n",
    "            effg_centrality[j] += score\n",
    "        return effg_centrality\n",
    "\n",
    "    # Calculate Effective Distance\n",
    "    effective_distance = calculate_effective_distance(G)\n",
    "\n",
    "    # Calculate Interaction Scores\n",
    "    interaction_scores = calculate_interaction_scores(G, effective_distance)\n",
    "\n",
    "    # Calculate EffG Centrality\n",
    "    effg_centrality = compute_effg_centrality(G, interaction_scores)\n",
    "    \n",
    "    return effg_centrality\n",
    "\n",
    "# method 41 (IS-PEW)\n",
    "def method41(G):\n",
    "    # Precompute expensive metrics\n",
    "    triangles = nx.triangles(G)  # Dictionary {node: triangle_count}\n",
    "    core_numbers = nx.core_number(G)  # Dictionary {node: core_number}\n",
    "    degrees = dict(G.degree())  # Dictionary {node: degree}\n",
    "    \n",
    "    # Compute total triangles once\n",
    "    TC = sum(triangles.values()) // 3 if sum(triangles.values()) > 0 else 1  # Avoid division by zero\n",
    "    \n",
    "    def compute_TP(v_A):\n",
    "        return triangles[v_A] / TC\n",
    "    \n",
    "    def compute_InfE(v_A, v_B):\n",
    "        common_neighbors = set(G[v_A]) & set(G[v_B])\n",
    "        sum_DG_k = sum(degrees[k] for k in common_neighbors)\n",
    "        return (degrees[v_A] * degrees[v_B]) / (1 + sum_DG_k)\n",
    "\n",
    "    def compute_NIP(v_A):\n",
    "        return sum(np.sqrt(degrees[v_a] * core_numbers[v_a]) for v_a in G[v_A])\n",
    "\n",
    "    # Store edge weights in a dictionary to avoid recomputation\n",
    "    edge_weights = {}\n",
    "\n",
    "    def compute_EW(v_A, v_B):\n",
    "        if (v_A, v_B) in edge_weights:\n",
    "            return edge_weights[(v_A, v_B)]\n",
    "        \n",
    "        TP_v_A, TP_v_B = compute_TP(v_A), compute_TP(v_B)\n",
    "        KS_v_A, KS_v_B = core_numbers[v_A], core_numbers[v_B]\n",
    "        NIP_v_A, NIP_v_B = compute_NIP(v_A), compute_NIP(v_B)\n",
    "        ED_v_A_v_B = 1 / compute_InfE(v_A, v_B)\n",
    "        \n",
    "        EW_value = ((KS_v_A * (1 + TP_v_A) * NIP_v_A) / ED_v_A_v_B) + ((KS_v_B * (1 + TP_v_B) * NIP_v_B) / ED_v_A_v_B)\n",
    "        edge_weights[(v_A, v_B)] = EW_value  # Store result for future use\n",
    "        edge_weights[(v_B, v_A)] = EW_value  # Store symmetric value\n",
    "        return EW_value\n",
    "\n",
    "    def compute_IS(v_A):\n",
    "        return sum(compute_EW(v_A, v_B) for v_B in G[v_A])\n",
    "\n",
    "    # Compute potential edge weights once\n",
    "    for u, v in G.edges():\n",
    "        G[u][v]['weight'] = compute_EW(u, v)\n",
    "\n",
    "    # Compute IS values for all nodes\n",
    "    IS_values = {v: compute_IS(v) for v in G.nodes()}\n",
    "    \n",
    "    return IS_values\n",
    "\n",
    "# k-shell iteration Factor (KS-IF)\n",
    "def method42(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    iteration_factors = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        if iteration_nodes:\n",
    "            m = iteration_nodes[-1][1]\n",
    "            for node, n in iteration_nodes:\n",
    "                iteration_factors[node] = k * (1 + n / m)\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    ks_IF = {}\n",
    "    deg = dict(G.degree())\n",
    "    for vi in G.nodes:\n",
    "        ks_IF[vi] = iteration_factors[vi] * deg[vi]\n",
    "        for vj in nx.neighbors(G, vi):\n",
    "            ks_IF[vi] += iteration_factors[vj] * deg[vj]\n",
    "\n",
    "    return ks_IF\n",
    "\n",
    "\n",
    "# Method 43 (DKGM)\n",
    "def method43(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    k_star = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        m = iteration_nodes[-1][1]\n",
    "        for node, n in iteration_nodes:\n",
    "            k_star[node] = k_shell[node] + (n / (m + 1))\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    DK = {}\n",
    "    DKGM = {}\n",
    "\n",
    "    for node in k_shell:\n",
    "        DK[node] = G.degree[node] + k_star[node]\n",
    "\n",
    "    R = 2  # Define the radius for DKGM computation\n",
    "    for node in G.nodes():\n",
    "        DKGM[node] = 0\n",
    "        for neighbor in nx.single_source_shortest_path_length(G, node, cutoff=R):\n",
    "            if node != neighbor:\n",
    "                try:\n",
    "                    d_ij = nx.shortest_path_length(G, source=node, target=neighbor)\n",
    "                    DKGM[node] += DK[node] * DK[neighbor] / d_ij**2\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "    return DKGM\n",
    "\n",
    "#modularity_vitality\n",
    "def method44(G):\n",
    "    \"\"\"Detect communities using Leiden algorithm and compute modularity vitality for each node.\"\"\"\n",
    "    \n",
    "    def detect_communities_leiden(G):\n",
    "        \"\"\"Detect communities using the Leiden algorithm.\n",
    "        \n",
    "        This function converts the NetworkX graph to an iGraph graph while preserving node names,\n",
    "        runs the Leiden algorithm, and returns a dictionary mapping node names to community indices.\n",
    "        \"\"\"\n",
    "        # Convert NetworkX graph to iGraph while preserving node names.\n",
    "        ig_G = ig.Graph.TupleList(G.edges(), directed=False, vertex_name_attr='name')\n",
    "        partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "        communities = {}\n",
    "        # Map each iGraph vertex back to the original NetworkX node name.\n",
    "        for idx, comm in enumerate(partition):\n",
    "            for v in comm:\n",
    "                communities[ig_G.vs[v][\"name\"]] = idx\n",
    "        return communities\n",
    "\n",
    "    def modularity(G, communities):\n",
    "        \"\"\"Compute the modularity of graph G with the given community partition.\"\"\"\n",
    "        M = G.number_of_edges()\n",
    "        Q = 0\n",
    "        for c in set(communities.values()):\n",
    "            nodes_c = [n for n in G.nodes() if communities[n] == c]\n",
    "            subgraph = G.subgraph(nodes_c)\n",
    "            L_c = subgraph.number_of_edges()\n",
    "            k_c = sum(G.degree(n) for n in nodes_c)\n",
    "            Q += (L_c / M) - (k_c / (2 * M)) ** 2\n",
    "        return Q\n",
    "\n",
    "    def compute_hi_c(G, node, communities):\n",
    "        \"\"\"Compute h_i,c values for a node as per Equation (10).\"\"\"\n",
    "        k_i = G.degree(node)\n",
    "        hi_c = {}\n",
    "        for c in set(communities.values()):\n",
    "            # Count links from node to neighbors in community c.\n",
    "            k_i_c = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == c)\n",
    "            hi_c[c] = k_i_c + (k_i if communities[node] == c else 0)\n",
    "        return hi_c\n",
    "\n",
    "    def modularity_vitality(G, communities, node):\n",
    "        \"\"\"Compute the modularity vitality of a node using Equation (9).\"\"\"\n",
    "        Q_initial = modularity(G, communities)\n",
    "        if not G.has_node(node):\n",
    "            return 0  \n",
    "\n",
    "        M = G.number_of_edges()\n",
    "        k_i = G.degree(node)\n",
    "        c_i = communities[node]\n",
    "        \n",
    "        # Compute h_i,c for the given node.\n",
    "        hi_c = compute_hi_c(G, node, communities)\n",
    "        \n",
    "        # Compute the total degree for each community.\n",
    "        d_c = {c: sum(G.degree(n) for n in G.nodes() if communities[n] == c)\n",
    "               for c in set(communities.values())}\n",
    "\n",
    "        # Get nodes in the same community as node.\n",
    "        community_nodes = [n for n in G.nodes() if communities[n] == c_i]\n",
    "        M_internal = G.subgraph(community_nodes).number_of_edges()\n",
    "        k_i_internal = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == c_i)\n",
    "        \n",
    "        # Compute the sum term over all communities.\n",
    "        sum_term = sum((d_c[c] - hi_c[c]) ** 2 for c in hi_c)\n",
    "        \n",
    "        # Compute updated modularity based on Equation (9).\n",
    "        Q_updated = (M_internal - k_i_internal) / (M - k_i) - (1 / (4 * (M - k_i) ** 2)) * sum_term\n",
    "\n",
    "        return Q_initial - Q_updated\n",
    "\n",
    "    # Detect communities using the Leiden algorithm.\n",
    "    communities = detect_communities_leiden(G)\n",
    "\n",
    "    # Compute modularity vitality for each node.\n",
    "    mod_vit = {node: modularity_vitality(G, communities, node) for node in G.nodes()}\n",
    "    \n",
    "    return mod_vit\n",
    "\n",
    "#Global-and-Local Centrality (GLC)\n",
    "def method45(G, lam=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Global-and-Local Centrality (GLC) of each node in graph G.\n",
    "    \n",
    "    The method first clusters the network using a potential-based approach,\n",
    "    then selects global critical nodes from each cluster, computes local influence\n",
    "    based on k-shell values, and finally calculates the overall GLC value.\n",
    "    \n",
    "    Parameters:\n",
    "        G   : networkx.Graph\n",
    "              The input graph.\n",
    "        lam : float (default=1.0)\n",
    "              The fraction (lambda) of nodes that must be covered by clusters.\n",
    "    \n",
    "    Returns:\n",
    "        GLC : dict\n",
    "              A dictionary mapping each node in G to its GLC centrality value.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Clustering and Global Critical Node Selection ---\n",
    "    clusters = []       # List to hold all clusters\n",
    "    assigned = set()    # Set of nodes that have been assigned or marked\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Helper function: Compute the potential of a node as defined in Eq.(10)\n",
    "    def compute_potential(node):\n",
    "        # Potential of node = degree(node) * (sum over j in N(node) of kin(j))\n",
    "        # where kin(j) = number of neighbors of j that are in {node} U N(node)\n",
    "        nbrs = set(G.neighbors(node))\n",
    "        target_set = nbrs.union({node})\n",
    "        s = 0\n",
    "        for j in nbrs:\n",
    "            s += sum(1 for nbr in G.neighbors(j) if nbr in target_set)\n",
    "        return G.degree(node) * s\n",
    "\n",
    "    # Continue clustering until the number of assigned nodes reaches lam * total_nodes\n",
    "    while len(assigned) < total_nodes * lam:\n",
    "        # Compute potentials for all unassigned nodes.\n",
    "        potentials = {}\n",
    "        for node in G.nodes():\n",
    "            if node not in assigned:\n",
    "                potentials[node] = compute_potential(node)\n",
    "        if not potentials:\n",
    "            break  # All nodes have been assigned\n",
    "        \n",
    "        # Select seed node with maximum potential.\n",
    "        seed = max(potentials, key=potentials.get)\n",
    "        pcmax = potentials[seed]\n",
    "        new_cluster = set([seed])\n",
    "        assigned.add(seed)\n",
    "        \n",
    "        # Include neighbors of the seed with potential >= pcmax/2.\n",
    "        for neighbor in G.neighbors(seed):\n",
    "            if neighbor not in assigned and compute_potential(neighbor) >= pcmax / 2:\n",
    "                new_cluster.add(neighbor)\n",
    "                assigned.add(neighbor)\n",
    "                \n",
    "        # Expand the cluster iteratively (three degrees of influence).\n",
    "        for _ in range(3):\n",
    "            # Gather neighbors of the current cluster (excluding nodes already in the cluster)\n",
    "            cluster_neighbors = set()\n",
    "            for node in new_cluster:\n",
    "                for nbr in G.neighbors(node):\n",
    "                    if nbr not in new_cluster:\n",
    "                        cluster_neighbors.add(nbr)\n",
    "            # Process neighbors in order of increasing degree.\n",
    "            cluster_neighbors = sorted(cluster_neighbors, key=lambda x: G.degree(x))\n",
    "            \n",
    "            added_flag = False\n",
    "            for candidate in cluster_neighbors:\n",
    "                # Count the number of edges from candidate to nodes in new_cluster (kin)\n",
    "                kin = sum(1 for nbr in G.neighbors(candidate) if nbr in new_cluster)\n",
    "                # Count the remaining edges from candidate to nodes outside new_cluster (kout)\n",
    "                kout = sum(1 for nbr in G.neighbors(candidate) if nbr not in new_cluster)\n",
    "                if kin >= kout:\n",
    "                    new_cluster.add(candidate)\n",
    "                    assigned.add(candidate)\n",
    "                    added_flag = True\n",
    "            if not added_flag:\n",
    "                break  # No more nodes can be added in this iteration\n",
    "        \n",
    "        # Mark all neighbors of the new cluster as assigned to avoid re-selection.\n",
    "        for node in new_cluster:\n",
    "            for nbr in G.neighbors(node):\n",
    "                assigned.add(nbr)\n",
    "        clusters.append(new_cluster)\n",
    "    \n",
    "    # From each cluster, select the global critical node (node with highest degree).\n",
    "    global_critical = []\n",
    "    for cluster in clusters:\n",
    "        if cluster:\n",
    "            gc = max(cluster, key=lambda node: G.degree(node))\n",
    "            global_critical.append(gc)\n",
    "    \n",
    "    # --- Step 2: Local Influence Calculation ---\n",
    "    # Compute the k-shell (core) numbers for all nodes.\n",
    "    core_numbers = nx.core_number(G)\n",
    "    \n",
    "    # Compute local influence LI for each node:\n",
    "    # LI(node) = sum_{j in N(node)} k_shell(j)\n",
    "    LI = {}\n",
    "    for node in G.nodes():\n",
    "        li = 0\n",
    "        for nbr in G.neighbors(node):\n",
    "            li += core_numbers[nbr]\n",
    "        LI[node] = li\n",
    "\n",
    "    # --- Step 3: Overall GLC Centrality Calculation ---\n",
    "    # For each node v, compute:\n",
    "    #   GlobalFactor(v) = sum_{u in global_critical} (LI(u) / (2 * max(d(v,u),1)))\n",
    "    #   GLC(v) = LI(v) * GlobalFactor(v)\n",
    "    GLC = {}\n",
    "    for v in G.nodes():\n",
    "        global_factor = 0\n",
    "        for u in global_critical:\n",
    "            try:\n",
    "                d = nx.shortest_path_length(G, source=v, target=u)\n",
    "            except nx.NetworkXNoPath:\n",
    "                d = float('inf')\n",
    "            d = max(d, 1)  # Avoid division by zero\n",
    "            global_factor += LI[u] / (2 * d)\n",
    "        GLC[v] = LI[v] * global_factor\n",
    "\n",
    "    return GLC\n",
    "\n",
    "#methods = {\n",
    "#    \"Degree Centrality\": method1,\n",
    "#    \"Betweenness Centrality\": method2,\n",
    "#    \"Closeness Centrality\": method3,\n",
    "#    \"K-shell\": method4,\n",
    "#    \"Cnc\": method5,\n",
    "#    \"Cnc Plus\": method6,\n",
    "#    \"Distance to Network Core\": method7,\n",
    "#    \"All Around Node\": method8,\n",
    "#    \"H-index Centrality\": method9,\n",
    "#    \"Local H-index\": method10,\n",
    "#    \"Semi-Local Centrality\": method11,\n",
    "#    \"DNC\": method12,\n",
    "#    \"ECLGC\": method13,\n",
    "#    \"Centripetal Centrality\": method14,\n",
    "#    \"Cluster Rank\": method15,\n",
    "#    \"Improved Gravity Centrality\":method16,\n",
    "#    \"EDBC\" : method17,\n",
    "#    \"Entropy Centrality\": method18,\n",
    "#    \"KNC\" : method19,\n",
    "#    \"NEDC\" : method20,\n",
    "#    \"LGC\" : method21,\n",
    "#    \"SEGM\" : method22,\n",
    "#    \"MCGM\" : method23,\n",
    "#    \"HVGC\" : method24,\n",
    "#    \"MDD\" : method25,\n",
    "#    \"BaseGM\" : method26,\n",
    "#    \"GlobalGM\" : method27,\n",
    "#    \"LocalGM\" : method28,\n",
    "#    \"WGravity\" : method29,\n",
    "#    \"HKS\" : method30,\n",
    "#    \"SHKS\" : method31,\n",
    "#    \"KSGC\" : method32,\n",
    "#    \"EFFC\" : method33,\n",
    "#    \"Local Relative ASP\" : method34,\n",
    "#    \"Information Rank\" : method35,\n",
    "#    \"Weighted K-shell\" : method36,\n",
    "#    \"Hybrid K-shell\" : method37,\n",
    "#    \"Social Capital\" : method38,\n",
    "#    \"Potential Edge Weight\" : method39,\n",
    "#    \"effg Gravity\" : method40,\n",
    "#    \"IS-PEW\" : method41,\n",
    "#    \"KS-IF\" : method42,\n",
    "#    \"DKGM\" : method43,\n",
    "#    \"modularity vitality\":method44,\n",
    "#    \"Global-and-Local Centrality (GLC)\":method45\n",
    "# }\n",
    "\n",
    "methods = [globals()[f'method{i}'] for i in range(1, 46)]\n",
    "\n",
    "\n",
    "# Define k values for Jaccard similarity\n",
    "k_factors = [0.01, 0.03, 0.05, 0.07, 0.085, 0.10]  # Percentages of top nodes to consider\n",
    "\n",
    "dataset_folder = 'dataset'\n",
    "beta_values = [0.02, 0.05, 0.08, 0.10, 0.13, 0.15, 0.18, 0.20, 0.22, 0.25, 0.30]\n",
    "alpha_values = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "f_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "\n",
    "for dataset_filename in os.listdir(dataset_folder):\n",
    "    if dataset_filename.endswith('.xlsx'):\n",
    "        dataset_path = os.path.join(dataset_folder, dataset_filename)\n",
    "        df_edges = pd.read_excel(dataset_path, sheet_name='Sheet1')\n",
    "        G = nx.from_pandas_edgelist(df_edges, source='source_column', target='target_column')\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "        \n",
    "        df_sir = pd.read_excel(dataset_path, sheet_name='SIR')\n",
    "        df_sir.set_index('Node', inplace=True)\n",
    "        \n",
    "        num_nodes = len(G.nodes())  # Get network size\n",
    "        \n",
    "        results = {}\n",
    "        method_results = {}\n",
    "        \n",
    "        for i, method in enumerate(methods, start=1):\n",
    "            method_name = f'method{i}'\n",
    "            try:\n",
    "                ranked_nodes, exec_time = measure_execution_time(method, G)\n",
    "                monotonicity = compute_monotonicity(G, ranked_nodes)\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': ranked_nodes,\n",
    "                    'execution_time': exec_time,\n",
    "                    'monotonicity': monotonicity\n",
    "                }\n",
    "            except Exception as e:\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': [],\n",
    "                    'execution_time': None,\n",
    "                    'monotonicity': None,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        \n",
    "        for beta in beta_values:\n",
    "            beta_column = f'Beta_{beta:.2f}'\n",
    "            if beta_column not in df_sir.columns:\n",
    "                print(f\"Warning: {beta_column} not found in {dataset_filename}. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            spread_power = df_sir[beta_column].to_dict()\n",
    "            sigma = sorted(spread_power.keys(), key=lambda x: spread_power[x], reverse=True)\n",
    "            \n",
    "            for method_name, method_data in method_results.items():\n",
    "                if 'error' in method_data:\n",
    "                    results[(method_name, beta)] = {'Beta': beta, 'Kendall Tau': None, 'P-Value': None}\n",
    "                    continue\n",
    "                \n",
    "                ranked_nodes = method_data['ranked_nodes']\n",
    "                tau, p_value = kendalltau(\n",
    "                    [spread_power[node] for node in sigma],\n",
    "                    [spread_power[node] for node, _ in ranked_nodes]\n",
    "                )\n",
    "                \n",
    "                R = [node for node, _ in ranked_nodes]\n",
    "                jaccard_scores = {}\n",
    "                si_scores = {}\n",
    "                rbo_scores = {}\n",
    "                \n",
    "                for k_factor in k_factors:\n",
    "                    k = max(1, int(k_factor * num_nodes))\n",
    "                    top_k_sigma = set(sigma[:k])\n",
    "                    top_k_R = set(R[:k])\n",
    "                    jaccard_scores[f'Jaccard k={k}'] = compute_jaccard_similarity(top_k_sigma, top_k_R)\n",
    "                \n",
    "                for f in f_values:\n",
    "                    f_count = max(1, int(f * num_nodes))\n",
    "                    top_f_nodes = [node for node, _ in ranked_nodes][:f_count]\n",
    "                    si_scores[f'SI {f:.2f}'] = sum(spread_power[node] for node in top_f_nodes) / (f * num_nodes)\n",
    "                    rbo_scores[f'RBO {f:.2f}'] = compute_rbo(sigma[:f_count], R[:f_count])\n",
    "                \n",
    "                results[(method_name, beta)] = {\n",
    "                    'Beta': beta,\n",
    "                    'Kendall Tau': tau,\n",
    "                    'P-Value': p_value,\n",
    "                    **jaccard_scores,\n",
    "                    **si_scores,\n",
    "                    **rbo_scores\n",
    "                }\n",
    "        \n",
    "        data = []\n",
    "        for (method_name, beta), result in results.items():\n",
    "            row = [\n",
    "                method_name,\n",
    "                method_results[method_name]['execution_time'],\n",
    "                method_results[method_name].get('monotonicity', 'N/A'),\n",
    "                result.get('Beta'),\n",
    "                result.get('Kendall Tau'),\n",
    "                result.get('P-Value')\n",
    "            ] + [result.get(f'Jaccard k={int(k_factor * num_nodes)}') for k_factor in k_factors] + \\\n",
    "              [result.get(f'SI {f:.2f}') for f in f_values] + \\\n",
    "              [result.get(f'RBO {f:.2f}') for f in f_values]\n",
    "            \n",
    "            data.append(row)\n",
    "        \n",
    "        expected_columns = ['Method', 'Execution Time', 'Monotonicity', 'Beta', 'Kendall Tau', 'P-Value'] + \\\n",
    "                           [f'Jaccard k={int(k_factor * num_nodes)}' for k_factor in k_factors] + \\\n",
    "                           [f'SI {f:.2f}' for f in f_values] + \\\n",
    "                           [f'RBO {f:.2f}' for f in f_values]\n",
    "        \n",
    "        result_df = pd.DataFrame(data, columns=expected_columns)\n",
    "        \n",
    "        dataset_name = os.path.splitext(dataset_filename)[0]\n",
    "        output_file = f'{dataset_name}_results.xlsx'\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df_edges.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "            df_sir.to_excel(writer, sheet_name='SIR')\n",
    "            result_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        \n",
    "        print(f\"Processed {dataset_filename}, results saved in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f229c7e2-15c2-4e22-9fb5-324389f4bc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Contiguous(SIR).xlsx, results saved in Contiguous(SIR)_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "def measure_execution_time(method, G):\n",
    "    start_time = time.time()\n",
    "    result = method(G)\n",
    "    ranked_nodes = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return ranked_nodes, execution_time\n",
    "\n",
    "def compute_monotonicity(G, ranked_nodes):\n",
    "    ranks = [score for node, score in ranked_nodes]\n",
    "    unique_ranks = list(set(ranks))\n",
    "    n = G.number_of_nodes()\n",
    "    nr_dict = {rank: ranks.count(rank) for rank in unique_ranks}\n",
    "    nr_sum = sum(nr * (nr - 1) for nr in nr_dict.values())\n",
    "    M = (1 - nr_sum / (n * (n - 1))) ** 2\n",
    "    return M\n",
    "\n",
    "def compute_mrr(ground_truth, ranked_list, top_ratios):\n",
    "    mrr_scores = {}\n",
    "    num_nodes = len(ground_truth)\n",
    "    \n",
    "    for ratio in top_ratios:\n",
    "        k = max(1, int(ratio * num_nodes))\n",
    "        reciprocal_ranks = []\n",
    "        \n",
    "        for node in ground_truth[:k]:\n",
    "            if node in ranked_list:\n",
    "                rank = ranked_list.index(node) + 1\n",
    "                reciprocal_ranks.append(1 / rank)\n",
    "        \n",
    "        mrr_scores[f'MRR {ratio:.2f}'] = sum(reciprocal_ranks) / k if reciprocal_ranks else 0\n",
    "    \n",
    "    return mrr_scores\n",
    "\n",
    "\n",
    "def compute_rbo(sigma, R, alpha=0.9):\n",
    "    \"\"\"Compute the Rank-Biased Overlap (RBO) between two ranking lists.\"\"\"\n",
    "    def A(sigma, R, f):\n",
    "        sigma_f = set(sigma[:f])\n",
    "        R_f = set(R[:f])\n",
    "        if len(sigma_f) == 0 and len(R_f) == 0:\n",
    "            return 1\n",
    "        return len(sigma_f & R_f) / len(sigma_f | R_f)\n",
    "\n",
    "    n = max(len(sigma), len(R))\n",
    "    rbo = (1 - alpha) * sum(alpha**(f - 1) * A(sigma, R, f) for f in range(1, n + 1))\n",
    "    return rbo\n",
    "\n",
    "def compute_jaccard_similarity(set1, set2):\n",
    "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0\n",
    "\n",
    "# Define methods\n",
    "def method1(G):\n",
    "    return nx.degree_centrality(G)\n",
    "\n",
    "def method2(G):\n",
    "    return nx.betweenness_centrality(G)\n",
    "\n",
    "def method3(G):\n",
    "    return nx.closeness_centrality(G)\n",
    "\n",
    "def method4(G):\n",
    "    return nx.core_number(G)\n",
    "\n",
    "def method5(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = {vi: sum(ks[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc\n",
    "\n",
    "def method6(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = method5(G)\n",
    "    cnc_plus = {vi: sum(cnc[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc_plus\n",
    "\n",
    "def method7(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_max_set = {node for node, k in ks.items() if k == ks_max}\n",
    "    teta = {}\n",
    "    for vi in G.nodes():\n",
    "        teta_vi = (ks_max - ks[vi] + 1)\n",
    "        total_dis = 0\n",
    "        for vj in ks_max_set:\n",
    "            if nx.has_path(G, vi, vj):\n",
    "                total_dis += nx.shortest_path_length(G, vi, vj)\n",
    "        teta[vi] = teta_vi * total_dis\n",
    "    return teta\n",
    "\n",
    "def method8(G):\n",
    "    ks = nx.core_number(G)\n",
    "    deg = nx.degree_centrality(G)\n",
    "    bet = nx.betweenness_centrality(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_norm = {node: ks[node] / ks_max for node in G.nodes()}\n",
    "    all_around = {v: math.sqrt(ks_norm[v]**2 + bet[v]**2 + deg[v]**2) for v in G.nodes()}\n",
    "    return all_around\n",
    "\n",
    "def method9(G):\n",
    "    h_index = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degrees = sorted([G.degree(neighbor) for neighbor in neighbors], reverse=True)\n",
    "        h = 0\n",
    "        for i, degree in enumerate(degrees):\n",
    "            if degree >= i + 1:\n",
    "                h = i + 1\n",
    "            else:\n",
    "                break\n",
    "        h_index[node] = h\n",
    "    return h_index\n",
    "\n",
    "def method10(G):\n",
    "    HI = method9(G)\n",
    "    LHI = {vi: HI[vi] + sum(HI[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return LHI\n",
    "\n",
    "def method11(G):\n",
    "    nv = {}\n",
    "    for vi in G.nodes():\n",
    "        set1 = set()\n",
    "        for vj in G.neighbors(vi):\n",
    "            set1.add(vj)\n",
    "            for vk in G.neighbors(vj):\n",
    "                set1.add(vk)\n",
    "        nv[vi] = len(set1) - 1\n",
    "    \n",
    "    c_local = {}\n",
    "    for vi in G.nodes():\n",
    "        total1 = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            total2 = 0\n",
    "            for vk in G.neighbors(vj):\n",
    "                total2 += nv[vk]\n",
    "            total1 += total2\n",
    "        c_local[vi] = total1\n",
    "    return c_local\n",
    "\n",
    "def method12(G):\n",
    "    SLC = {}\n",
    "    k = dict(nx.degree(G))\n",
    "    C = nx.clustering(G)\n",
    "    for i in G.nodes():\n",
    "        SLC[i] = sum(C[j] for j in G.neighbors(i))\n",
    "    \n",
    "    DNC = {}\n",
    "    alpha = 1\n",
    "    for i in G.nodes():\n",
    "        DNC[i] = k[i] + alpha * SLC[i]\n",
    "    return DNC\n",
    "\n",
    "def method13(G):\n",
    "    CLC = nx.clustering(G)\n",
    "    CLGC = {}\n",
    "    for i in G.nodes():\n",
    "        CLGC[i] = sum(\n",
    "            math.sqrt(CLC[j]) / nx.shortest_path_length(G, i, j)\n",
    "            for j in G.nodes()\n",
    "            if i != j and nx.has_path(G, i, j)\n",
    "        ) * CLC[i]\n",
    "    \n",
    "    ECLGC = {}\n",
    "    for v in G.nodes():\n",
    "        sum1 = sum2 = sum3 = 0\n",
    "        neighbors_v = list(G.neighbors(v))\n",
    "        for u in G.nodes():\n",
    "            if u != v and nx.has_path(G, v, u):\n",
    "                neighbors_u = list(G.neighbors(u))\n",
    "                lu = len(neighbors_u)\n",
    "                sum1 += CLC[u] / len(neighbors_v)\n",
    "                sum2 = sum(CLC[j] / lu for j in neighbors_u)\n",
    "                sum3 += math.sqrt(CLC[u] + sum2) / nx.shortest_path_length(G, v, u)\n",
    "        ECLGC[v] = sum1 * sum3\n",
    "    return ECLGC\n",
    "\n",
    "def compute_alpha(graph):\n",
    "    size_of_V = graph.number_of_nodes()\n",
    "    order_of_magnitude = int(math.log10(size_of_V))\n",
    "    alpha = size_of_V / order_of_magnitude\n",
    "    return alpha\n",
    "\n",
    "def method14(G, m=1):\n",
    "    alpha = compute_alpha(G)\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    kshell = nx.core_number(G)\n",
    "    \n",
    "    centrality = {}\n",
    "    \n",
    "    for i in G.nodes:\n",
    "        cenC_i = 0\n",
    "        \n",
    "        # Get m-neighborhood of node i\n",
    "        m_neighborhood = set(nx.single_source_shortest_path_length(G, i, cutoff=m).keys())\n",
    "        m_neighborhood.remove(i)  # Exclude the node itself\n",
    "        \n",
    "        for j in m_neighborhood:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij == 0:\n",
    "                continue\n",
    "            DC_i = degree_centrality[i]\n",
    "            C_i = clustering_coeff[i]\n",
    "            k_s_i = kshell[i]\n",
    "            k_s_j = kshell[j]\n",
    "            \n",
    "            term1 = (math.exp(alpha) * DC_i + (1 / C_i if C_i != 0 else 0))\n",
    "            term2 = (k_s_j / (abs(k_s_i - k_s_j) + 1))\n",
    "            \n",
    "            cenC_i += (4 * math.pi**2) * (term1 / (d_ij**2)) * term2\n",
    "        \n",
    "        centrality[i] = cenC_i\n",
    "    \n",
    "    return centrality\n",
    "\n",
    "def method15(G):\n",
    "    clustering_coeffs = nx.clustering(G)\n",
    "    cluster_rank_scores = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    degrees = dict(G.degree())\n",
    "\n",
    "    # Calculate ClusterRank scores\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(degrees[neighbor] + 1 for neighbor in neighbors)\n",
    "        cluster_rank_scores[node] = 10 ** (-clustering_coeffs[node]) * sum_term\n",
    "\n",
    "    return cluster_rank_scores\n",
    "\n",
    "# Optimized methods\n",
    "def get_neighborhood(G, node, radius):\n",
    "    neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "    neighborhood.pop(node, None)\n",
    "    return list(neighborhood.keys())\n",
    "\n",
    "# Improved Gravity Centrality\n",
    "def method16(G):\n",
    "    IGC = {}\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        IGC[i] = 0\n",
    "        for j in G.neighbors(i):\n",
    "            neighborhood = get_neighborhood(G, j, 2)\n",
    "            for p in neighborhood:\n",
    "                IGC[i] += (ks[j] * k[p]) / (nx.shortest_path_length(G, j, p)**2)\n",
    "    return IGC\n",
    "\n",
    "def k_shell(graph):\n",
    "    return nx.core_number(graph)\n",
    "\n",
    "def evaluate_power_k_shell(k_si, k_sj):\n",
    "    return math.sqrt(k_si + k_sj)\n",
    "\n",
    "def compute_P_ji(graph, node_i, node_j):\n",
    "    try:\n",
    "        a_ji = graph[node_j][node_i]['weight'] if 'weight' in graph[node_j][node_i] else 1\n",
    "        total_a_ji = sum(graph[node_j][nbr]['weight'] if 'weight' in graph[node_j][nbr] else 1 for nbr in graph.neighbors(node_j))\n",
    "        return a_ji / total_a_ji\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def evaluate_ED(graph, node_i, node_j):\n",
    "    P_ji = compute_P_ji(graph, node_i, node_j)\n",
    "    if P_ji == 0:\n",
    "        return 1  # Prevent log(0)\n",
    "    return max(1 - math.log(P_ji), 1)\n",
    "\n",
    "def evaluate_influence(graph, node_i, node_j, k_shell_values):\n",
    "    alpha_ij = evaluate_power_k_shell(k_shell_values[node_i], k_shell_values[node_j])\n",
    "    ED_ij = evaluate_ED(graph, node_i, node_j)\n",
    "    degree_vi = graph.degree(node_i)\n",
    "    return alpha_ij * degree_vi / ED_ij\n",
    "\n",
    "# EDBC\n",
    "def method17(G):\n",
    "    k_shell_values = k_shell(G)\n",
    "    EDBC = {u: 0 for u in G.nodes()}\n",
    "    \n",
    "    for u in G.nodes():\n",
    "        for v in G.neighbors(u):\n",
    "            for neighbor_u in G.neighbors(u):\n",
    "                EDBC[v] += evaluate_influence(G, v, neighbor_u, k_shell_values)\n",
    "    \n",
    "    return EDBC\n",
    "\n",
    "def method18(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    entropy_centrality = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        if len(neighbors) == 0:\n",
    "            entropy_centrality[node] = 0\n",
    "        else:\n",
    "            sum_degrees = sum(G.degree(neighbor) for neighbor in neighbors)\n",
    "            sum_centrality = sum(degree_centrality[neighbor] for neighbor in neighbors)\n",
    "            if sum_centrality == 0:\n",
    "                entropy_centrality[node] = 0\n",
    "            else:\n",
    "                entropy_centrality[node] = closeness_centrality[node] * (sum_degrees / sum_centrality)\n",
    "    \n",
    "    return entropy_centrality\n",
    "\n",
    "# KNC\n",
    "def method19(G):\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    degree_values = dict(G.degree())\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    \n",
    "    alpha = compute_alpha(G)\n",
    "    KNC = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        sum_term = sum(k_shell_values[neighbor] * degree_values[neighbor] * clustering_coeff[neighbor] for neighbor in neighbors)\n",
    "        KNC[node] = alpha * sum_term\n",
    "    \n",
    "    return KNC\n",
    "\n",
    "# NEDC\n",
    "def method20(G):\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    k_shell_values = nx.core_number(G)\n",
    "    \n",
    "    nodes = G.nodes()\n",
    "    NEDC = {}\n",
    "    \n",
    "    for node in nodes:\n",
    "        term1 = degree_centrality[node] * betweenness_centrality[node]\n",
    "        term2 = closeness_centrality[node] * k_shell_values[node]\n",
    "        NEDC[node] = term1 + term2\n",
    "    \n",
    "    return NEDC\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "# Method 21 (LGC)\n",
    "def method21(G):\n",
    "    LC = {}\n",
    "    K = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        LC[i] = sum(K[j] for j in nx.neighbors(G, i)) * 2 + K[i] ** 2 + K[i]\n",
    "\n",
    "    LGC = {}\n",
    "    radius = 3\n",
    "    for vi in G.nodes():\n",
    "        LGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            try:\n",
    "                d_ij = nx.shortest_path_length(G, vi, vj)\n",
    "                if d_ij != 0:\n",
    "                    LGC[vi] += LC[vi] * LC[vj] / d_ij ** 2\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    return LGC\n",
    "\n",
    "# Method 22 (SEGM)\n",
    "def method22(G):\n",
    "    k = dict(G.degree())\n",
    "    I = dict()\n",
    "    E = dict()\n",
    "    SE = dict()\n",
    "\n",
    "    for node in G.nodes:\n",
    "        I[node] = 0\n",
    "\n",
    "    for i in G.nodes:\n",
    "        sum_kj = 0\n",
    "        for j in G.neighbors(i):\n",
    "            sum_kj += k[j]\n",
    "        if sum_kj != 0:\n",
    "            I[i] = k[i] / sum_kj\n",
    "        sum_e = 0\n",
    "        for j in G.neighbors(i):\n",
    "            if I[j] != 0:\n",
    "                sum_e += I[j] * np.log(I[j])\n",
    "        E[i] = -1 * sum_e\n",
    "        SE[i] = np.exp(E[i]) * k[i]\n",
    "\n",
    "    SEGM = dict()\n",
    "    R = 3\n",
    "    for i in G.nodes:\n",
    "        SEGM[i] = 0\n",
    "        neighbors = get_neighborhood(G, i, R)\n",
    "        for j in neighbors:\n",
    "            if nx.shortest_path_length(G, i, j) != 0:\n",
    "                SEGM[i] += (SE[i] * SE[j]) / (nx.shortest_path_length(G, i, j) ** 2)\n",
    "\n",
    "    return SEGM\n",
    "\n",
    "# Method 23 (MCGM)\n",
    "def method23(G):\n",
    "    k = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    x = nx.eigenvector_centrality(G)\n",
    "\n",
    "    k_values = np.array(list(k.values()))\n",
    "    ks_values = np.array(list(ks.values()))\n",
    "    x_values = np.array(list(x.values()))\n",
    "\n",
    "    k_mid = np.median(k_values)\n",
    "    ks_mid = np.median(ks_values)\n",
    "    x_mid = np.median(x_values)\n",
    "\n",
    "    k_max = max(k_values)\n",
    "    ks_max = max(ks_values)\n",
    "    x_max = max(x_values)\n",
    "\n",
    "    alpha = max(k_mid / k_max, x_mid / x_max) / (ks_mid / ks_max)\n",
    "\n",
    "    MCGM = dict()\n",
    "    R = 3\n",
    "\n",
    "    for i in G.nodes():\n",
    "        MCGM[i] = 0\n",
    "        for j in G.nodes():\n",
    "            if i != j and nx.shortest_path_length(G, source=i, target=j) <= R:\n",
    "                MCGM[i] += ((k[i] / k_max + alpha * ks[i] / ks_max + x[i] / x_max) *\n",
    "                            (k[j] / k_max + alpha * ks[j] / ks_max + x[j] / x_max) /\n",
    "                            (nx.shortest_path_length(G, source=i, target=j) ** 2))\n",
    "\n",
    "    return MCGM\n",
    "\n",
    "# Method 24 (HVGC)\n",
    "def compute_H_index(G, i):\n",
    "    degrees = [G.degree(neighbor) for neighbor in G.neighbors(i)]\n",
    "    degrees.sort(reverse=True)\n",
    "    H_index = 0\n",
    "    for idx, degree in enumerate(degrees):\n",
    "        if degree >= idx + 1:\n",
    "            H_index = idx + 1\n",
    "        else:\n",
    "            break\n",
    "    return H_index\n",
    "\n",
    "def compute_H_v(G, i):\n",
    "    H_i = compute_H_index(G, i)\n",
    "    H_v_i = sum(G.degree(j) for j in G.neighbors(i) if G.degree(j) >= H_i)\n",
    "    return H_v_i\n",
    "\n",
    "def compute_c_i(G, i):\n",
    "    def compute_p_ij(G, i, j):\n",
    "        neighbors_i = list(G.neighbors(i))\n",
    "        sum_z_iw = sum(G[i][w].get('weight', 1) for w in neighbors_i)\n",
    "        p_ij = G[i][j].get('weight', 1) / sum_z_iw if sum_z_iw != 0 else 0\n",
    "        return p_ij\n",
    "\n",
    "    c_i = 0\n",
    "    neighbors_i = list(G.neighbors(i))\n",
    "    for j in neighbors_i:\n",
    "        p_ij = compute_p_ij(G, i, j)\n",
    "        neighbors_j = list(G.neighbors(j))\n",
    "        common_neighbors = set(neighbors_i).intersection(neighbors_j)\n",
    "        sum_piw_pwj = sum(compute_p_ij(G, i, w) * compute_p_ij(G, w, j) for w in common_neighbors)\n",
    "        term = p_ij + sum_piw_pwj\n",
    "        c_i += term ** 2\n",
    "    \n",
    "    return c_i\n",
    "\n",
    "def compute_HVGC(G, i, H_v, R):\n",
    "    c_i = compute_c_i(G, i)\n",
    "    HVGC_i = 0\n",
    "    for j in G.nodes():\n",
    "        if j != i:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij <= R:\n",
    "                HVGC_i += (math.exp(-c_i) * H_v[i] * H_v[j]) / (d_ij ** 2)\n",
    "    return HVGC_i\n",
    "\n",
    "#HVGC\n",
    "def method24(G):\n",
    "    H_v = {i: compute_H_v(G, i) for i in G.nodes()}\n",
    "    R = 2\n",
    "    HVGC = {node: compute_HVGC(G, node, H_v, R) for node in G.nodes()}\n",
    "    return HVGC\n",
    "\n",
    "# Method 25 (MDD)\n",
    "def method25(G):\n",
    "    def compute_mixed_degrees(H, landa):\n",
    "        km = {}\n",
    "        for v in H.nodes():\n",
    "            kr = H.degree(v)\n",
    "            ke = G.degree(v) - kr\n",
    "            km[v] = kr + landa * ke\n",
    "        return km\n",
    "\n",
    "    H = G.copy()\n",
    "    landa = 0.7\n",
    "    rank = {}\n",
    "    km = {v: 1 for v in G.nodes()}  # Initialize km values\n",
    "\n",
    "    while H.number_of_nodes() > 0:\n",
    "        km = compute_mixed_degrees(H, landa)\n",
    "        min_km_value = min(km.values())\n",
    "\n",
    "        # Find and remove nodes with mixed degree <= min_km_value\n",
    "        node_set = [v for v in H.nodes() if km[v] == min_km_value]\n",
    "        for each in node_set:\n",
    "            rank[each] = km[each]\n",
    "            H.remove_node(each)\n",
    "\n",
    "    return rank\n",
    "\n",
    "# Method 26 (BaseGM)\n",
    "def method26(G):\n",
    "    def get_neighborhood(G, node, radius):\n",
    "        neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "        neighborhood.pop(node, None)\n",
    "        return list(neighborhood.keys())\n",
    "\n",
    "    gravity = dict()\n",
    "    kshell = nx.core_number(G)\n",
    "    for vi in G.nodes():\n",
    "        gravity[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, 3)\n",
    "        for vj in neighbors:\n",
    "            gravity[vi] += ((kshell[vi] * kshell[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    gravity_plus = dict()\n",
    "    for vi in G.nodes():\n",
    "        gravity_plus[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            gravity_plus[vi] += gravity[vj]\n",
    "\n",
    "    return gravity_plus\n",
    "\n",
    "# Method 27 (GlobalGM)\n",
    "def method27(G):\n",
    "    gm = dict()\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        gm[vi] = 0\n",
    "        for vj in G.nodes():\n",
    "            if vj != vi:\n",
    "                gm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gm\n",
    "\n",
    "# Method 28 (LocalGM)\n",
    "def method28(G):\n",
    "    lgm = dict()\n",
    "    radius = 3\n",
    "    k = dict(nx.degree(G))\n",
    "    for vi in G.nodes():\n",
    "        lgm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                lgm[vi] += ((k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return lgm\n",
    "\n",
    "# Method 29 (WGravity)\n",
    "def method29(G):\n",
    "    gmm = dict()\n",
    "    ev = nx.eigenvector_centrality(G)\n",
    "    k = dict(nx.degree(G))\n",
    "\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        gmm[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            if vj != vi:\n",
    "                gmm[vi] += (ev[vi] * (k[vi] * k[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "    return gmm\n",
    "\n",
    "\n",
    "# Method 30 (HKS)\n",
    "def method30(G):\n",
    "    def set_b_values(G):\n",
    "        b, Shell, b_values = 1, 1, {}\n",
    "        while G.number_of_nodes() > 0:\n",
    "            flag = False\n",
    "            for v in list(G.nodes):\n",
    "                if G.degree[v] <= Shell:\n",
    "                    b_values[v] = b\n",
    "                    flag = True\n",
    "            \n",
    "            if flag:\n",
    "                G.remove_nodes_from([v for v in list(G.nodes) if b_values.get(v) == b])\n",
    "                b += 1\n",
    "            else:\n",
    "                Shell += 1\n",
    "        \n",
    "        return b_values\n",
    "\n",
    "    def set_f(G, b_values):\n",
    "        V, f = list(G.nodes), max(b_values.values())\n",
    "        fi = {v: b_values[v] if all(b_values[v] >= b_values[vj] for vj in G.neighbors(v)) else 0 for v in V}\n",
    "\n",
    "        while V:\n",
    "            for vi in V:\n",
    "                if fi[vi] == f:\n",
    "                    for vj in G.neighbors(vi):\n",
    "                        if fi[vi] - 1 > fi[vj]:\n",
    "                            fi[vj] = fi[vi] - 1\n",
    "            V.remove(vi)\n",
    "\n",
    "        return fi\n",
    "\n",
    "    b_values = set_b_values(G.copy())\n",
    "    fi_values = set_f(G.copy(), b_values)\n",
    "\n",
    "    S = {vi: sum(G.degree[vj] * (b_values[vj] + fi_values[vj]) for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    HKS = {vi: sum(S[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return HKS\n",
    "\n",
    "# Method 31 (SHKS)\n",
    "def method31(G):\n",
    "    def compute_efficiency(G):\n",
    "        n = len(G)\n",
    "        if n <= 1:\n",
    "            return 0\n",
    "        inv_distances = (1 / d for u, v_dict in nx.shortest_path_length(G) for v, d in v_dict.items() if d > 0)\n",
    "        return sum(inv_distances) / (n * (n - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_copy = G.copy()\n",
    "            G_copy.remove_node(k)\n",
    "            centrality[k] = (efficiency_G - compute_efficiency(G_copy)) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    def network_constrained_coefficient(G, v):\n",
    "        neighbors = list(G.neighbors(v))\n",
    "        coefficient = sum(calculate_pvu(G, v, u) + sum((calculate_pvu(G, v, w) * calculate_pvu(G, w, u)) ** 2 for w in set(G.neighbors(u)) & set(G.neighbors(v))) for u in neighbors)\n",
    "        return coefficient\n",
    "\n",
    "    def calculate_pvu(G, v, u):\n",
    "        return (1 if u in G.neighbors(v) else 0) / G.degree(v)\n",
    "\n",
    "    efficiency_centrality = compute_efficiency_centrality(G)\n",
    "    Constraint_Coef = {node: network_constrained_coefficient(G, node) for node in G.nodes()}\n",
    "    sh = {node: 1 / Constraint_Coef[node] for node in G.nodes()}\n",
    "    ks = nx.core_number(G)\n",
    "    alpha = 0.2\n",
    "    I = {node: alpha * sh[node] + ks[node] for node in G.nodes()}\n",
    "    C = {v: sum(I[v] + I[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    IS = {v: sum(C[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    SHKS = {v: sum(IS[u] for u in G.neighbors(v)) for v in G.nodes()}\n",
    "    return SHKS\n",
    "\n",
    "# Method 32 (KSGC)\n",
    "def method32(G):\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_min = min(ks.values())\n",
    "    k = dict(G.degree())\n",
    "    n = G.number_of_nodes()\n",
    "    C = np.zeros((n, n))\n",
    "    F = np.zeros((n, n))\n",
    "    nodes = list(G.nodes())\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                vi, vj = nodes[i], nodes[j]\n",
    "                try:\n",
    "                    C[i, j] = np.exp((ks[vi] - ks[vj]) / (ks_max - ks_min))\n",
    "                    F[i, j] = C[i, j] * (k[vi] * k[vj] / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    F[i, j] = 0\n",
    "\n",
    "    KSGC = dict()\n",
    "    radius = int(0.5 * nx.diameter(G))\n",
    "    for vi in G.nodes():\n",
    "        vi_index = node_index[vi]\n",
    "        KSGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            vj_index = node_index[vj]\n",
    "            KSGC[vi] += F[vi_index, vj_index]\n",
    "    \n",
    "    return KSGC\n",
    "\n",
    "# Method 33 (EFFC)\n",
    "def method33(G):\n",
    "    def compute_efficiency(G):\n",
    "        N = len(G.nodes)\n",
    "        if N < 2:\n",
    "            return 0\n",
    "        efficiency_sum = 0\n",
    "        for i in G.nodes:\n",
    "            for j in G.nodes:\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        shortest_path_length = nx.shortest_path_length(G, source=i, target=j)\n",
    "                        efficiency_sum += 1 / shortest_path_length\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        efficiency_sum += 0  # Adding 0 for disconnected pairs\n",
    "        return efficiency_sum / (N * (N - 1))\n",
    "\n",
    "    def compute_efficiency_centrality(G):\n",
    "        efficiency_G = compute_efficiency(G)\n",
    "        centrality = {}\n",
    "        for k in G.nodes:\n",
    "            G_k_prime = G.copy()\n",
    "            G_k_prime.remove_node(k)\n",
    "            efficiency_G_k_prime = compute_efficiency(G_k_prime)\n",
    "            centrality[k] = (efficiency_G - efficiency_G_k_prime) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "\n",
    "    return compute_efficiency_centrality(G)\n",
    "\n",
    "# Method 34 (Local Relative ASP)\n",
    "def method34(G):\n",
    "    def find_diameter(G):\n",
    "        \"\"\"\n",
    "        Find the diameter (longest shortest path) of the graph G.\n",
    "        \"\"\"\n",
    "        if nx.is_connected(G):\n",
    "            return nx.diameter(G)\n",
    "        else:\n",
    "            lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "            diameter = 0\n",
    "            for u in lengths:\n",
    "                for v in lengths[u]:\n",
    "                    if lengths[u][v] > diameter:\n",
    "                        diameter = lengths[u][v]\n",
    "            return diameter\n",
    "\n",
    "    def calculate_asp(G):\n",
    "        \"\"\"\n",
    "        Calculate the Average Shortest Path (ASP) for the graph G.\n",
    "        \"\"\"\n",
    "        n = len(G.nodes)\n",
    "        if n < 2:\n",
    "            return 0\n",
    "        \n",
    "        if nx.is_connected(G):\n",
    "            return nx.average_shortest_path_length(G)\n",
    "        \n",
    "        diameter = find_diameter(G)\n",
    "        asp_sum = 0\n",
    "        \n",
    "        lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "        for u in G.nodes:\n",
    "            for v in G.nodes:\n",
    "                if u != v:\n",
    "                    asp_sum += lengths[u].get(v, diameter)\n",
    "        \n",
    "        return asp_sum / (n * (n - 1))\n",
    "\n",
    "    asp = calculate_asp(G)\n",
    "    AC = {}\n",
    "\n",
    "    for k in G.nodes():\n",
    "        G_removed = G.copy()\n",
    "        G_removed.remove_node(k)\n",
    "        asp_removed = calculate_asp(G_removed)\n",
    "        AC[k] = abs(asp_removed - asp) / asp if asp > 0 else 0\n",
    "\n",
    "    return AC\n",
    "\n",
    "def k_neighbors(G, node, radius):\n",
    "        neighbors = set(nx.single_source_shortest_path_length(G, node, cutoff=radius).keys())\n",
    "        neighbors.discard(node)  # Remove the node itself if present\n",
    "        return neighbors\n",
    "    \n",
    "# Method 35 (InformationRank)\n",
    "def method35(G):\n",
    "    \n",
    "    PROPA = {}\n",
    "    Score = {}\n",
    "    L = 2\n",
    "    Miu = 0.2\n",
    "\n",
    "    # Initialize PROPA and Score dictionaries\n",
    "    for v in G.nodes():\n",
    "        PROPA[v] = {}\n",
    "        Score[v] = 0\n",
    "\n",
    "    # Calculate PROPA and Score\n",
    "    for v in G.nodes():\n",
    "        neighbors = k_neighbors(G, v, L)\n",
    "        for w in neighbors:\n",
    "            PROPA[v][w] = 1\n",
    "            path_length_counts = {}\n",
    "            for path in nx.all_simple_paths(G, v, w, cutoff=L):\n",
    "                length = len(path) - 1\n",
    "                path_length_counts[length] = path_length_counts.get(length, 0) + 1\n",
    "\n",
    "            for l in range(1, L + 1):\n",
    "                pow_val = path_length_counts.get(l, 0)\n",
    "                PROPA[v][w] *= (1 - Miu ** l) ** pow_val\n",
    "            PROPA[v][w] = 1 - PROPA[v][w]\n",
    "            Score[v] += PROPA[v][w]\n",
    "\n",
    "    return Score\n",
    "\n",
    "\n",
    "# Method 36 (Weighted K-shell)\n",
    "def method36(G):\n",
    "    c1 = 0.1\n",
    "    c2 = 0.4\n",
    "    deg = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    ksdw = dict()\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksdw[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            ksdw[vi] += (c1 * deg[vi] + c2 * ks[vi]) * (c1 * deg[vj] + c2 * ks[vj])\n",
    "\n",
    "    return ksdw\n",
    "\n",
    "# Method 37 (HybridKshell)\n",
    "def method37(G):\n",
    "    radius = 2\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(G.degree())\n",
    "    ksh = dict()\n",
    "    landa = 0.4\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksh[vi] = 0\n",
    "        neighbors = k_neighbors(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            ksh[vi] += (math.sqrt(ks[vi] + ks[vj]) + landa * k[vj]) / (nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    return ksh\n",
    "\n",
    "# Optimized method 38 (Social Capital)\n",
    "def method38(G):\n",
    "    SC = {}\n",
    "    K = dict(G.degree())\n",
    "    for i in G.nodes:\n",
    "        SC[i] = K[i] + sum(K[j] for j in G.neighbors(i))\n",
    "    return SC\n",
    "\n",
    "# Optimized method 39 (Potential Edge Weight)\n",
    "def method39(G):\n",
    "    KW = {}\n",
    "    k = dict(G.degree())\n",
    "    Landa = 0.5\n",
    "\n",
    "    for vi in G.nodes:\n",
    "        KW[vi] = Landa * k[vi] + sum((1 - Landa) * (k[vi] + k[vj]) for vj in G.neighbors(vi))\n",
    "        KW[vi] = int(KW[vi])\n",
    "\n",
    "    def find_and_remove_nodes(H, ks):\n",
    "        nodes_to_remove = [node for node in list(H.nodes) if KW[node] <= ks]\n",
    "        H.remove_nodes_from(nodes_to_remove)\n",
    "        return nodes_to_remove\n",
    "\n",
    "    H = G.copy()\n",
    "    ks = min(KW.values())\n",
    "    kshell = {}\n",
    "    tmp = []\n",
    "\n",
    "    while H.nodes:\n",
    "        nodes_to_remove = find_and_remove_nodes(H, ks)\n",
    "        if not nodes_to_remove:\n",
    "            if tmp:\n",
    "                kshell[ks] = tmp\n",
    "            ks += 1\n",
    "            tmp = []\n",
    "        else:\n",
    "            tmp.extend(nodes_to_remove)\n",
    "        if not H.nodes:\n",
    "            kshell[ks] = tmp\n",
    "            break\n",
    "\n",
    "    weightedks = {}\n",
    "    wks = 1\n",
    "    for ks, value in kshell.items():\n",
    "        if value:\n",
    "            weightedks[wks] = value\n",
    "            wks += 1\n",
    "            \n",
    "    output_wks = {}\n",
    "    for weight, nodes in weightedks.items():\n",
    "        for node in nodes:\n",
    "            output_wks[node] = weight\n",
    "\n",
    "    return output_wks\n",
    "\n",
    "# method calculate effg centrality\n",
    "def method40(G):\n",
    "    def calculate_effective_distance(G):\n",
    "        degree = dict(G.degree())\n",
    "        \n",
    "        # Step 1: Calculate the probability P n|m\n",
    "        probability = {(m, n): (1 / degree[m]) if G.has_edge(m, n) else 0 \n",
    "                       for m in G.nodes for n in G.nodes if m != n}\n",
    "\n",
    "        # Step 2: Calculate the effective distance D_{n|m} for directly connected nodes\n",
    "        effective_distance = {(m, n): 1 - np.log2(p) if p > 0 else float('inf') \n",
    "                              for (m, n), p in probability.items()}\n",
    "\n",
    "        # Step 3: Calculate the effective distance for indirectly connected nodes using the shortest path\n",
    "        all_pairs_shortest_path_length = dict(nx.all_pairs_dijkstra_path_length(G))\n",
    "        for m in G.nodes:\n",
    "            for n in G.nodes:\n",
    "                if m != n:\n",
    "                    shortest_path_length = all_pairs_shortest_path_length[m].get(n, float('inf'))\n",
    "                    if shortest_path_length != float('inf'):\n",
    "                        effective_distance[(m, n)] = shortest_path_length\n",
    "\n",
    "        return effective_distance\n",
    "\n",
    "    def calculate_interaction_scores(G, effective_distance):\n",
    "        interaction_scores = {}\n",
    "        for (i, j), d in effective_distance.items():\n",
    "            k_i = G.degree[i]\n",
    "            k_j = G.degree[j]\n",
    "            interaction_scores[(i, j)] = (k_i * k_j) / (d ** 2)\n",
    "        return interaction_scores\n",
    "\n",
    "    def compute_effg_centrality(G, interaction_scores):\n",
    "        effg_centrality = {node: 0 for node in G.nodes()}\n",
    "        for (i, j), score in interaction_scores.items():\n",
    "            effg_centrality[i] += score\n",
    "            effg_centrality[j] += score\n",
    "        return effg_centrality\n",
    "\n",
    "    # Calculate Effective Distance\n",
    "    effective_distance = calculate_effective_distance(G)\n",
    "\n",
    "    # Calculate Interaction Scores\n",
    "    interaction_scores = calculate_interaction_scores(G, effective_distance)\n",
    "\n",
    "    # Calculate EffG Centrality\n",
    "    effg_centrality = compute_effg_centrality(G, interaction_scores)\n",
    "    \n",
    "    return effg_centrality\n",
    "\n",
    "# method 41 (IS-PEW)\n",
    "def method41(G):\n",
    "    # Precompute expensive metrics\n",
    "    triangles = nx.triangles(G)  # Dictionary {node: triangle_count}\n",
    "    core_numbers = nx.core_number(G)  # Dictionary {node: core_number}\n",
    "    degrees = dict(G.degree())  # Dictionary {node: degree}\n",
    "    \n",
    "    # Compute total triangles once\n",
    "    TC = sum(triangles.values()) // 3 if sum(triangles.values()) > 0 else 1  # Avoid division by zero\n",
    "    \n",
    "    def compute_TP(v_A):\n",
    "        return triangles[v_A] / TC\n",
    "    \n",
    "    def compute_InfE(v_A, v_B):\n",
    "        common_neighbors = set(G[v_A]) & set(G[v_B])\n",
    "        sum_DG_k = sum(degrees[k] for k in common_neighbors)\n",
    "        return (degrees[v_A] * degrees[v_B]) / (1 + sum_DG_k)\n",
    "\n",
    "    def compute_NIP(v_A):\n",
    "        return sum(np.sqrt(degrees[v_a] * core_numbers[v_a]) for v_a in G[v_A])\n",
    "\n",
    "    # Store edge weights in a dictionary to avoid recomputation\n",
    "    edge_weights = {}\n",
    "\n",
    "    def compute_EW(v_A, v_B):\n",
    "        if (v_A, v_B) in edge_weights:\n",
    "            return edge_weights[(v_A, v_B)]\n",
    "        \n",
    "        TP_v_A, TP_v_B = compute_TP(v_A), compute_TP(v_B)\n",
    "        KS_v_A, KS_v_B = core_numbers[v_A], core_numbers[v_B]\n",
    "        NIP_v_A, NIP_v_B = compute_NIP(v_A), compute_NIP(v_B)\n",
    "        ED_v_A_v_B = 1 / compute_InfE(v_A, v_B)\n",
    "        \n",
    "        EW_value = ((KS_v_A * (1 + TP_v_A) * NIP_v_A) / ED_v_A_v_B) + ((KS_v_B * (1 + TP_v_B) * NIP_v_B) / ED_v_A_v_B)\n",
    "        edge_weights[(v_A, v_B)] = EW_value  # Store result for future use\n",
    "        edge_weights[(v_B, v_A)] = EW_value  # Store symmetric value\n",
    "        return EW_value\n",
    "\n",
    "    def compute_IS(v_A):\n",
    "        return sum(compute_EW(v_A, v_B) for v_B in G[v_A])\n",
    "\n",
    "    # Compute potential edge weights once\n",
    "    for u, v in G.edges():\n",
    "        G[u][v]['weight'] = compute_EW(u, v)\n",
    "\n",
    "    # Compute IS values for all nodes\n",
    "    IS_values = {v: compute_IS(v) for v in G.nodes()}\n",
    "    \n",
    "    return IS_values\n",
    "\n",
    "# k-shell iteration Factor (KS-IF)\n",
    "def method42(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    iteration_factors = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        if iteration_nodes:\n",
    "            m = iteration_nodes[-1][1]\n",
    "            for node, n in iteration_nodes:\n",
    "                iteration_factors[node] = k * (1 + n / m)\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    ks_IF = {}\n",
    "    deg = dict(G.degree())\n",
    "    for vi in G.nodes:\n",
    "        ks_IF[vi] = iteration_factors[vi] * deg[vi]\n",
    "        for vj in nx.neighbors(G, vi):\n",
    "            ks_IF[vi] += iteration_factors[vj] * deg[vj]\n",
    "\n",
    "    return ks_IF\n",
    "\n",
    "\n",
    "# Method 43 (DKGM)\n",
    "def method43(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    k_star = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        m = iteration_nodes[-1][1]\n",
    "        for node, n in iteration_nodes:\n",
    "            k_star[node] = k_shell[node] + (n / (m + 1))\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    DK = {}\n",
    "    DKGM = {}\n",
    "\n",
    "    for node in k_shell:\n",
    "        DK[node] = G.degree[node] + k_star[node]\n",
    "\n",
    "    R = 2  # Define the radius for DKGM computation\n",
    "    for node in G.nodes():\n",
    "        DKGM[node] = 0\n",
    "        for neighbor in nx.single_source_shortest_path_length(G, node, cutoff=R):\n",
    "            if node != neighbor:\n",
    "                try:\n",
    "                    d_ij = nx.shortest_path_length(G, source=node, target=neighbor)\n",
    "                    DKGM[node] += DK[node] * DK[neighbor] / d_ij**2\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "    return DKGM\n",
    "\n",
    "#modularity_vitality\n",
    "def method44(G):\n",
    "    \"\"\"Detect communities using Leiden algorithm and compute modularity vitality for each node.\"\"\"\n",
    "    \n",
    "    def detect_communities_leiden(G):\n",
    "        \"\"\"Detect communities using the Leiden algorithm.\n",
    "        \n",
    "        This function converts the NetworkX graph to an iGraph graph while preserving node names,\n",
    "        runs the Leiden algorithm, and returns a dictionary mapping node names to community indices.\n",
    "        \"\"\"\n",
    "        # Convert NetworkX graph to iGraph while preserving node names.\n",
    "        ig_G = ig.Graph.TupleList(G.edges(), directed=False, vertex_name_attr='name')\n",
    "        partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "        communities = {}\n",
    "        # Map each iGraph vertex back to the original NetworkX node name.\n",
    "        for idx, comm in enumerate(partition):\n",
    "            for v in comm:\n",
    "                communities[ig_G.vs[v][\"name\"]] = idx\n",
    "        return communities\n",
    "\n",
    "    def modularity(G, communities):\n",
    "        \"\"\"Compute the modularity of graph G with the given community partition.\"\"\"\n",
    "        M = G.number_of_edges()\n",
    "        Q = 0\n",
    "        for c in set(communities.values()):\n",
    "            nodes_c = [n for n in G.nodes() if communities[n] == c]\n",
    "            subgraph = G.subgraph(nodes_c)\n",
    "            L_c = subgraph.number_of_edges()\n",
    "            k_c = sum(G.degree(n) for n in nodes_c)\n",
    "            Q += (L_c / M) - (k_c / (2 * M)) ** 2\n",
    "        return Q\n",
    "\n",
    "    def compute_hi_c(G, node, communities):\n",
    "        \"\"\"Compute h_i,c values for a node as per Equation (10).\"\"\"\n",
    "        k_i = G.degree(node)\n",
    "        hi_c = {}\n",
    "        for c in set(communities.values()):\n",
    "            # Count links from node to neighbors in community c.\n",
    "            k_i_c = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == c)\n",
    "            hi_c[c] = k_i_c + (k_i if communities[node] == c else 0)\n",
    "        return hi_c\n",
    "\n",
    "    def modularity_vitality(G, communities, node):\n",
    "        \"\"\"Compute the modularity vitality of a node using Equation (9).\"\"\"\n",
    "        Q_initial = modularity(G, communities)\n",
    "        if not G.has_node(node):\n",
    "            return 0  \n",
    "\n",
    "        M = G.number_of_edges()\n",
    "        k_i = G.degree(node)\n",
    "        c_i = communities[node]\n",
    "        \n",
    "        # Compute h_i,c for the given node.\n",
    "        hi_c = compute_hi_c(G, node, communities)\n",
    "        \n",
    "        # Compute the total degree for each community.\n",
    "        d_c = {c: sum(G.degree(n) for n in G.nodes() if communities[n] == c)\n",
    "               for c in set(communities.values())}\n",
    "\n",
    "        # Get nodes in the same community as node.\n",
    "        community_nodes = [n for n in G.nodes() if communities[n] == c_i]\n",
    "        M_internal = G.subgraph(community_nodes).number_of_edges()\n",
    "        k_i_internal = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == c_i)\n",
    "        \n",
    "        # Compute the sum term over all communities.\n",
    "        sum_term = sum((d_c[c] - hi_c[c]) ** 2 for c in hi_c)\n",
    "        \n",
    "        # Compute updated modularity based on Equation (9).\n",
    "        Q_updated = (M_internal - k_i_internal) / (M - k_i) - (1 / (4 * (M - k_i) ** 2)) * sum_term\n",
    "\n",
    "        return Q_initial - Q_updated\n",
    "\n",
    "    # Detect communities using the Leiden algorithm.\n",
    "    communities = detect_communities_leiden(G)\n",
    "\n",
    "    # Compute modularity vitality for each node.\n",
    "    mod_vit = {node: modularity_vitality(G, communities, node) for node in G.nodes()}\n",
    "    \n",
    "    return mod_vit\n",
    "\n",
    "#Global-and-Local Centrality (GLC)\n",
    "def method45(G, lam=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Global-and-Local Centrality (GLC) of each node in graph G.\n",
    "    \n",
    "    The method first clusters the network using a potential-based approach,\n",
    "    then selects global critical nodes from each cluster, computes local influence\n",
    "    based on k-shell values, and finally calculates the overall GLC value.\n",
    "    \n",
    "    Parameters:\n",
    "        G   : networkx.Graph\n",
    "              The input graph.\n",
    "        lam : float (default=1.0)\n",
    "              The fraction (lambda) of nodes that must be covered by clusters.\n",
    "    \n",
    "    Returns:\n",
    "        GLC : dict\n",
    "              A dictionary mapping each node in G to its GLC centrality value.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Clustering and Global Critical Node Selection ---\n",
    "    clusters = []       # List to hold all clusters\n",
    "    assigned = set()    # Set of nodes that have been assigned or marked\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Helper function: Compute the potential of a node as defined in Eq.(10)\n",
    "    def compute_potential(node):\n",
    "        # Potential of node = degree(node) * (sum over j in N(node) of kin(j))\n",
    "        # where kin(j) = number of neighbors of j that are in {node} U N(node)\n",
    "        nbrs = set(G.neighbors(node))\n",
    "        target_set = nbrs.union({node})\n",
    "        s = 0\n",
    "        for j in nbrs:\n",
    "            s += sum(1 for nbr in G.neighbors(j) if nbr in target_set)\n",
    "        return G.degree(node) * s\n",
    "\n",
    "    # Continue clustering until the number of assigned nodes reaches lam * total_nodes\n",
    "    while len(assigned) < total_nodes * lam:\n",
    "        # Compute potentials for all unassigned nodes.\n",
    "        potentials = {}\n",
    "        for node in G.nodes():\n",
    "            if node not in assigned:\n",
    "                potentials[node] = compute_potential(node)\n",
    "        if not potentials:\n",
    "            break  # All nodes have been assigned\n",
    "        \n",
    "        # Select seed node with maximum potential.\n",
    "        seed = max(potentials, key=potentials.get)\n",
    "        pcmax = potentials[seed]\n",
    "        new_cluster = set([seed])\n",
    "        assigned.add(seed)\n",
    "        \n",
    "        # Include neighbors of the seed with potential >= pcmax/2.\n",
    "        for neighbor in G.neighbors(seed):\n",
    "            if neighbor not in assigned and compute_potential(neighbor) >= pcmax / 2:\n",
    "                new_cluster.add(neighbor)\n",
    "                assigned.add(neighbor)\n",
    "                \n",
    "        # Expand the cluster iteratively (three degrees of influence).\n",
    "        for _ in range(3):\n",
    "            # Gather neighbors of the current cluster (excluding nodes already in the cluster)\n",
    "            cluster_neighbors = set()\n",
    "            for node in new_cluster:\n",
    "                for nbr in G.neighbors(node):\n",
    "                    if nbr not in new_cluster:\n",
    "                        cluster_neighbors.add(nbr)\n",
    "            # Process neighbors in order of increasing degree.\n",
    "            cluster_neighbors = sorted(cluster_neighbors, key=lambda x: G.degree(x))\n",
    "            \n",
    "            added_flag = False\n",
    "            for candidate in cluster_neighbors:\n",
    "                # Count the number of edges from candidate to nodes in new_cluster (kin)\n",
    "                kin = sum(1 for nbr in G.neighbors(candidate) if nbr in new_cluster)\n",
    "                # Count the remaining edges from candidate to nodes outside new_cluster (kout)\n",
    "                kout = sum(1 for nbr in G.neighbors(candidate) if nbr not in new_cluster)\n",
    "                if kin >= kout:\n",
    "                    new_cluster.add(candidate)\n",
    "                    assigned.add(candidate)\n",
    "                    added_flag = True\n",
    "            if not added_flag:\n",
    "                break  # No more nodes can be added in this iteration\n",
    "        \n",
    "        # Mark all neighbors of the new cluster as assigned to avoid re-selection.\n",
    "        for node in new_cluster:\n",
    "            for nbr in G.neighbors(node):\n",
    "                assigned.add(nbr)\n",
    "        clusters.append(new_cluster)\n",
    "    \n",
    "    # From each cluster, select the global critical node (node with highest degree).\n",
    "    global_critical = []\n",
    "    for cluster in clusters:\n",
    "        if cluster:\n",
    "            gc = max(cluster, key=lambda node: G.degree(node))\n",
    "            global_critical.append(gc)\n",
    "    \n",
    "    # --- Step 2: Local Influence Calculation ---\n",
    "    # Compute the k-shell (core) numbers for all nodes.\n",
    "    core_numbers = nx.core_number(G)\n",
    "    \n",
    "    # Compute local influence LI for each node:\n",
    "    # LI(node) = sum_{j in N(node)} k_shell(j)\n",
    "    LI = {}\n",
    "    for node in G.nodes():\n",
    "        li = 0\n",
    "        for nbr in G.neighbors(node):\n",
    "            li += core_numbers[nbr]\n",
    "        LI[node] = li\n",
    "\n",
    "    # --- Step 3: Overall GLC Centrality Calculation ---\n",
    "    # For each node v, compute:\n",
    "    #   GlobalFactor(v) = sum_{u in global_critical} (LI(u) / (2 * max(d(v,u),1)))\n",
    "    #   GLC(v) = LI(v) * GlobalFactor(v)\n",
    "    GLC = {}\n",
    "    for v in G.nodes():\n",
    "        global_factor = 0\n",
    "        for u in global_critical:\n",
    "            try:\n",
    "                d = nx.shortest_path_length(G, source=v, target=u)\n",
    "            except nx.NetworkXNoPath:\n",
    "                d = float('inf')\n",
    "            d = max(d, 1)  # Avoid division by zero\n",
    "            global_factor += LI[u] / (2 * d)\n",
    "        GLC[v] = LI[v] * global_factor\n",
    "\n",
    "    return GLC\n",
    "\n",
    "#methods = {\n",
    "#    \"Degree Centrality\": method1,\n",
    "#    \"Betweenness Centrality\": method2,\n",
    "#    \"Closeness Centrality\": method3,\n",
    "#    \"K-shell\": method4,\n",
    "#    \"Cnc\": method5,\n",
    "#    \"Cnc Plus\": method6,\n",
    "#    \"Distance to Network Core\": method7,\n",
    "#    \"All Around Node\": method8,\n",
    "#    \"H-index Centrality\": method9,\n",
    "#    \"Local H-index\": method10,\n",
    "#    \"Semi-Local Centrality\": method11,\n",
    "#    \"DNC\": method12,\n",
    "#    \"ECLGC\": method13,\n",
    "#    \"Centripetal Centrality\": method14,\n",
    "#    \"Cluster Rank\": method15,\n",
    "#    \"Improved Gravity Centrality\":method16,\n",
    "#    \"EDBC\" : method17,\n",
    "#    \"Entropy Centrality\": method18,\n",
    "#    \"KNC\" : method19,\n",
    "#    \"NEDC\" : method20,\n",
    "#    \"LGC\" : method21,\n",
    "#    \"SEGM\" : method22,\n",
    "#    \"MCGM\" : method23,\n",
    "#    \"HVGC\" : method24,\n",
    "#    \"MDD\" : method25,\n",
    "#    \"BaseGM\" : method26,\n",
    "#    \"GlobalGM\" : method27,\n",
    "#    \"LocalGM\" : method28,\n",
    "#    \"WGravity\" : method29,\n",
    "#    \"HKS\" : method30,\n",
    "#    \"SHKS\" : method31,\n",
    "#    \"KSGC\" : method32,\n",
    "#    \"EFFC\" : method33,\n",
    "#    \"Local Relative ASP\" : method34,\n",
    "#    \"Information Rank\" : method35,\n",
    "#    \"Weighted K-shell\" : method36,\n",
    "#    \"Hybrid K-shell\" : method37,\n",
    "#    \"Social Capital\" : method38,\n",
    "#    \"Potential Edge Weight\" : method39,\n",
    "#    \"effg Gravity\" : method40,\n",
    "#    \"IS-PEW\" : method41,\n",
    "#    \"KS-IF\" : method42,\n",
    "#    \"DKGM\" : method43,\n",
    "#    \"modularity vitality\":method44,\n",
    "#    \"Global-and-Local Centrality (GLC)\":method45\n",
    "# }\n",
    "\n",
    "methods = [globals()[f'method{i}'] for i in range(1, 46)]\n",
    "\n",
    "dataset_folder = 'dataset'\n",
    "beta_values = [0.02, 0.05, 0.08, 0.10, 0.13, 0.15, 0.18, 0.20, 0.22, 0.25, 0.30]\n",
    "k_factors = [0.01, 0.03, 0.05, 0.07, 0.085, 0.10]\n",
    "f_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "for dataset_filename in os.listdir(dataset_folder):\n",
    "    if dataset_filename.endswith('.xlsx'):\n",
    "        dataset_path = os.path.join(dataset_folder, dataset_filename)\n",
    "        df_edges = pd.read_excel(dataset_path, sheet_name='Sheet1')\n",
    "        G = nx.from_pandas_edgelist(df_edges, source='source_column', target='target_column')\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "        \n",
    "        df_sir = pd.read_excel(dataset_path, sheet_name='SIR')\n",
    "        df_sir.set_index('Node', inplace=True)\n",
    "        \n",
    "        num_nodes = len(G.nodes())\n",
    "        results = {}\n",
    "        method_results = {}\n",
    "        \n",
    "        for i, method in enumerate(methods, start=1):\n",
    "            method_name = f'method{i}'\n",
    "            try:\n",
    "                ranked_nodes, exec_time = measure_execution_time(method, G)\n",
    "                monotonicity = compute_monotonicity(G, ranked_nodes)\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': ranked_nodes,\n",
    "                    'execution_time': exec_time,\n",
    "                    'monotonicity': monotonicity\n",
    "                }\n",
    "            except Exception as e:\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': [],\n",
    "                    'execution_time': None,\n",
    "                    'monotonicity': None,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        \n",
    "        for beta in beta_values:\n",
    "            beta_column = f'Beta_{beta:.2f}'\n",
    "            if beta_column not in df_sir.columns:\n",
    "                print(f\"Warning: {beta_column} not found in {dataset_filename}. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            spread_power = df_sir[beta_column].to_dict()\n",
    "            sigma = sorted(spread_power.keys(), key=lambda x: spread_power[x], reverse=True)\n",
    "            \n",
    "            for method_name, method_data in method_results.items():\n",
    "                if 'error' in method_data:\n",
    "                    results[(method_name, beta)] = {'Beta': beta, 'Kendall Tau': None, 'P-Value': None}\n",
    "                    continue\n",
    "                \n",
    "                ranked_nodes = method_data['ranked_nodes']\n",
    "                tau, p_value = kendalltau(\n",
    "                    [spread_power[node] for node in sigma],\n",
    "                    [spread_power[node] for node, _ in ranked_nodes]\n",
    "                )\n",
    "                \n",
    "                R = [node for node, _ in ranked_nodes]\n",
    "                jaccard_scores = {}\n",
    "                si_scores = {}\n",
    "                rbo_scores = {}\n",
    "                mrr_scores = compute_mrr(sigma, R, k_factors)\n",
    "                \n",
    "                for k_factor in k_factors:\n",
    "                    k = max(1, int(k_factor * num_nodes))\n",
    "                    top_k_sigma = set(sigma[:k])\n",
    "                    top_k_R = set(R[:k])\n",
    "                    jaccard_scores[f'Jaccard k={k}'] = compute_jaccard_similarity(top_k_sigma, top_k_R)\n",
    "                \n",
    "                for f in f_values:\n",
    "                    f_count = max(1, int(f * num_nodes))\n",
    "                    top_f_nodes = [node for node, _ in ranked_nodes][:f_count]\n",
    "                    si_scores[f'SI {f:.2f}'] = sum(spread_power[node] for node in top_f_nodes) / (f * num_nodes)\n",
    "                    rbo_scores[f'RBO {f:.2f}'] = compute_rbo(sigma[:f_count], R[:f_count])\n",
    "                \n",
    "                results[(method_name, beta)] = {\n",
    "                    'Beta': beta,\n",
    "                    'Kendall Tau': tau,\n",
    "                    'P-Value': p_value,\n",
    "                    **jaccard_scores,\n",
    "                    **si_scores,\n",
    "                    **rbo_scores,\n",
    "                    **mrr_scores\n",
    "                }\n",
    "        \n",
    "        data = []\n",
    "        for (method_name, beta), result in results.items():\n",
    "            row = [\n",
    "                method_name,\n",
    "                method_results[method_name]['execution_time'],\n",
    "                method_results[method_name].get('monotonicity', 'N/A'),\n",
    "                result.get('Beta'),\n",
    "                result.get('Kendall Tau'),\n",
    "                result.get('P-Value')\n",
    "            ] + [result.get(f'Jaccard k={int(k_factor * num_nodes)}') for k_factor in k_factors] + \\\n",
    "              [result.get(f'SI {f:.2f}') for f in f_values] + \\\n",
    "              [result.get(f'RBO {f:.2f}') for f in f_values] + \\\n",
    "              [result.get(f'MRR {k_factor:.2f}') for k_factor in k_factors]\n",
    "            \n",
    "            data.append(row)\n",
    "        \n",
    "        expected_columns = ['Method', 'Execution Time', 'Monotonicity', 'Beta', 'Kendall Tau', 'P-Value'] + \\\n",
    "                           [f'Jaccard k={int(k_factor * num_nodes)}' for k_factor in k_factors] + \\\n",
    "                           [f'SI {f:.2f}' for f in f_values] + \\\n",
    "                           [f'RBO {f:.2f}' for f in f_values] + \\\n",
    "                           [f'MRR {k_factor:.2f}' for k_factor in k_factors]\n",
    "        \n",
    "        result_df = pd.DataFrame(data, columns=expected_columns)\n",
    "        \n",
    "        dataset_name = os.path.splitext(dataset_filename)[0]\n",
    "        output_file = f'{dataset_name}_results.xlsx'\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df_edges.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "            df_sir.to_excel(writer, sheet_name='SIR')\n",
    "            result_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        \n",
    "        print(f\"Processed {dataset_filename}, results saved in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac80c8-79b7-4b03-9ebc-a5bfd691a0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
