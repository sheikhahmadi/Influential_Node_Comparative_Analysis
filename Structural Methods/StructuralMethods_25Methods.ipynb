{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59181e7-9671-4771-ad80-34ad76b56c23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: EgoFacebook(SIR)...\n",
      "\n",
      "Applying method1 on EgoFacebook(SIR)...\n",
      "Applying method2 on EgoFacebook(SIR)...\n",
      "Applying method3 on EgoFacebook(SIR)...\n",
      "Applying method4 on EgoFacebook(SIR)...\n",
      "Applying method5 on EgoFacebook(SIR)...\n",
      "Applying method6 on EgoFacebook(SIR)...\n",
      "Applying method7 on EgoFacebook(SIR)...\n",
      "Applying method8 on EgoFacebook(SIR)...\n",
      "Applying method9 on EgoFacebook(SIR)...\n",
      "Applying method10 on EgoFacebook(SIR)...\n",
      "Applying method11 on EgoFacebook(SIR)...\n",
      "Applying method12 on EgoFacebook(SIR)...\n",
      "Applying method13 on EgoFacebook(SIR)...\n",
      "Applying method14 on EgoFacebook(SIR)...\n",
      "Applying method15 on EgoFacebook(SIR)...\n",
      "Applying method16 on EgoFacebook(SIR)...\n",
      "Applying method17 on EgoFacebook(SIR)...\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "def measure_execution_time(method, G):\n",
    "    start_time = time.time()\n",
    "    result = method(G)\n",
    "    ranked_nodes = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return ranked_nodes, execution_time\n",
    "\n",
    "def compute_monotonicity(G, ranked_nodes):\n",
    "    ranks = [score for node, score in ranked_nodes]\n",
    "    unique_ranks = list(set(ranks))\n",
    "    n = G.number_of_nodes()\n",
    "    nr_dict = {rank: ranks.count(rank) for rank in unique_ranks}\n",
    "    nr_sum = sum(nr * (nr - 1) for nr in nr_dict.values())\n",
    "    M = (1 - nr_sum / (n * (n - 1))) ** 2\n",
    "    return M\n",
    "\n",
    "def compute_mrr(ground_truth, ranked_list, top_ratios):\n",
    "    mrr_scores = {}\n",
    "    num_nodes = len(ground_truth)\n",
    "    \n",
    "    for ratio in top_ratios:\n",
    "        k = max(1, int(ratio * num_nodes))\n",
    "        reciprocal_ranks = []\n",
    "        \n",
    "        for node in ground_truth[:k]:\n",
    "            if node in ranked_list:\n",
    "                rank = ranked_list.index(node) + 1\n",
    "                reciprocal_ranks.append(1 / rank)\n",
    "        \n",
    "        mrr_scores[f'MRR {ratio:.2f}'] = sum(reciprocal_ranks) / k if reciprocal_ranks else 0\n",
    "    \n",
    "    return mrr_scores\n",
    "\n",
    "\n",
    "def compute_rbo(sigma, R, alpha=0.9):\n",
    "    \"\"\"Compute the Rank-Biased Overlap (RBO) between two ranking lists.\"\"\"\n",
    "    def A(sigma, R, f):\n",
    "        sigma_f = set(sigma[:f])\n",
    "        R_f = set(R[:f])\n",
    "        if len(sigma_f) == 0 and len(R_f) == 0:\n",
    "            return 1\n",
    "        return len(sigma_f & R_f) / len(sigma_f | R_f)\n",
    "\n",
    "    n = max(len(sigma), len(R))\n",
    "    rbo = (1 - alpha) * sum(alpha**(f - 1) * A(sigma, R, f) for f in range(1, n + 1))\n",
    "    return rbo\n",
    "\n",
    "def compute_jaccard_similarity(set1, set2):\n",
    "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0\n",
    "\n",
    "# Define methods\n",
    "# method1: Degree\n",
    "def method1(G):\n",
    "    return nx.degree_centrality(G)\n",
    "\n",
    "# method2: Betweenness Centrality\n",
    "def method2(G):\n",
    "    return nx.betweenness_centrality(G)\n",
    "\n",
    "# method3: Closeness Centrality\n",
    "def method3(G):\n",
    "    return nx.closeness_centrality(G)\n",
    "\n",
    "# method4: k-shell\n",
    "def method4(G):\n",
    "    return nx.core_number(G)\n",
    "\n",
    "def CnC(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = {vi: sum(ks[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc\n",
    "\n",
    "# method5: CncPlus\n",
    "def method5(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = CnC(G)\n",
    "    cnc_plus = {vi: sum(cnc[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc_plus\n",
    "\n",
    "def HIndex(G):\n",
    "    h_index = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degrees = sorted([G.degree(neighbor) for neighbor in neighbors], reverse=True)\n",
    "        h = 0\n",
    "        for i, degree in enumerate(degrees):\n",
    "            if degree >= i + 1:\n",
    "                h = i + 1\n",
    "            else:\n",
    "                break\n",
    "        h_index[node] = h\n",
    "    return h_index\n",
    "\n",
    "# method6: Local H-Index\n",
    "def method6(G):\n",
    "    HI = HIndex(G)\n",
    "    LHI = {vi: HI[vi] + sum(HI[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return LHI\n",
    "\n",
    "# method7: DNC\n",
    "def method7(G):\n",
    "    SLC = {}\n",
    "    k = dict(nx.degree(G))\n",
    "    C = nx.clustering(G)\n",
    "    for i in G.nodes():\n",
    "        SLC[i] = sum(C[j] for j in G.neighbors(i))\n",
    "    \n",
    "    DNC = {}\n",
    "    alpha = 1\n",
    "    for i in G.nodes():\n",
    "        DNC[i] = k[i] + alpha * SLC[i]\n",
    "    return DNC\n",
    "\n",
    "#method8:ECLGC \n",
    "# Method 8: ECLGC\n",
    "def method8(G):\n",
    "    # Precompute clustering coefficients\n",
    "    CLC = nx.clustering(G)\n",
    "    \n",
    "    # Precompute neighbors for each node\n",
    "    neighbors = {node: list(G.neighbors(node)) for node in G.nodes()}\n",
    "    \n",
    "    # Precompute shortest path lengths for all node pairs\n",
    "    sp_lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    \n",
    "    # Compute CLGC (not used later, but computed as in the original code)\n",
    "    CLGC = {}\n",
    "    for i in G.nodes():\n",
    "        s = 0.0\n",
    "        for j in G.nodes():\n",
    "            if i == j:\n",
    "                continue\n",
    "            if j in sp_lengths[i]:\n",
    "                s += math.sqrt(CLC[j]) / sp_lengths[i][j]\n",
    "        CLGC[i] = s * CLC[i]\n",
    "    \n",
    "    # Compute ECLGC\n",
    "    ECLGC = {}\n",
    "    for v in G.nodes():\n",
    "        sum1 = 0.0\n",
    "        sum3 = 0.0\n",
    "        neighbors_v = neighbors[v]\n",
    "        if not neighbors_v:  # if no neighbors, ECLGC is 0\n",
    "            ECLGC[v] = 0\n",
    "            continue\n",
    "        \n",
    "        for u in G.nodes():\n",
    "            if u == v:\n",
    "                continue\n",
    "            if u in sp_lengths[v]:\n",
    "                # Precompute sum2 for u's neighbors\n",
    "                neighbors_u = neighbors[u]\n",
    "                lu = len(neighbors_u) if neighbors_u else 1  # avoid division by zero\n",
    "                sum2 = 0.0\n",
    "                for j in neighbors_u:\n",
    "                    sum2 += CLC[j] / lu\n",
    "                \n",
    "                sum1 += CLC[u] / len(neighbors_v)\n",
    "                d = sp_lengths[v][u]  # shortest path length from v to u\n",
    "                sum3 += math.sqrt(CLC[u] + sum2) / d\n",
    "        ECLGC[v] = sum1 * sum3\n",
    "    return ECLGC\n",
    "\n",
    "\n",
    "def get_neighborhood(G, node, radius):\n",
    "    neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "    neighborhood.pop(node, None)\n",
    "    return list(neighborhood.keys())\n",
    "\n",
    "\n",
    "# method9: CenC\n",
    "def compute_alpha(graph):\n",
    "    size_of_V = graph.number_of_nodes()\n",
    "    # Ensure order_of_magnitude is at least 1 to avoid division by zero.\n",
    "    order_of_magnitude = max(int(math.log10(size_of_V)), 1)\n",
    "    alpha = size_of_V / order_of_magnitude\n",
    "    return alpha\n",
    "\n",
    "def method9(G, m=1):\n",
    "    alpha = compute_alpha(G)\n",
    "    # Cap alpha to avoid math range error in math.exp (overflow for very large alpha)\n",
    "    capped_alpha = min(alpha, 700)\n",
    "    \n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    kshell = nx.core_number(G)\n",
    "    \n",
    "    centrality = {}\n",
    "    \n",
    "    for i in G.nodes():\n",
    "        cenC_i = 0\n",
    "        \n",
    "        # Precompute the m-neighborhood and distances from i\n",
    "        sp_lengths_i = nx.single_source_shortest_path_length(G, i, cutoff=m)\n",
    "        m_neighborhood = set(sp_lengths_i.keys())\n",
    "        m_neighborhood.discard(i)  # Exclude the node itself\n",
    "        \n",
    "        for j in m_neighborhood:\n",
    "            d_ij = sp_lengths_i[j]  # Use precomputed distance\n",
    "            if d_ij == 0:\n",
    "                continue\n",
    "            \n",
    "            DC_i = degree_centrality[i]\n",
    "            C_i = clustering_coeff[i]\n",
    "            k_s_i = kshell[i]\n",
    "            k_s_j = kshell[j]\n",
    "            \n",
    "            # Compute term1 with a safe exponentiation\n",
    "            term1 = (math.exp(capped_alpha) * DC_i + (1 / C_i if C_i != 0 else 0))\n",
    "            term2 = k_s_j / (abs(k_s_i - k_s_j) + 1)\n",
    "            \n",
    "            cenC_i += (4 * math.pi**2) * (term1 / (d_ij**2)) * term2\n",
    "        \n",
    "        centrality[i] = cenC_i\n",
    "    \n",
    "    return centrality\n",
    "\n",
    "def k_shell(graph):\n",
    "    return nx.core_number(graph)\n",
    "\n",
    "def evaluate_power_k_shell(k_si, k_sj):\n",
    "    return math.sqrt(k_si + k_sj)\n",
    "\n",
    "def compute_P_ji(graph, node_i, node_j):\n",
    "    try:\n",
    "        a_ji = graph[node_j][node_i]['weight'] if 'weight' in graph[node_j][node_i] else 1\n",
    "        total_a_ji = sum(graph[node_j][nbr]['weight'] if 'weight' in graph[node_j][nbr] else 1 for nbr in graph.neighbors(node_j))\n",
    "        return a_ji / total_a_ji\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def evaluate_ED(graph, node_i, node_j):\n",
    "    P_ji = compute_P_ji(graph, node_i, node_j)\n",
    "    if P_ji == 0:\n",
    "        return 1  # Prevent log(0)\n",
    "    return max(1 - math.log(P_ji), 1)\n",
    "\n",
    "def evaluate_influence(graph, node_i, node_j, k_shell_values):\n",
    "    alpha_ij = evaluate_power_k_shell(k_shell_values[node_i], k_shell_values[node_j])\n",
    "    ED_ij = evaluate_ED(graph, node_i, node_j)\n",
    "    degree_vi = graph.degree(node_i)\n",
    "    return alpha_ij * degree_vi / ED_ij\n",
    "\n",
    "# EDBC\n",
    "def method10(G):\n",
    "    k_shell_values = k_shell(G)\n",
    "    EDBC = {u: 0 for u in G.nodes()}\n",
    "    \n",
    "    for u in G.nodes():\n",
    "        for v in G.neighbors(u):\n",
    "            for neighbor_u in G.neighbors(u):\n",
    "                EDBC[v] += evaluate_influence(G, v, neighbor_u, k_shell_values)\n",
    "    \n",
    "    return EDBC\n",
    "\n",
    "# Method 11 (LGC)\n",
    "def method11(G):\n",
    "    LC = {}\n",
    "    K = dict(nx.degree(G))\n",
    "    for i in G.nodes():\n",
    "        LC[i] = sum(K[j] for j in nx.neighbors(G, i)) * 2 + K[i] ** 2 + K[i]\n",
    "\n",
    "    LGC = {}\n",
    "    radius = 3\n",
    "    for vi in G.nodes():\n",
    "        LGC[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, radius)\n",
    "        for vj in neighbors:\n",
    "            try:\n",
    "                d_ij = nx.shortest_path_length(G, vi, vj)\n",
    "                if d_ij != 0:\n",
    "                    LGC[vi] += LC[vi] * LC[vj] / d_ij ** 2\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "    return LGC\n",
    "\n",
    "# Method 12 (SEGM)\n",
    "def method12(G):\n",
    "    k = dict(G.degree())\n",
    "    I = dict()\n",
    "    E = dict()\n",
    "    SE = dict()\n",
    "\n",
    "    for node in G.nodes:\n",
    "        I[node] = 0\n",
    "\n",
    "    for i in G.nodes:\n",
    "        sum_kj = 0\n",
    "        for j in G.neighbors(i):\n",
    "            sum_kj += k[j]\n",
    "        if sum_kj != 0:\n",
    "            I[i] = k[i] / sum_kj\n",
    "        sum_e = 0\n",
    "        for j in G.neighbors(i):\n",
    "            if I[j] != 0:\n",
    "                sum_e += I[j] * np.log(I[j])\n",
    "        E[i] = -1 * sum_e\n",
    "        SE[i] = np.exp(E[i]) * k[i]\n",
    "\n",
    "    SEGM = dict()\n",
    "    R = 3\n",
    "    for i in G.nodes:\n",
    "        SEGM[i] = 0\n",
    "        neighbors = get_neighborhood(G, i, R)\n",
    "        for j in neighbors:\n",
    "            if nx.shortest_path_length(G, i, j) != 0:\n",
    "                SEGM[i] += (SE[i] * SE[j]) / (nx.shortest_path_length(G, i, j) ** 2)\n",
    "\n",
    "    return SEGM\n",
    "\n",
    "# Method 13 (MCGM)\n",
    "def method13(G):\n",
    "    # Compute degree and core number\n",
    "    k = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    \n",
    "    # Compute eigenvector centrality with increased iterations.\n",
    "    try:\n",
    "        x = nx.eigenvector_centrality(G, max_iter=500, tol=1e-06)\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        # Fallback to NumPy-based eigenvector centrality if convergence fails.\n",
    "        x = nx.eigenvector_centrality_numpy(G)\n",
    "    \n",
    "    # Convert centrality measures to arrays for median and max computations.\n",
    "    k_values = np.array(list(k.values()))\n",
    "    ks_values = np.array(list(ks.values()))\n",
    "    x_values = np.array(list(x.values()))\n",
    "    \n",
    "    # Compute medians and maximum values.\n",
    "    k_mid = np.median(k_values)\n",
    "    ks_mid = np.median(ks_values)\n",
    "    x_mid = np.median(x_values)\n",
    "    \n",
    "    k_max = np.max(k_values)\n",
    "    ks_max = np.max(ks_values)\n",
    "    x_max = np.max(x_values)\n",
    "    \n",
    "    # Calculate alpha, ensuring no division by zero.\n",
    "    if ks_mid == 0:\n",
    "        alpha = 0\n",
    "    else:\n",
    "        alpha = max(k_mid / k_max, x_mid / x_max) / (ks_mid / ks_max)\n",
    "    \n",
    "    MCGM = {}\n",
    "    R = 3  # Radius for considering node pairs.\n",
    "    \n",
    "    # Precompute shortest path lengths for all nodes with a cutoff of R.\n",
    "    sp_lengths = dict(nx.all_pairs_shortest_path_length(G, cutoff=R))\n",
    "    \n",
    "    for i in G.nodes():\n",
    "        MCGM[i] = 0\n",
    "        # Iterate over nodes reachable from i within cutoff R.\n",
    "        for j, d in sp_lengths.get(i, {}).items():\n",
    "            if i == j or d == 0:\n",
    "                continue\n",
    "            # Only consider nodes within radius R.\n",
    "            if d <= R:\n",
    "                # Compute contribution from node j.\n",
    "                term_i = (k[i] / k_max + alpha * ks[i] / ks_max + x[i] / x_max)\n",
    "                term_j = (k[j] / k_max + alpha * ks[j] / ks_max + x[j] / x_max)\n",
    "                MCGM[i] += term_i * term_j / (d ** 2)\n",
    "    \n",
    "    return MCGM\n",
    "\n",
    "# Method 14 (HVGC)\n",
    "def compute_H_index(G, i):\n",
    "    degrees = [G.degree(neighbor) for neighbor in G.neighbors(i)]\n",
    "    degrees.sort(reverse=True)\n",
    "    H_index = 0\n",
    "    for idx, degree in enumerate(degrees):\n",
    "        if degree >= idx + 1:\n",
    "            H_index = idx + 1\n",
    "        else:\n",
    "            break\n",
    "    return H_index\n",
    "\n",
    "def compute_H_v(G, i):\n",
    "    H_i = compute_H_index(G, i)\n",
    "    H_v_i = sum(G.degree(j) for j in G.neighbors(i) if G.degree(j) >= H_i)\n",
    "    return H_v_i\n",
    "\n",
    "def compute_c_i(G, i):\n",
    "    def compute_p_ij(G, i, j):\n",
    "        neighbors_i = list(G.neighbors(i))\n",
    "        sum_z_iw = sum(G[i][w].get('weight', 1) for w in neighbors_i)\n",
    "        p_ij = G[i][j].get('weight', 1) / sum_z_iw if sum_z_iw != 0 else 0\n",
    "        return p_ij\n",
    "\n",
    "    c_i = 0\n",
    "    neighbors_i = list(G.neighbors(i))\n",
    "    for j in neighbors_i:\n",
    "        p_ij = compute_p_ij(G, i, j)\n",
    "        neighbors_j = list(G.neighbors(j))\n",
    "        common_neighbors = set(neighbors_i).intersection(neighbors_j)\n",
    "        sum_piw_pwj = sum(compute_p_ij(G, i, w) * compute_p_ij(G, w, j) for w in common_neighbors)\n",
    "        term = p_ij + sum_piw_pwj\n",
    "        c_i += term ** 2\n",
    "    \n",
    "    return c_i\n",
    "\n",
    "def compute_HVGC(G, i, H_v, R):\n",
    "    c_i = compute_c_i(G, i)\n",
    "    HVGC_i = 0\n",
    "    for j in G.nodes():\n",
    "        if j != i:\n",
    "            d_ij = nx.shortest_path_length(G, source=i, target=j)\n",
    "            if d_ij <= R:\n",
    "                HVGC_i += (math.exp(-c_i) * H_v[i] * H_v[j]) / (d_ij ** 2)\n",
    "    return HVGC_i\n",
    "\n",
    "#HVGC\n",
    "def method14(G):\n",
    "    H_v = {i: compute_H_v(G, i) for i in G.nodes()}\n",
    "    R = 2\n",
    "    HVGC = {node: compute_HVGC(G, node, H_v, R) for node in G.nodes()}\n",
    "    return HVGC\n",
    "\n",
    "\n",
    "# Method15 (BaseGM)\n",
    "def method15(G):\n",
    "    def get_neighborhood(G, node, radius):\n",
    "        neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "        neighborhood.pop(node, None)\n",
    "        return list(neighborhood.keys())\n",
    "\n",
    "    gravity = dict()\n",
    "    kshell = nx.core_number(G)\n",
    "    for vi in G.nodes():\n",
    "        gravity[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, 3)\n",
    "        for vj in neighbors:\n",
    "            gravity[vi] += ((kshell[vi] * kshell[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    gravity_plus = dict()\n",
    "    for vi in G.nodes():\n",
    "        gravity_plus[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            gravity_plus[vi] += gravity[vj]\n",
    "\n",
    "    return gravity_plus\n",
    "\n",
    "# Method 16 (HKS)\n",
    "def method16(G):\n",
    "    def set_b_values(G):\n",
    "        b, Shell, b_values = 1, 1, {}\n",
    "        while G.number_of_nodes() > 0:\n",
    "            flag = False\n",
    "            for v in list(G.nodes):\n",
    "                if G.degree[v] <= Shell:\n",
    "                    b_values[v] = b\n",
    "                    flag = True\n",
    "            \n",
    "            if flag:\n",
    "                G.remove_nodes_from([v for v in list(G.nodes) if b_values.get(v) == b])\n",
    "                b += 1\n",
    "            else:\n",
    "                Shell += 1\n",
    "        \n",
    "        return b_values\n",
    "\n",
    "    def set_f(G, b_values):\n",
    "        V, f = list(G.nodes), max(b_values.values())\n",
    "        fi = {v: b_values[v] if all(b_values[v] >= b_values[vj] for vj in G.neighbors(v)) else 0 for v in V}\n",
    "\n",
    "        while V:\n",
    "            for vi in V:\n",
    "                if fi[vi] == f:\n",
    "                    for vj in G.neighbors(vi):\n",
    "                        if fi[vi] - 1 > fi[vj]:\n",
    "                            fi[vj] = fi[vi] - 1\n",
    "            V.remove(vi)\n",
    "\n",
    "        return fi\n",
    "\n",
    "    b_values = set_b_values(G.copy())\n",
    "    fi_values = set_f(G.copy(), b_values)\n",
    "\n",
    "    S = {vi: sum(G.degree[vj] * (b_values[vj] + fi_values[vj]) for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    HKS = {vi: sum(S[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return HKS\n",
    "\n",
    "# Method 17 (SHKS)\n",
    "def method17(G):\n",
    "    # Precompute neighbor sets and node degrees for quick lookups.\n",
    "    neighbors = {node: set(G.neighbors(node)) for node in G.nodes()}\n",
    "    degrees = dict(G.degree())\n",
    "    \n",
    "    def compute_efficiency(G_local):\n",
    "        n = G_local.number_of_nodes()\n",
    "        if n <= 1:\n",
    "            return 0\n",
    "        total = 0.0\n",
    "        # Precompute all-pairs shortest path lengths.\n",
    "        for u, dist_dict in nx.all_pairs_shortest_path_length(G_local):\n",
    "            for v, d in dist_dict.items():\n",
    "                if d > 0:\n",
    "                    total += 1 / d\n",
    "        return total / (n * (n - 1))\n",
    "    \n",
    "    def compute_efficiency_centrality(G_local):\n",
    "        efficiency_G = compute_efficiency(G_local)\n",
    "        centrality = {}\n",
    "        for node in G_local.nodes():\n",
    "            # Remove node and compute efficiency for the resulting graph.\n",
    "            G_copy = G_local.copy()\n",
    "            G_copy.remove_node(node)\n",
    "            e_copy = compute_efficiency(G_copy)\n",
    "            centrality[node] = (efficiency_G - e_copy) / efficiency_G if efficiency_G > 0 else 0\n",
    "        return centrality\n",
    "    \n",
    "    # Optimized pvu: if u is a neighbor of v, this value is 1/degree(v)\n",
    "    def calculate_pvu(v, u):\n",
    "        return 1 / degrees[v] if degrees[v] > 0 else 0\n",
    "\n",
    "    # Compute the network-constrained coefficient for node v using precomputed neighbor sets.\n",
    "    def network_constrained_coefficient(v):\n",
    "        coeff = 0.0\n",
    "        for u in neighbors[v]:\n",
    "            # Outer term: always 1/deg(v) since u is a neighbor of v.\n",
    "            outer_term = calculate_pvu(v, u)\n",
    "            inner_sum = 0.0\n",
    "            # Common neighbors of v and u.\n",
    "            common = neighbors[v].intersection(neighbors[u])\n",
    "            for w in common:\n",
    "                term_vw = calculate_pvu(v, w)\n",
    "                term_wu = calculate_pvu(w, u)  # u is in neighbors[w] since w is common.\n",
    "                inner_sum += (term_vw * term_wu) ** 2\n",
    "            coeff += outer_term + inner_sum\n",
    "        return coeff\n",
    "\n",
    "    # Compute efficiency centrality (if needed later; not used further in SHKS here)\n",
    "    efficiency_centrality = compute_efficiency_centrality(G)\n",
    "    \n",
    "    # Compute the constraint coefficient for each node.\n",
    "    Constraint_Coef = {node: network_constrained_coefficient(node) for node in G.nodes()}\n",
    "    # Compute sh values with safe division.\n",
    "    sh = {node: (1 / Constraint_Coef[node]) if Constraint_Coef[node] != 0 else 0 for node in G.nodes()}\n",
    "    \n",
    "    # Compute core numbers.\n",
    "    ks = nx.core_number(G)\n",
    "    alpha = 0.2\n",
    "    I = {node: alpha * sh[node] + ks[node] for node in G.nodes()}\n",
    "    # Compute C, IS, and finally SHKS using precomputed neighbor sets.\n",
    "    C = {v: sum(I[v] + I[u] for u in neighbors[v]) for v in G.nodes()}\n",
    "    IS = {v: sum(C[u] for u in neighbors[v]) for v in G.nodes()}\n",
    "    SHKS = {v: sum(IS[u] for u in neighbors[v]) for v in G.nodes()}\n",
    "    \n",
    "    return SHKS\n",
    "\n",
    "\n",
    "# method18: KSGC\n",
    "def method18(G):\n",
    "    # Precompute core numbers and degrees.\n",
    "    ks = nx.core_number(G)\n",
    "    ks_max = max(ks.values())\n",
    "    ks_min = min(ks.values())\n",
    "    k = dict(G.degree())\n",
    "    \n",
    "    n = G.number_of_nodes()\n",
    "    nodes = list(G.nodes())\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}\n",
    "    \n",
    "    # Build arrays for core numbers and degrees.\n",
    "    ks_array = np.array([ks[node] for node in nodes])\n",
    "    k_array  = np.array([k[node] for node in nodes])\n",
    "    \n",
    "    # Compute the distance matrix using Floyd-Warshall.\n",
    "    # This returns a NumPy array of shape (n, n) with distances;\n",
    "    # unreachable pairs have np.inf.\n",
    "    D_matrix = nx.floyd_warshall_numpy(G)\n",
    "    \n",
    "    # Compute the C matrix using vectorized operations.\n",
    "    # Use a safe denominator to avoid division by zero.\n",
    "    denom = (ks_max - ks_min) if (ks_max - ks_min) != 0 else 1\n",
    "    C = np.exp((ks_array[:, None] - ks_array[None, :]) / denom)\n",
    "    \n",
    "    # Compute the F matrix:\n",
    "    # F[i, j] = C[i, j] * (k[i] * k[j] / (d(i,j)^2)), if a path exists, else 0.\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        F = C * ((k_array[:, None] * k_array[None, :]) / (D_matrix ** 2))\n",
    "        # For unreachable pairs (distance == inf), the division yields 0.\n",
    "        F[np.isinf(D_matrix)] = 0\n",
    "        # Ensure self-loops (i == j) are zero.\n",
    "        np.fill_diagonal(F, 0)\n",
    "    \n",
    "    # Define the neighborhood radius based on the graph's diameter.\n",
    "    diameter = nx.diameter(G)\n",
    "    radius = int(0.5 * diameter)\n",
    "    \n",
    "    # Create a boolean mask for node pairs within the specified radius.\n",
    "    # For each (i, j), mask[i, j] is True if the distance is less than or equal to radius.\n",
    "    mask = D_matrix <= radius\n",
    "    \n",
    "    # Compute KSGC for each node by summing F over nodes in its neighborhood.\n",
    "    # (F * mask) zeroes out contributions from nodes beyond the radius.\n",
    "    KSGC_values = np.sum(F * mask, axis=1)\n",
    "    \n",
    "    # Build a result dictionary mapping node to its computed KSGC value.\n",
    "    KSGC = {node: float(KSGC_values[node_index[node]]) for node in nodes}\n",
    "    \n",
    "    return KSGC\n",
    "\n",
    "\n",
    "# Method 19 (Local Relative ASP)\n",
    "def method19(G):\n",
    "    def find_diameter(G):\n",
    "        \"\"\"\n",
    "        Find the diameter (longest shortest path) of the graph G.\n",
    "        \"\"\"\n",
    "        if nx.is_connected(G):\n",
    "            return nx.diameter(G)\n",
    "        else:\n",
    "            lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "            diameter = 0\n",
    "            for u in lengths:\n",
    "                for v in lengths[u]:\n",
    "                    if lengths[u][v] > diameter:\n",
    "                        diameter = lengths[u][v]\n",
    "            return diameter\n",
    "\n",
    "    def calculate_asp(G):\n",
    "        \"\"\"\n",
    "        Calculate the Average Shortest Path (ASP) for the graph G.\n",
    "        \"\"\"\n",
    "        n = len(G.nodes)\n",
    "        if n < 2:\n",
    "            return 0\n",
    "        \n",
    "        if nx.is_connected(G):\n",
    "            return nx.average_shortest_path_length(G)\n",
    "        \n",
    "        diameter = find_diameter(G)\n",
    "        asp_sum = 0\n",
    "        \n",
    "        lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "        for u in G.nodes:\n",
    "            for v in G.nodes:\n",
    "                if u != v:\n",
    "                    asp_sum += lengths[u].get(v, diameter)\n",
    "        \n",
    "        return asp_sum / (n * (n - 1))\n",
    "\n",
    "    asp = calculate_asp(G)\n",
    "    AC = {}\n",
    "\n",
    "    for k in G.nodes():\n",
    "        G_removed = G.copy()\n",
    "        G_removed.remove_node(k)\n",
    "        asp_removed = calculate_asp(G_removed)\n",
    "        AC[k] = abs(asp_removed - asp) / asp if asp > 0 else 0\n",
    "\n",
    "    return AC\n",
    "\n",
    "def k_neighbors(G, node, radius):\n",
    "        neighbors = set(nx.single_source_shortest_path_length(G, node, cutoff=radius).keys())\n",
    "        neighbors.discard(node)  # Remove the node itself if present\n",
    "        return neighbors\n",
    "    \n",
    "# Method 20 (InformationRank)\n",
    "def method20(G):\n",
    "    \n",
    "    PROPA = {}\n",
    "    Score = {}\n",
    "    L = 2\n",
    "    Miu = 0.2\n",
    "\n",
    "    # Initialize PROPA and Score dictionaries\n",
    "    for v in G.nodes():\n",
    "        PROPA[v] = {}\n",
    "        Score[v] = 0\n",
    "\n",
    "    # Calculate PROPA and Score\n",
    "    for v in G.nodes():\n",
    "        neighbors = k_neighbors(G, v, L)\n",
    "        for w in neighbors:\n",
    "            PROPA[v][w] = 1\n",
    "            path_length_counts = {}\n",
    "            for path in nx.all_simple_paths(G, v, w, cutoff=L):\n",
    "                length = len(path) - 1\n",
    "                path_length_counts[length] = path_length_counts.get(length, 0) + 1\n",
    "\n",
    "            for l in range(1, L + 1):\n",
    "                pow_val = path_length_counts.get(l, 0)\n",
    "                PROPA[v][w] *= (1 - Miu ** l) ** pow_val\n",
    "            PROPA[v][w] = 1 - PROPA[v][w]\n",
    "            Score[v] += PROPA[v][w]\n",
    "\n",
    "    return Score\n",
    "\n",
    "# method 21 (IS-PEW)\n",
    "def method21(G):\n",
    "    # Precompute expensive metrics\n",
    "    triangles = nx.triangles(G)  # Dictionary {node: triangle_count}\n",
    "    core_numbers = nx.core_number(G)  # Dictionary {node: core_number}\n",
    "    degrees = dict(G.degree())  # Dictionary {node: degree}\n",
    "    \n",
    "    # Compute total triangles once\n",
    "    TC = sum(triangles.values()) // 3 if sum(triangles.values()) > 0 else 1  # Avoid division by zero\n",
    "    \n",
    "    def compute_TP(v_A):\n",
    "        return triangles[v_A] / TC\n",
    "    \n",
    "    def compute_InfE(v_A, v_B):\n",
    "        common_neighbors = set(G[v_A]) & set(G[v_B])\n",
    "        sum_DG_k = sum(degrees[k] for k in common_neighbors)\n",
    "        return (degrees[v_A] * degrees[v_B]) / (1 + sum_DG_k)\n",
    "\n",
    "    def compute_NIP(v_A):\n",
    "        return sum(np.sqrt(degrees[v_a] * core_numbers[v_a]) for v_a in G[v_A])\n",
    "\n",
    "    # Store edge weights in a dictionary to avoid recomputation\n",
    "    edge_weights = {}\n",
    "\n",
    "    def compute_EW(v_A, v_B):\n",
    "        if (v_A, v_B) in edge_weights:\n",
    "            return edge_weights[(v_A, v_B)]\n",
    "        \n",
    "        TP_v_A, TP_v_B = compute_TP(v_A), compute_TP(v_B)\n",
    "        KS_v_A, KS_v_B = core_numbers[v_A], core_numbers[v_B]\n",
    "        NIP_v_A, NIP_v_B = compute_NIP(v_A), compute_NIP(v_B)\n",
    "        ED_v_A_v_B = 1 / compute_InfE(v_A, v_B)\n",
    "        \n",
    "        EW_value = ((KS_v_A * (1 + TP_v_A) * NIP_v_A) / ED_v_A_v_B) + ((KS_v_B * (1 + TP_v_B) * NIP_v_B) / ED_v_A_v_B)\n",
    "        edge_weights[(v_A, v_B)] = EW_value  # Store result for future use\n",
    "        edge_weights[(v_B, v_A)] = EW_value  # Store symmetric value\n",
    "        return EW_value\n",
    "\n",
    "    def compute_IS(v_A):\n",
    "        return sum(compute_EW(v_A, v_B) for v_B in G[v_A])\n",
    "\n",
    "    # Compute potential edge weights once\n",
    "    for u, v in G.edges():\n",
    "        G[u][v]['weight'] = compute_EW(u, v)\n",
    "\n",
    "    # Compute IS values for all nodes\n",
    "    IS_values = {v: compute_IS(v) for v in G.nodes()}\n",
    "    \n",
    "    return IS_values\n",
    "\n",
    "# Method 22 (DKGM)\n",
    "def method22(G):\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    k_star = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while len(G_copy.nodes) > 0:\n",
    "        iteration_nodes = []\n",
    "        while True:\n",
    "            nodes_to_remove = [node for node in G_copy.nodes if G_copy.degree[node] <= k]\n",
    "            if not nodes_to_remove:\n",
    "                break\n",
    "            for node in nodes_to_remove:\n",
    "                k_shell[node] = k\n",
    "                iteration_nodes.append((node, iteration))\n",
    "                G_copy.remove_node(node)\n",
    "            iteration += 1\n",
    "\n",
    "        if iteration_nodes:  # Ensure the list is not empty\n",
    "            m = iteration_nodes[-1][1]  # Last iteration number\n",
    "            for node, n in iteration_nodes:\n",
    "                k_star[node] = k_shell[node] + (n / (m + 1))\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset iteration for next k-shell\n",
    "\n",
    "    DK = {}\n",
    "    DKGM = {}\n",
    "\n",
    "    for node in k_shell:\n",
    "        DK[node] = G.degree[node] + k_star.get(node, 0)  # Avoid KeyError\n",
    "\n",
    "    R = 2  # Define the radius for DKGM computation\n",
    "    for node in G.nodes():\n",
    "        DKGM[node] = 0\n",
    "        for neighbor in nx.single_source_shortest_path_length(G, node, cutoff=R):\n",
    "            if node != neighbor:\n",
    "                try:\n",
    "                    d_ij = nx.shortest_path_length(G, source=node, target=neighbor)\n",
    "                    DKGM[node] += DK[node] * DK.get(neighbor, 0) / d_ij**2  # Avoid KeyError\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "    return DKGM\n",
    "\n",
    "\n",
    "#method 23: modularity_vitality (MV)\n",
    "def method23(G):\n",
    "    \"\"\"Detect communities using Leiden algorithm and compute modularity vitality for each node.\"\"\"\n",
    "    \n",
    "    def detect_communities_leiden(G):\n",
    "        \"\"\"Detect communities using the Leiden algorithm.\n",
    "        \n",
    "        This function converts the NetworkX graph to an iGraph graph while preserving node names,\n",
    "        runs the Leiden algorithm, and returns a dictionary mapping node names to community indices.\n",
    "        \"\"\"\n",
    "        # Convert NetworkX graph to iGraph while preserving node names.\n",
    "        ig_G = ig.Graph.TupleList(G.edges(), directed=False, vertex_name_attr='name')\n",
    "        partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "        communities = {}\n",
    "        # Map each iGraph vertex back to the original NetworkX node name.\n",
    "        for idx, comm in enumerate(partition):\n",
    "            for v in comm:\n",
    "                communities[ig_G.vs[v][\"name\"]] = idx\n",
    "        return communities\n",
    "\n",
    "    def modularity(G, communities):\n",
    "        \"\"\"Compute the modularity of graph G with the given community partition.\"\"\"\n",
    "        M = G.number_of_edges()\n",
    "        Q = 0\n",
    "        for c in set(communities.values()):\n",
    "            nodes_c = [n for n in G.nodes() if communities[n] == c]\n",
    "            subgraph = G.subgraph(nodes_c)\n",
    "            L_c = subgraph.number_of_edges()\n",
    "            k_c = sum(G.degree(n) for n in nodes_c)\n",
    "            Q += (L_c / M) - (k_c / (2 * M)) ** 2\n",
    "        return Q\n",
    "\n",
    "    def compute_hi_c(G, node, communities):\n",
    "        \"\"\"Compute h_i,c values for a node as per Equation (10).\"\"\"\n",
    "        k_i = G.degree(node)\n",
    "        hi_c = {}\n",
    "        for c in set(communities.values()):\n",
    "            # Count links from node to neighbors in community c.\n",
    "            k_i_c = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == c)\n",
    "            hi_c[c] = k_i_c + (k_i if communities[node] == c else 0)\n",
    "        return hi_c\n",
    "\n",
    "    def modularity_vitality(G, communities, node):\n",
    "        \"\"\"Compute the modularity vitality of a node using Equation (9).\"\"\"\n",
    "        Q_initial = modularity(G, communities)\n",
    "        if not G.has_node(node):\n",
    "            return 0  \n",
    "\n",
    "        M = G.number_of_edges()\n",
    "        k_i = G.degree(node)\n",
    "        c_i = communities[node]\n",
    "        \n",
    "        # Compute h_i,c for the given node.\n",
    "        hi_c = compute_hi_c(G, node, communities)\n",
    "        \n",
    "        # Compute the total degree for each community.\n",
    "        d_c = {c: sum(G.degree(n) for n in G.nodes() if communities[n] == c)\n",
    "               for c in set(communities.values())}\n",
    "\n",
    "        # Get nodes in the same community as node.\n",
    "        community_nodes = [n for n in G.nodes() if communities[n] == c_i]\n",
    "        M_internal = G.subgraph(community_nodes).number_of_edges()\n",
    "        k_i_internal = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == c_i)\n",
    "        \n",
    "        # Compute the sum term over all communities.\n",
    "        sum_term = sum((d_c[c] - hi_c[c]) ** 2 for c in hi_c)\n",
    "        \n",
    "        # Compute updated modularity based on Equation (9).\n",
    "        Q_updated = (M_internal - k_i_internal) / (M - k_i) - (1 / (4 * (M - k_i) ** 2)) * sum_term\n",
    "\n",
    "        return Q_initial - Q_updated\n",
    "\n",
    "    # Detect communities using the Leiden algorithm.\n",
    "    communities = detect_communities_leiden(G)\n",
    "\n",
    "    # Compute modularity vitality for each node.\n",
    "    mod_vit = {node: modularity_vitality(G, communities, node) for node in G.nodes()}\n",
    "    \n",
    "    return mod_vit\n",
    "\n",
    "#method 24: Global-and-Local Centrality (GLC)\n",
    "def method24(G, lam=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Global-and-Local Centrality (GLC) of each node in graph G.\n",
    "    \n",
    "    The method first clusters the network using a potential-based approach,\n",
    "    then selects global critical nodes from each cluster, computes local influence\n",
    "    based on k-shell values, and finally calculates the overall GLC value.\n",
    "    \n",
    "    Parameters:\n",
    "        G   : networkx.Graph\n",
    "              The input graph.\n",
    "        lam : float (default=1.0)\n",
    "              The fraction (lambda) of nodes that must be covered by clusters.\n",
    "    \n",
    "    Returns:\n",
    "        GLC : dict\n",
    "              A dictionary mapping each node in G to its GLC centrality value.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Clustering and Global Critical Node Selection ---\n",
    "    clusters = []       # List to hold all clusters\n",
    "    assigned = set()    # Set of nodes that have been assigned or marked\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # Helper function: Compute the potential of a node as defined in Eq.(10)\n",
    "    def compute_potential(node):\n",
    "        # Potential of node = degree(node) * (sum over j in N(node) of kin(j))\n",
    "        # where kin(j) = number of neighbors of j that are in {node} U N(node)\n",
    "        nbrs = set(G.neighbors(node))\n",
    "        target_set = nbrs.union({node})\n",
    "        s = 0\n",
    "        for j in nbrs:\n",
    "            s += sum(1 for nbr in G.neighbors(j) if nbr in target_set)\n",
    "        return G.degree(node) * s\n",
    "\n",
    "    # Continue clustering until the number of assigned nodes reaches lam * total_nodes\n",
    "    while len(assigned) < total_nodes * lam:\n",
    "        # Compute potentials for all unassigned nodes.\n",
    "        potentials = {}\n",
    "        for node in G.nodes():\n",
    "            if node not in assigned:\n",
    "                potentials[node] = compute_potential(node)\n",
    "        if not potentials:\n",
    "            break  # All nodes have been assigned\n",
    "        \n",
    "        # Select seed node with maximum potential.\n",
    "        seed = max(potentials, key=potentials.get)\n",
    "        pcmax = potentials[seed]\n",
    "        new_cluster = set([seed])\n",
    "        assigned.add(seed)\n",
    "        \n",
    "        # Include neighbors of the seed with potential >= pcmax/2.\n",
    "        for neighbor in G.neighbors(seed):\n",
    "            if neighbor not in assigned and compute_potential(neighbor) >= pcmax / 2:\n",
    "                new_cluster.add(neighbor)\n",
    "                assigned.add(neighbor)\n",
    "                \n",
    "        # Expand the cluster iteratively (three degrees of influence).\n",
    "        for _ in range(3):\n",
    "            # Gather neighbors of the current cluster (excluding nodes already in the cluster)\n",
    "            cluster_neighbors = set()\n",
    "            for node in new_cluster:\n",
    "                for nbr in G.neighbors(node):\n",
    "                    if nbr not in new_cluster:\n",
    "                        cluster_neighbors.add(nbr)\n",
    "            # Process neighbors in order of increasing degree.\n",
    "            cluster_neighbors = sorted(cluster_neighbors, key=lambda x: G.degree(x))\n",
    "            \n",
    "            added_flag = False\n",
    "            for candidate in cluster_neighbors:\n",
    "                # Count the number of edges from candidate to nodes in new_cluster (kin)\n",
    "                kin = sum(1 for nbr in G.neighbors(candidate) if nbr in new_cluster)\n",
    "                # Count the remaining edges from candidate to nodes outside new_cluster (kout)\n",
    "                kout = sum(1 for nbr in G.neighbors(candidate) if nbr not in new_cluster)\n",
    "                if kin >= kout:\n",
    "                    new_cluster.add(candidate)\n",
    "                    assigned.add(candidate)\n",
    "                    added_flag = True\n",
    "            if not added_flag:\n",
    "                break  # No more nodes can be added in this iteration\n",
    "        \n",
    "        # Mark all neighbors of the new cluster as assigned to avoid re-selection.\n",
    "        for node in new_cluster:\n",
    "            for nbr in G.neighbors(node):\n",
    "                assigned.add(nbr)\n",
    "        clusters.append(new_cluster)\n",
    "    \n",
    "    # From each cluster, select the global critical node (node with highest degree).\n",
    "    global_critical = []\n",
    "    for cluster in clusters:\n",
    "        if cluster:\n",
    "            gc = max(cluster, key=lambda node: G.degree(node))\n",
    "            global_critical.append(gc)\n",
    "    \n",
    "    # --- Step 2: Local Influence Calculation ---\n",
    "    # Compute the k-shell (core) numbers for all nodes.\n",
    "    core_numbers = nx.core_number(G)\n",
    "    \n",
    "    # Compute local influence LI for each node:\n",
    "    # LI(node) = sum_{j in N(node)} k_shell(j)\n",
    "    LI = {}\n",
    "    for node in G.nodes():\n",
    "        li = 0\n",
    "        for nbr in G.neighbors(node):\n",
    "            li += core_numbers[nbr]\n",
    "        LI[node] = li\n",
    "\n",
    "    # --- Step 3: Overall GLC Centrality Calculation ---\n",
    "    # For each node v, compute:\n",
    "    #   GlobalFactor(v) = sum_{u in global_critical} (LI(u) / (2 * max(d(v,u),1)))\n",
    "    #   GLC(v) = LI(v) * GlobalFactor(v)\n",
    "    GLC = {}\n",
    "    for v in G.nodes():\n",
    "        global_factor = 0\n",
    "        for u in global_critical:\n",
    "            try:\n",
    "                d = nx.shortest_path_length(G, source=v, target=u)\n",
    "            except nx.NetworkXNoPath:\n",
    "                d = float('inf')\n",
    "            d = max(d, 1)  # Avoid division by zero\n",
    "            global_factor += LI[u] / (2 * d)\n",
    "        GLC[v] = LI[v] * global_factor\n",
    "\n",
    "    return GLC\n",
    "\n",
    "# Method 25 (Weighted K-shell)\n",
    "def method25(G):\n",
    "    c1 = 0.1\n",
    "    c2 = 0.4\n",
    "    deg = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    ksdw = dict()\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksdw[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            ksdw[vi] += (c1 * deg[vi] + c2 * ks[vi]) * (c1 * deg[vj] + c2 * ks[vj])\n",
    "\n",
    "    return ksdw\n",
    "\n",
    "# method 26:EFFD centrality\n",
    "def method26(G):\n",
    "    def calculate_effective_distance(G):\n",
    "        degree = dict(G.degree())\n",
    "        \n",
    "        # Step 1: Calculate the probability P n|m\n",
    "        probability = {(m, n): (1 / degree[m]) if G.has_edge(m, n) else 0 \n",
    "                       for m in G.nodes for n in G.nodes if m != n}\n",
    "\n",
    "        # Step 2: Calculate the effective distance D_{n|m} for directly connected nodes\n",
    "        effective_distance = {(m, n): 1 - np.log2(p) if p > 0 else float('inf') \n",
    "                              for (m, n), p in probability.items()}\n",
    "\n",
    "        # Step 3: Calculate the effective distance for indirectly connected nodes using the shortest path\n",
    "        all_pairs_shortest_path_length = dict(nx.all_pairs_dijkstra_path_length(G))\n",
    "        for m in G.nodes:\n",
    "            for n in G.nodes:\n",
    "                if m != n:\n",
    "                    shortest_path_length = all_pairs_shortest_path_length[m].get(n, float('inf'))\n",
    "                    if shortest_path_length != float('inf'):\n",
    "                        effective_distance[(m, n)] = shortest_path_length\n",
    "\n",
    "        return effective_distance\n",
    "\n",
    "    def calculate_interaction_scores(G, effective_distance):\n",
    "        interaction_scores = {}\n",
    "        for (i, j), d in effective_distance.items():\n",
    "            k_i = G.degree[i]\n",
    "            k_j = G.degree[j]\n",
    "            interaction_scores[(i, j)] = (k_i * k_j) / (d ** 2)\n",
    "        return interaction_scores\n",
    "\n",
    "    def compute_effg_centrality(G, interaction_scores):\n",
    "        effg_centrality = {node: 0 for node in G.nodes()}\n",
    "        for (i, j), score in interaction_scores.items():\n",
    "            effg_centrality[i] += score\n",
    "            effg_centrality[j] += score\n",
    "        return effg_centrality\n",
    "\n",
    "    # Calculate Effective Distance\n",
    "    effective_distance = calculate_effective_distance(G)\n",
    "\n",
    "    # Calculate Interaction Scores\n",
    "    interaction_scores = calculate_interaction_scores(G, effective_distance)\n",
    "\n",
    "    # Calculate EffG Centrality\n",
    "    effg_centrality = compute_effg_centrality(G, interaction_scores)\n",
    "    \n",
    "    return effg_centrality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#methods = {\n",
    "#    \"Degree Centrality\": method1,\n",
    "#    \"Betweenness Centrality\": method2,\n",
    "#    \"Closeness Centrality\": method3,\n",
    "#    \"K-shell\": method4,\n",
    "#    \"Cnc Plus\": method5,\n",
    "#    \"Local H-index\": method6,\n",
    "#    \"DNC\": method7,\n",
    "#    \"ECLGC\": method8,\n",
    "#    \"Centripetal Centrality\": method9,\n",
    "#    \"EDBC\" : method10,\n",
    "#    \"LGC\" : method11,\n",
    "#    \"SEGM\" : method12,\n",
    "#    \"MCGM\" : method13,\n",
    "#    \"HVGC\" : method14,\n",
    "#    \"BaseGM\" : method15,\n",
    "#    \"HKS\" : method16,\n",
    "#    \"SHKS\" : method17,\n",
    "#    \"KSGC\" : method18,\n",
    "#    \"Local Relative ASP\" : method19,\n",
    "#    \"Information Rank\" : method20,\n",
    "#    \"IS-PEW\" : method21,\n",
    "#    \"DKGM\" : method22,\n",
    "#    \"modularity vitality\":method23,\n",
    "#    \"Global-and-Local Centrality (GLC)\":method24,\n",
    "#    \"Weighted K-shell\" : method25,\n",
    "#    \"EFFD gravity\" : method26,\n",
    "# }\n",
    "\n",
    "# List of methods to be applied\n",
    "methods = [globals()[f'method{i}'] for i in range(1, 27)]\n",
    "\n",
    "dataset_folder = 'dataset'\n",
    "beta_values = [0.02, 0.05, 0.08, 0.10, 0.13, 0.15, 0.18, 0.20, 0.22, 0.25, 0.30]\n",
    "k_factors = [0.01, 0.03, 0.05, 0.07, 0.085, 0.10]\n",
    "f_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "for dataset_filename in os.listdir(dataset_folder):\n",
    "    if dataset_filename.endswith('.xlsx'):\n",
    "        dataset_path = os.path.join(dataset_folder, dataset_filename)\n",
    "        dataset_name = os.path.splitext(dataset_filename)[0]\n",
    "        print(f\"\\nProcessing dataset: {dataset_name}...\\n\")\n",
    "        \n",
    "        df_edges = pd.read_excel(dataset_path, sheet_name='Sheet1')\n",
    "        G = nx.from_pandas_edgelist(df_edges, source='source_column', target='target_column')\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "        \n",
    "        df_sir = pd.read_excel(dataset_path, sheet_name='SIR')\n",
    "        df_sir.set_index('Node', inplace=True)\n",
    "        \n",
    "        num_nodes = len(G.nodes())\n",
    "        results = {}\n",
    "        method_results = {}\n",
    "\n",
    "        for i, method in enumerate(methods, start=1):\n",
    "            method_name = f'method{i}'\n",
    "            print(f\"Applying {method_name} on {dataset_name}...\")\n",
    "            try:\n",
    "                ranked_nodes, exec_time = measure_execution_time(method, G)\n",
    "                monotonicity = compute_monotonicity(G, ranked_nodes)\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': ranked_nodes,\n",
    "                    'execution_time': exec_time,\n",
    "                    'monotonicity': monotonicity\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {method_name} for {dataset_name}: {e}\")\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': [],\n",
    "                    'execution_time': None,\n",
    "                    'monotonicity': None,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "\n",
    "        for beta in beta_values:\n",
    "            beta_column = f'Beta_{beta:.2f}'\n",
    "            if beta_column not in df_sir.columns:\n",
    "                print(f\"Warning: {beta_column} not found in {dataset_name}. Skipping beta {beta}...\")\n",
    "                continue\n",
    "            \n",
    "            spread_power = df_sir[beta_column].to_dict()\n",
    "            sigma = sorted(spread_power.keys(), key=lambda x: spread_power[x], reverse=True)\n",
    "            \n",
    "            for method_name, method_data in method_results.items():\n",
    "                if 'error' in method_data:\n",
    "                    results[(method_name, beta)] = {'Beta': beta, 'Kendall Tau': None, 'P-Value': None}\n",
    "                    continue\n",
    "                \n",
    "                ranked_nodes = method_data['ranked_nodes']\n",
    "                tau, p_value = kendalltau(\n",
    "                    [spread_power[node] for node in sigma],\n",
    "                    [spread_power[node] for node, _ in ranked_nodes]\n",
    "                )\n",
    "                \n",
    "                R = [node for node, _ in ranked_nodes]\n",
    "                jaccard_scores = {}\n",
    "                si_scores = {}\n",
    "                rbo_scores = {}\n",
    "                mrr_scores = compute_mrr(sigma, R, k_factors)\n",
    "                \n",
    "                for k_factor in k_factors:\n",
    "                    k = max(1, int(k_factor * num_nodes))\n",
    "                    top_k_sigma = set(sigma[:k])\n",
    "                    top_k_R = set(R[:k])\n",
    "                    jaccard_scores[f'Jaccard k={k}'] = compute_jaccard_similarity(top_k_sigma, top_k_R)\n",
    "                \n",
    "                for f in f_values:\n",
    "                    f_count = max(1, int(f * num_nodes))\n",
    "                    top_f_nodes = [node for node, _ in ranked_nodes][:f_count]\n",
    "                    si_scores[f'SI {f:.2f}'] = sum(spread_power[node] for node in top_f_nodes) / (f * num_nodes)\n",
    "                    rbo_scores[f'RBO {f:.2f}'] = compute_rbo(sigma[:f_count], R[:f_count])\n",
    "                \n",
    "                results[(method_name, beta)] = {\n",
    "                    'Beta': beta,\n",
    "                    'Kendall Tau': tau,\n",
    "                    'P-Value': p_value,\n",
    "                    **jaccard_scores,\n",
    "                    **si_scores,\n",
    "                    **rbo_scores,\n",
    "                    **mrr_scores\n",
    "                }\n",
    "        \n",
    "        data = []\n",
    "        for (method_name, beta), result in results.items():\n",
    "            row = [\n",
    "                dataset_name,  # Include dataset name in output\n",
    "                method_name,\n",
    "                method_results[method_name]['execution_time'],\n",
    "                method_results[method_name].get('monotonicity', 'N/A'),\n",
    "                result.get('Beta'),\n",
    "                result.get('Kendall Tau'),\n",
    "                result.get('P-Value')\n",
    "            ] + [result.get(f'Jaccard k={int(k_factor * num_nodes)}') for k_factor in k_factors] + \\\n",
    "              [result.get(f'SI {f:.2f}') for f in f_values] + \\\n",
    "              [result.get(f'RBO {f:.2f}') for f in f_values] + \\\n",
    "              [result.get(f'MRR {k_factor:.2f}') for k_factor in k_factors]\n",
    "            \n",
    "            data.append(row)\n",
    "        \n",
    "        expected_columns = ['Dataset', 'Method', 'Execution Time', 'Monotonicity', 'Beta', 'Kendall Tau', 'P-Value'] + \\\n",
    "                           [f'Jaccard k={int(k_factor * num_nodes)}' for k_factor in k_factors] + \\\n",
    "                           [f'SI {f:.2f}' for f in f_values] + \\\n",
    "                           [f'RBO {f:.2f}' for f in f_values] + \\\n",
    "                           [f'MRR {k_factor:.2f}' for k_factor in k_factors]\n",
    "        \n",
    "        result_df = pd.DataFrame(data, columns=expected_columns)\n",
    "        \n",
    "        output_file = f'{dataset_name}_results.xlsx'\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df_edges.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "            df_sir.to_excel(writer, sheet_name='SIR')\n",
    "            result_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        \n",
    "        print(f\"✅ Completed processing: {dataset_name}. Results saved in {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a9e0a-505e-4741-9a63-7324185d036b",
   "metadata": {},
   "source": [
    "# Optimized Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e91c4e-6ae3-475f-bb9f-a6f0cb72901e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: PowerGrid(SIR)...\n",
      "\n",
      "Applying method1 on PowerGrid(SIR)...\n",
      "Applying method2 on PowerGrid(SIR)...\n",
      "Applying method3 on PowerGrid(SIR)...\n",
      "Applying method4 on PowerGrid(SIR)...\n",
      "Applying method5 on PowerGrid(SIR)...\n",
      "Applying method6 on PowerGrid(SIR)...\n",
      "Applying method7 on PowerGrid(SIR)...\n",
      "Applying method8 on PowerGrid(SIR)...\n",
      "Applying method9 on PowerGrid(SIR)...\n",
      "Applying method10 on PowerGrid(SIR)...\n",
      "Applying method11 on PowerGrid(SIR)...\n",
      "Applying method12 on PowerGrid(SIR)...\n",
      "Applying method13 on PowerGrid(SIR)...\n",
      "Applying method14 on PowerGrid(SIR)...\n",
      "Applying method15 on PowerGrid(SIR)...\n",
      "Applying method16 on PowerGrid(SIR)...\n",
      "Applying method17 on PowerGrid(SIR)...\n",
      "Applying method18 on PowerGrid(SIR)...\n",
      "Applying method19 on PowerGrid(SIR)...\n",
      "Applying method20 on PowerGrid(SIR)...\n",
      "Applying method21 on PowerGrid(SIR)...\n",
      "Applying method22 on PowerGrid(SIR)...\n",
      "Applying method23 on PowerGrid(SIR)...\n",
      "Applying method24 on PowerGrid(SIR)...\n",
      "Applying method25 on PowerGrid(SIR)...\n",
      "✅ Completed processing: PowerGrid(SIR). Results saved in PowerGrid(SIR)_results.xlsx\n",
      "\n",
      "\n",
      "Processing dataset: advogato(SIR)...\n",
      "\n",
      "Applying method1 on advogato(SIR)...\n",
      "Applying method2 on advogato(SIR)...\n",
      "Applying method3 on advogato(SIR)...\n",
      "Applying method4 on advogato(SIR)...\n",
      "Applying method5 on advogato(SIR)...\n",
      "Applying method6 on advogato(SIR)...\n",
      "Applying method7 on advogato(SIR)...\n",
      "Applying method8 on advogato(SIR)...\n",
      "Applying method9 on advogato(SIR)...\n",
      "Applying method10 on advogato(SIR)...\n",
      "Applying method11 on advogato(SIR)...\n",
      "Applying method12 on advogato(SIR)...\n",
      "Applying method13 on advogato(SIR)...\n",
      "Applying method14 on advogato(SIR)...\n",
      "Applying method15 on advogato(SIR)...\n",
      "Applying method16 on advogato(SIR)...\n",
      "Applying method17 on advogato(SIR)...\n",
      "Applying method18 on advogato(SIR)...\n",
      "Applying method19 on advogato(SIR)...\n",
      "Applying method20 on advogato(SIR)...\n",
      "Applying method21 on advogato(SIR)...\n",
      "Applying method22 on advogato(SIR)...\n",
      "Applying method23 on advogato(SIR)...\n",
      "Applying method24 on advogato(SIR)...\n",
      "Applying method25 on advogato(SIR)...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "149",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_88/2299171124.py\u001b[0m in \u001b[0;36m<cell line: 1122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1174\u001b[0m                 tau, p_value = kendalltau(\n\u001b[1;32m   1175\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mspread_power\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0mspread_power\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranked_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m                 )\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_88/2299171124.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1174\u001b[0m                 tau, p_value = kendalltau(\n\u001b[1;32m   1175\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mspread_power\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0mspread_power\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranked_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m                 )\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 149"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "from collections import deque\n",
    "import random\n",
    "from scipy.stats import kendalltau\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "def measure_execution_time(method, G):\n",
    "    start_time = time.time()\n",
    "    result = method(G)\n",
    "    ranked_nodes = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return ranked_nodes, execution_time\n",
    "\n",
    "def compute_monotonicity(G, ranked_nodes):\n",
    "    ranks = [score for node, score in ranked_nodes]\n",
    "    unique_ranks = list(set(ranks))\n",
    "    n = G.number_of_nodes()\n",
    "    nr_dict = {rank: ranks.count(rank) for rank in unique_ranks}\n",
    "    nr_sum = sum(nr * (nr - 1) for nr in nr_dict.values())\n",
    "    M = (1 - nr_sum / (n * (n - 1))) ** 2\n",
    "    return M\n",
    "\n",
    "def compute_mrr(ground_truth, ranked_list, top_ratios):\n",
    "    mrr_scores = {}\n",
    "    num_nodes = len(ground_truth)\n",
    "    \n",
    "    for ratio in top_ratios:\n",
    "        k = max(1, int(ratio * num_nodes))\n",
    "        reciprocal_ranks = []\n",
    "        \n",
    "        for node in ground_truth[:k]:\n",
    "            if node in ranked_list:\n",
    "                rank = ranked_list.index(node) + 1\n",
    "                reciprocal_ranks.append(1 / rank)\n",
    "        \n",
    "        mrr_scores[f'MRR {ratio:.2f}'] = sum(reciprocal_ranks) / k if reciprocal_ranks else 0\n",
    "    \n",
    "    return mrr_scores\n",
    "\n",
    "\n",
    "def compute_rbo(sigma, R, alpha=0.9):\n",
    "    \"\"\"Compute the Rank-Biased Overlap (RBO) between two ranking lists.\"\"\"\n",
    "    def A(sigma, R, f):\n",
    "        sigma_f = set(sigma[:f])\n",
    "        R_f = set(R[:f])\n",
    "        if len(sigma_f) == 0 and len(R_f) == 0:\n",
    "            return 1\n",
    "        return len(sigma_f & R_f) / len(sigma_f | R_f)\n",
    "\n",
    "    n = max(len(sigma), len(R))\n",
    "    rbo = (1 - alpha) * sum(alpha**(f - 1) * A(sigma, R, f) for f in range(1, n + 1))\n",
    "    return rbo\n",
    "\n",
    "def compute_jaccard_similarity(set1, set2):\n",
    "    return len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0\n",
    "\n",
    "# Define methods\n",
    "# method1: Degree\n",
    "def method1(G):\n",
    "    return nx.degree_centrality(G)\n",
    "\n",
    "# method2: Betweenness Centrality\n",
    "def method2(G, epsilon=0.1, delta=0.1, c=1.0):\n",
    "    \"\"\"\n",
    "    Computes an approximate betweenness centrality for all nodes in G.\n",
    "    \n",
    "    Parameters:\n",
    "    G : networkx.Graph\n",
    "        The input graph.\n",
    "    epsilon : float, optional\n",
    "        Error tolerance parameter.\n",
    "    delta : float, optional\n",
    "        Probability of failure parameter.\n",
    "    c : float, optional\n",
    "        Scaling constant.\n",
    "    \n",
    "    Returns:\n",
    "    dict\n",
    "        Dictionary where keys are nodes and values are their approximate betweenness centrality.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize betweenness values\n",
    "    betweenness = {v: 0 for v in G.nodes()}\n",
    "    \n",
    "    # Step 2: Compute estimated number of samples r\n",
    "    diameter = nx.diameter(G) if nx.is_connected(G) else max(nx.eccentricity(G).values())\n",
    "    r = int((c / (epsilon ** 2)) * (math.log2(max(diameter - 2, 1)) + math.log(1 / delta)))\n",
    "    \n",
    "    nodes = list(G.nodes())\n",
    "    \n",
    "    # Step 3: Perform sampling\n",
    "    for _ in range(r):\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        shortest_paths = list(nx.all_shortest_paths(G, source=u, target=v))\n",
    "        \n",
    "        if len(shortest_paths) > 1:\n",
    "            path = random.choice(shortest_paths)  # Randomly select one shortest path\n",
    "            \n",
    "            for z in path[1:-1]:  # Exclude source (u) and target (v)\n",
    "                betweenness[z] += 1 / r  # Update betweenness estimation\n",
    "    \n",
    "    return betweenness\n",
    "\n",
    "\n",
    "# method3: Closeness Centrality\n",
    "def method3(G):\n",
    "    return nx.closeness_centrality(G)\n",
    "\n",
    "# method4: k-shell\n",
    "def method4(G):\n",
    "    return nx.core_number(G)\n",
    "\n",
    "def CnC(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = {vi: sum(ks[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc\n",
    "\n",
    "# method5: CncPlus\n",
    "def method5(G):\n",
    "    ks = nx.core_number(G)\n",
    "    cnc = CnC(G)\n",
    "    cnc_plus = {vi: sum(cnc[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return cnc_plus\n",
    "\n",
    "def HIndex(G):\n",
    "    h_index = {}\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        degrees = sorted([G.degree(neighbor) for neighbor in neighbors], reverse=True)\n",
    "        h = 0\n",
    "        for i, degree in enumerate(degrees):\n",
    "            if degree >= i + 1:\n",
    "                h = i + 1\n",
    "            else:\n",
    "                break\n",
    "        h_index[node] = h\n",
    "    return h_index\n",
    "\n",
    "# method6: Local H-Index\n",
    "def method6(G):\n",
    "    HI = HIndex(G)\n",
    "    LHI = {vi: HI[vi] + sum(HI[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return LHI\n",
    "\n",
    "# method7: DNC\n",
    "def method7(G):\n",
    "    SLC = {}\n",
    "    k = dict(nx.degree(G))\n",
    "    C = nx.clustering(G)\n",
    "    for i in G.nodes():\n",
    "        SLC[i] = sum(C[j] for j in G.neighbors(i))\n",
    "    \n",
    "    DNC = {}\n",
    "    alpha = 1\n",
    "    for i in G.nodes():\n",
    "        DNC[i] = k[i] + alpha * SLC[i]\n",
    "    return DNC\n",
    "\n",
    "# Method 8: ECLGC\n",
    "def method8(G):\n",
    "    # Precompute clustering coefficients\n",
    "    CLC = nx.clustering(G)\n",
    "    \n",
    "    # Precompute neighbors for each node\n",
    "    neighbors = {node: list(G.neighbors(node)) for node in G.nodes()}\n",
    "    \n",
    "    # Precompute shortest path lengths for all node pairs\n",
    "    sp_lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    \n",
    "    # Compute CLGC (not used later, but computed as in the original code)\n",
    "    CLGC = {}\n",
    "    for i in G.nodes():\n",
    "        s = 0.0\n",
    "        for j in G.nodes():\n",
    "            if i == j:\n",
    "                continue\n",
    "            if j in sp_lengths[i]:\n",
    "                s += math.sqrt(CLC[j]) / sp_lengths[i][j]\n",
    "        CLGC[i] = s * CLC[i]\n",
    "    \n",
    "    # Compute ECLGC\n",
    "    ECLGC = {}\n",
    "    for v in G.nodes():\n",
    "        sum1 = 0.0\n",
    "        sum3 = 0.0\n",
    "        neighbors_v = neighbors[v]\n",
    "        if not neighbors_v:  # if no neighbors, ECLGC is 0\n",
    "            ECLGC[v] = 0\n",
    "            continue\n",
    "        \n",
    "        for u in G.nodes():\n",
    "            if u == v:\n",
    "                continue\n",
    "            if u in sp_lengths[v]:\n",
    "                # Precompute sum2 for u's neighbors\n",
    "                neighbors_u = neighbors[u]\n",
    "                lu = len(neighbors_u) if neighbors_u else 1  # avoid division by zero\n",
    "                sum2 = 0.0\n",
    "                for j in neighbors_u:\n",
    "                    sum2 += CLC[j] / lu\n",
    "                \n",
    "                sum1 += CLC[u] / len(neighbors_v)\n",
    "                d = sp_lengths[v][u]  # shortest path length from v to u\n",
    "                sum3 += math.sqrt(CLC[u] + sum2) / d\n",
    "        ECLGC[v] = sum1 * sum3\n",
    "    return ECLGC\n",
    "\n",
    "\n",
    "\n",
    "def get_neighborhood(G, node, radius):\n",
    "    neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "    neighborhood.pop(node, None)\n",
    "    return list(neighborhood.keys())\n",
    "\n",
    "\n",
    "# method9: CenC\n",
    "def compute_alpha(graph):\n",
    "    size_of_V = graph.number_of_nodes()\n",
    "    # Ensure order_of_magnitude is at least 1 to avoid division by zero.\n",
    "    order_of_magnitude = max(int(math.log10(size_of_V)), 1)\n",
    "    alpha = size_of_V / order_of_magnitude\n",
    "    return alpha\n",
    "\n",
    "def method9(G, m=1):\n",
    "    alpha = compute_alpha(G)\n",
    "    # Cap alpha to avoid math range error in math.exp (overflow for very large alpha)\n",
    "    capped_alpha = min(alpha, 700)\n",
    "    \n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    clustering_coeff = nx.clustering(G)\n",
    "    kshell = nx.core_number(G)\n",
    "    \n",
    "    centrality = {}\n",
    "    \n",
    "    for i in G.nodes():\n",
    "        cenC_i = 0\n",
    "        \n",
    "        # Precompute the m-neighborhood and distances from i\n",
    "        sp_lengths_i = nx.single_source_shortest_path_length(G, i, cutoff=m)\n",
    "        m_neighborhood = set(sp_lengths_i.keys())\n",
    "        m_neighborhood.discard(i)  # Exclude the node itself\n",
    "        \n",
    "        for j in m_neighborhood:\n",
    "            d_ij = sp_lengths_i[j]  # Use precomputed distance\n",
    "            if d_ij == 0:\n",
    "                continue\n",
    "            \n",
    "            DC_i = degree_centrality[i]\n",
    "            C_i = clustering_coeff[i]\n",
    "            k_s_i = kshell[i]\n",
    "            k_s_j = kshell[j]\n",
    "            \n",
    "            # Compute term1 with a safe exponentiation\n",
    "            term1 = (math.exp(capped_alpha) * DC_i + (1 / C_i if C_i != 0 else 0))\n",
    "            term2 = k_s_j / (abs(k_s_i - k_s_j) + 1)\n",
    "            \n",
    "            cenC_i += (4 * math.pi**2) * (term1 / (d_ij**2)) * term2\n",
    "        \n",
    "        centrality[i] = cenC_i\n",
    "    \n",
    "    return centrality\n",
    "\n",
    "def k_shell(graph):\n",
    "    return nx.core_number(graph)\n",
    "\n",
    "def evaluate_power_k_shell(k_si, k_sj):\n",
    "    return math.sqrt(k_si + k_sj)\n",
    "\n",
    "def compute_P_ji(graph, node_i, node_j):\n",
    "    try:\n",
    "        a_ji = graph[node_j][node_i]['weight'] if 'weight' in graph[node_j][node_i] else 1\n",
    "        total_a_ji = sum(graph[node_j][nbr]['weight'] if 'weight' in graph[node_j][nbr] else 1 for nbr in graph.neighbors(node_j))\n",
    "        return a_ji / total_a_ji\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def evaluate_ED(graph, node_i, node_j):\n",
    "    P_ji = compute_P_ji(graph, node_i, node_j)\n",
    "    if P_ji == 0:\n",
    "        return 1  # Prevent log(0)\n",
    "    return max(1 - math.log(P_ji), 1)\n",
    "\n",
    "def evaluate_influence(graph, node_i, node_j, k_shell_values):\n",
    "    alpha_ij = evaluate_power_k_shell(k_shell_values[node_i], k_shell_values[node_j])\n",
    "    ED_ij = evaluate_ED(graph, node_i, node_j)\n",
    "    degree_vi = graph.degree(node_i)\n",
    "    return alpha_ij * degree_vi / ED_ij\n",
    "\n",
    "# EDBC\n",
    "def method10(G):\n",
    "    k_shell_values = k_shell(G)\n",
    "    EDBC = {u: 0 for u in G.nodes()}\n",
    "    \n",
    "    for u in G.nodes():\n",
    "        for v in G.neighbors(u):\n",
    "            for neighbor_u in G.neighbors(u):\n",
    "                EDBC[v] += evaluate_influence(G, v, neighbor_u, k_shell_values)\n",
    "    \n",
    "    return EDBC\n",
    "\n",
    "# Method 11 (LGC)\n",
    "def method11(G):\n",
    "    # Precompute degrees and neighbor lists.\n",
    "    K = dict(G.degree())\n",
    "    neighbor_dict = {node: list(G.neighbors(node)) for node in G.nodes()}\n",
    "    \n",
    "    # Precompute LC values for each node.\n",
    "    LC = {}\n",
    "    for i in G.nodes():\n",
    "        LC[i] = sum(K[j] for j in neighbor_dict[i]) * 2 + K[i] ** 2 + K[i]\n",
    "    \n",
    "    LGC = {}\n",
    "    radius = 3\n",
    "    # For each node, compute the distances to nodes within the given radius.\n",
    "    for vi in G.nodes():\n",
    "        # Use a single BFS to get distances up to radius.\n",
    "        sp_lengths = nx.single_source_shortest_path_length(G, vi, cutoff=radius)\n",
    "        total = 0.0\n",
    "        for vj, d in sp_lengths.items():\n",
    "            if vi == vj or d == 0:\n",
    "                continue  # Skip self-loops or zero distance.\n",
    "            total += LC[vi] * LC[vj] / (d ** 2)\n",
    "        LGC[vi] = total\n",
    "    \n",
    "    return LGC\n",
    "\n",
    "# Method 12 (SEGM)\n",
    "def method12(G):\n",
    "    k = dict(G.degree())\n",
    "    I = dict()\n",
    "    E = dict()\n",
    "    SE = dict()\n",
    "\n",
    "    for node in G.nodes:\n",
    "        I[node] = 0\n",
    "\n",
    "    for i in G.nodes:\n",
    "        sum_kj = 0\n",
    "        for j in G.neighbors(i):\n",
    "            sum_kj += k[j]\n",
    "        if sum_kj != 0:\n",
    "            I[i] = k[i] / sum_kj\n",
    "        sum_e = 0\n",
    "        for j in G.neighbors(i):\n",
    "            if I[j] != 0:\n",
    "                sum_e += I[j] * np.log(I[j])\n",
    "        E[i] = -1 * sum_e\n",
    "        SE[i] = np.exp(E[i]) * k[i]\n",
    "\n",
    "    SEGM = dict()\n",
    "    R = 3\n",
    "    for i in G.nodes:\n",
    "        SEGM[i] = 0\n",
    "        neighbors = get_neighborhood(G, i, R)\n",
    "        for j in neighbors:\n",
    "            if nx.shortest_path_length(G, i, j) != 0:\n",
    "                SEGM[i] += (SE[i] * SE[j]) / (nx.shortest_path_length(G, i, j) ** 2)\n",
    "\n",
    "    return SEGM\n",
    "\n",
    "# Method 13 (MCGM)\n",
    "def method13(G):\n",
    "    # Compute degree and core number\n",
    "    k = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    \n",
    "    # Compute eigenvector centrality with increased iterations.\n",
    "    try:\n",
    "        x = nx.eigenvector_centrality(G, max_iter=500, tol=1e-06)\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        # Fallback to NumPy-based eigenvector centrality if convergence fails.\n",
    "        x = nx.eigenvector_centrality_numpy(G)\n",
    "    \n",
    "    # Convert centrality measures to arrays for median and max computations.\n",
    "    k_values = np.array(list(k.values()))\n",
    "    ks_values = np.array(list(ks.values()))\n",
    "    x_values = np.array(list(x.values()))\n",
    "    \n",
    "    # Compute medians and maximum values.\n",
    "    k_mid = np.median(k_values)\n",
    "    ks_mid = np.median(ks_values)\n",
    "    x_mid = np.median(x_values)\n",
    "    \n",
    "    k_max = np.max(k_values)\n",
    "    ks_max = np.max(ks_values)\n",
    "    x_max = np.max(x_values)\n",
    "    \n",
    "    # Calculate alpha, ensuring no division by zero.\n",
    "    if ks_mid == 0:\n",
    "        alpha = 0\n",
    "    else:\n",
    "        alpha = max(k_mid / k_max, x_mid / x_max) / (ks_mid / ks_max)\n",
    "    \n",
    "    MCGM = {}\n",
    "    R = 3  # Radius for considering node pairs.\n",
    "    \n",
    "    # Precompute shortest path lengths for all nodes with a cutoff of R.\n",
    "    sp_lengths = dict(nx.all_pairs_shortest_path_length(G, cutoff=R))\n",
    "    \n",
    "    for i in G.nodes():\n",
    "        MCGM[i] = 0\n",
    "        # Iterate over nodes reachable from i within cutoff R.\n",
    "        for j, d in sp_lengths.get(i, {}).items():\n",
    "            if i == j or d == 0:\n",
    "                continue\n",
    "            # Only consider nodes within radius R.\n",
    "            if d <= R:\n",
    "                # Compute contribution from node j.\n",
    "                term_i = (k[i] / k_max + alpha * ks[i] / ks_max + x[i] / x_max)\n",
    "                term_j = (k[j] / k_max + alpha * ks[j] / ks_max + x[j] / x_max)\n",
    "                MCGM[i] += term_i * term_j / (d ** 2)\n",
    "    \n",
    "    return MCGM\n",
    "\n",
    "# Method 14 (HVGC)\n",
    "def compute_H_index(G, i, degree_dict, neighbors):\n",
    "    # Use precomputed degrees for neighbors.\n",
    "    neighbor_degrees = [degree_dict[nb] for nb in neighbors[i]]\n",
    "    neighbor_degrees.sort(reverse=True)\n",
    "    H_index = 0\n",
    "    for idx, deg in enumerate(neighbor_degrees):\n",
    "        if deg >= idx + 1:\n",
    "            H_index = idx + 1\n",
    "        else:\n",
    "            break\n",
    "    return H_index\n",
    "\n",
    "def compute_H_v(G, i, degree_dict, neighbors):\n",
    "    H_i = compute_H_index(G, i, degree_dict, neighbors)\n",
    "    # Sum degrees of neighbors meeting the H_index threshold.\n",
    "    H_v_i = sum(degree_dict[j] for j in neighbors[i] if degree_dict[j] >= H_i)\n",
    "    return H_v_i\n",
    "\n",
    "def compute_c_i(i, neighbors, sum_weights, edge_weights):\n",
    "    # Compute c_i based on neighbors of node i.\n",
    "    c_i = 0\n",
    "    # Use the precomputed neighbor list for i.\n",
    "    neighbors_i = neighbors[i]\n",
    "    for j in neighbors_i:\n",
    "        # p_ij = weight(i,j) / (sum of weights from i)\n",
    "        p_ij = edge_weights.get((i, j), 1) / sum_weights[i] if sum_weights[i] != 0 else 0\n",
    "        # common neighbors between i and j\n",
    "        common_neighbors = set(neighbors_i).intersection(neighbors[j])\n",
    "        sum_piw_pwj = 0\n",
    "        for w in common_neighbors:\n",
    "            p_iw = edge_weights.get((i, w), 1) / sum_weights[i] if sum_weights[i] != 0 else 0\n",
    "            p_wj = edge_weights.get((w, j), 1) / sum_weights[w] if sum_weights[w] != 0 else 0\n",
    "            sum_piw_pwj += p_iw * p_wj\n",
    "        term = p_ij + sum_piw_pwj\n",
    "        c_i += term ** 2\n",
    "    return c_i\n",
    "\n",
    "def compute_HVGC(G, i, H_v, R, neighbors, sum_weights, edge_weights):\n",
    "    # Compute c_i using the optimized helper.\n",
    "    c_i = compute_c_i(i, neighbors, sum_weights, edge_weights)\n",
    "    HVGC_i = 0\n",
    "    # Use BFS (single_source_shortest_path_length) with cutoff R to consider only nearby nodes.\n",
    "    sp_lengths = nx.single_source_shortest_path_length(G, i, cutoff=R)\n",
    "    for j, d_ij in sp_lengths.items():\n",
    "        if j == i:\n",
    "            continue\n",
    "        # d_ij is guaranteed > 0; add contribution from node j.\n",
    "        HVGC_i += (math.exp(-c_i) * H_v[i] * H_v[j]) / (d_ij ** 2)\n",
    "    return HVGC_i\n",
    "\n",
    "def method14(G):\n",
    "    \"\"\"\n",
    "    Optimized HVGC computation.\n",
    "    \n",
    "    Precomputes node degrees, neighbors, edge weights, and sum of weights.\n",
    "    Then computes H_v for each node and finally HVGC using a BFS-based approach.\n",
    "    \"\"\"\n",
    "    # Precompute node degrees and neighbors\n",
    "    degree_dict = dict(G.degree())\n",
    "    neighbors = {i: list(G.neighbors(i)) for i in G.nodes()}\n",
    "    \n",
    "    # Precompute the sum of weights for each node (using weight attribute or defaulting to 1)\n",
    "    sum_weights = {}\n",
    "    for i in G.nodes():\n",
    "        sum_weights[i] = sum(G[i][w].get('weight', 1) for w in neighbors[i]) if neighbors[i] else 0\n",
    "    \n",
    "    # Precompute edge weights (store for both directions in the undirected graph)\n",
    "    edge_weights = {}\n",
    "    for i, j in G.edges():\n",
    "        w = G[i][j].get('weight', 1)\n",
    "        edge_weights[(i, j)] = w\n",
    "        edge_weights[(j, i)] = w\n",
    "\n",
    "    # Compute H_v for each node\n",
    "    H_v = {}\n",
    "    for i in G.nodes():\n",
    "        H_v[i] = compute_H_v(G, i, degree_dict, neighbors)\n",
    "\n",
    "    R = 2  # Neighborhood radius\n",
    "    HVGC = {}\n",
    "    for i in G.nodes():\n",
    "        HVGC[i] = compute_HVGC(G, i, H_v, R, neighbors, sum_weights, edge_weights)\n",
    "    \n",
    "    return HVGC\n",
    "\n",
    "\n",
    "\n",
    "# Method15 (BaseGM)\n",
    "def method15(G):\n",
    "    def get_neighborhood(G, node, radius):\n",
    "        neighborhood = nx.single_source_shortest_path_length(G, node, cutoff=radius)\n",
    "        neighborhood.pop(node, None)\n",
    "        return list(neighborhood.keys())\n",
    "\n",
    "    gravity = dict()\n",
    "    kshell = nx.core_number(G)\n",
    "    for vi in G.nodes():\n",
    "        gravity[vi] = 0\n",
    "        neighbors = get_neighborhood(G, vi, 3)\n",
    "        for vj in neighbors:\n",
    "            gravity[vi] += ((kshell[vi] * kshell[vj]) / nx.shortest_path_length(G, vi, vj) ** 2)\n",
    "\n",
    "    gravity_plus = dict()\n",
    "    for vi in G.nodes():\n",
    "        gravity_plus[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            gravity_plus[vi] += gravity[vj]\n",
    "\n",
    "    return gravity_plus\n",
    "\n",
    "# Method 16 (HKS)\n",
    "def method16(G):\n",
    "    def set_b_values(G):\n",
    "        b, Shell, b_values = 1, 1, {}\n",
    "        while G.number_of_nodes() > 0:\n",
    "            flag = False\n",
    "            for v in list(G.nodes):\n",
    "                if G.degree[v] <= Shell:\n",
    "                    b_values[v] = b\n",
    "                    flag = True\n",
    "            \n",
    "            if flag:\n",
    "                G.remove_nodes_from([v for v in list(G.nodes) if b_values.get(v) == b])\n",
    "                b += 1\n",
    "            else:\n",
    "                Shell += 1\n",
    "        \n",
    "        return b_values\n",
    "\n",
    "    def set_f(G, b_values):\n",
    "        V, f = list(G.nodes), max(b_values.values())\n",
    "        fi = {v: b_values[v] if all(b_values[v] >= b_values[vj] for vj in G.neighbors(v)) else 0 for v in V}\n",
    "\n",
    "        while V:\n",
    "            for vi in V:\n",
    "                if fi[vi] == f:\n",
    "                    for vj in G.neighbors(vi):\n",
    "                        if fi[vi] - 1 > fi[vj]:\n",
    "                            fi[vj] = fi[vi] - 1\n",
    "            V.remove(vi)\n",
    "\n",
    "        return fi\n",
    "\n",
    "    b_values = set_b_values(G.copy())\n",
    "    fi_values = set_f(G.copy(), b_values)\n",
    "\n",
    "    S = {vi: sum(G.degree[vj] * (b_values[vj] + fi_values[vj]) for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    HKS = {vi: sum(S[vj] for vj in G.neighbors(vi)) for vi in G.nodes()}\n",
    "    return HKS\n",
    "\n",
    "# Method 17 (SHKS)\n",
    "def method17(G):\n",
    "    # Precompute neighbor sets and node degrees.\n",
    "    neighbors = {node: set(G.neighbors(node)) for node in G.nodes()}\n",
    "    degrees = dict(G.degree())\n",
    "    \n",
    "    # Precompute pvu values for all nodes: 1/degree if degree > 0 else 0.\n",
    "    pvu_value = {node: 1/deg if deg > 0 else 0 for node, deg in degrees.items()}\n",
    "    \n",
    "    # Compute the network-constrained coefficient for node v.\n",
    "    def network_constrained_coefficient(v):\n",
    "        coeff = 0.0\n",
    "        pvu_v = pvu_value[v]\n",
    "        for u in neighbors[v]:\n",
    "            # Outer term is simply 1/deg(v)\n",
    "            outer_term = pvu_v\n",
    "            # For common neighbors between v and u, sum (1/deg(v)^2 * (1/deg(w))^2)\n",
    "            common = neighbors[v].intersection(neighbors[u])\n",
    "            inner_sum = (pvu_v ** 2) * sum(pvu_value[w] ** 2 for w in common)\n",
    "            coeff += outer_term + inner_sum\n",
    "        return coeff\n",
    "\n",
    "    # Compute constraint coefficient for each node.\n",
    "    Constraint_Coef = {node: network_constrained_coefficient(node) for node in G.nodes()}\n",
    "    # Compute sh values safely.\n",
    "    sh = {node: 1 / Constraint_Coef[node] if Constraint_Coef[node] != 0 else 0 for node in G.nodes()}\n",
    "    \n",
    "    # Compute core numbers.\n",
    "    ks = nx.core_number(G)\n",
    "    alpha = 0.2\n",
    "    # Compute influence I for each node.\n",
    "    I = {node: alpha * sh[node] + ks[node] for node in G.nodes()}\n",
    "    # Compute C[v] = I[v] plus the sum of I over its neighbors.\n",
    "    C = {v: I[v] + sum(I[u] for u in neighbors[v]) for v in G.nodes()}\n",
    "    # Compute IS[v] as the sum of C over neighbors of v.\n",
    "    IS = {v: sum(C[u] for u in neighbors[v]) for v in G.nodes()}\n",
    "    # Finally, compute SHKS[v] as the sum of IS over neighbors of v.\n",
    "    SHKS = {v: sum(IS[u] for u in neighbors[v]) for v in G.nodes()}\n",
    "    \n",
    "    return SHKS\n",
    "\n",
    "\n",
    "\n",
    "# method18: KSGC\n",
    "import numpy as np\n",
    "from heapq import heappop, heappush\n",
    "\n",
    "def method18(G):\n",
    "    # Precompute core numbers and degrees\n",
    "    ks = nx.core_number(G)\n",
    "    k = dict(G.degree())\n",
    "\n",
    "    ks_max, ks_min = max(ks.values()), min(ks.values())\n",
    "    denom = ks_max - ks_min if ks_max != ks_min else 1  # Avoid division by zero\n",
    "    \n",
    "    nodes = list(G.nodes())\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    ks_array = np.array([ks[node] for node in nodes], dtype=np.float64)\n",
    "    k_array = np.array([k[node] for node in nodes], dtype=np.float64)\n",
    "    \n",
    "    n = len(nodes)\n",
    "    \n",
    "    # Compute shortest path lengths using Dijkstra (faster than Floyd-Warshall)\n",
    "    shortest_paths = {v: nx.single_source_dijkstra_path_length(G, v) for v in G.nodes()}\n",
    "\n",
    "    # Compute the neighborhood radius (half of the graph's diameter)\n",
    "    diameter = nx.diameter(G)\n",
    "    radius = int(0.5 * diameter)\n",
    "    \n",
    "    # Compute KSGC\n",
    "    KSGC = {v: 0.0 for v in G.nodes()}\n",
    "    \n",
    "    for v in G.nodes():\n",
    "        index_v = node_index[v]\n",
    "        ks_v = ks_array[index_v]\n",
    "        k_v = k_array[index_v]\n",
    "        \n",
    "        sum_F = 0.0\n",
    "        for u, d in shortest_paths[v].items():\n",
    "            if u == v or d > radius:\n",
    "                continue  # Skip self-loops and nodes outside the radius\n",
    "            \n",
    "            index_u = node_index[u]\n",
    "            ks_u = ks_array[index_u]\n",
    "            k_u = k_array[index_u]\n",
    "            \n",
    "            # Compute C value\n",
    "            C_val = np.exp((ks_v - ks_u) / denom)\n",
    "            \n",
    "            # Compute F value\n",
    "            F_val = C_val * (k_v * k_u) / (d ** 2)\n",
    "            \n",
    "            sum_F += F_val\n",
    "        \n",
    "        KSGC[v] = sum_F\n",
    "\n",
    "    return KSGC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def k_neighbors(G, node, radius):\n",
    "        neighbors = set(nx.single_source_shortest_path_length(G, node, cutoff=radius).keys())\n",
    "        neighbors.discard(node)  # Remove the node itself if present\n",
    "        return neighbors\n",
    "    \n",
    "# Method 19 (InformationRank)\n",
    "def method19(G):\n",
    "    \n",
    "    PROPA = {}\n",
    "    Score = {}\n",
    "    L = 2\n",
    "    Miu = 0.2\n",
    "\n",
    "    # Initialize PROPA and Score dictionaries\n",
    "    for v in G.nodes():\n",
    "        PROPA[v] = {}\n",
    "        Score[v] = 0\n",
    "\n",
    "    # Calculate PROPA and Score\n",
    "    for v in G.nodes():\n",
    "        neighbors = k_neighbors(G, v, L)\n",
    "        for w in neighbors:\n",
    "            PROPA[v][w] = 1\n",
    "            path_length_counts = {}\n",
    "            for path in nx.all_simple_paths(G, v, w, cutoff=L):\n",
    "                length = len(path) - 1\n",
    "                path_length_counts[length] = path_length_counts.get(length, 0) + 1\n",
    "\n",
    "            for l in range(1, L + 1):\n",
    "                pow_val = path_length_counts.get(l, 0)\n",
    "                PROPA[v][w] *= (1 - Miu ** l) ** pow_val\n",
    "            PROPA[v][w] = 1 - PROPA[v][w]\n",
    "            Score[v] += PROPA[v][w]\n",
    "\n",
    "    return Score\n",
    "\n",
    "# method 20 (IS-PEW)\n",
    "def method20(G):\n",
    "    # Precompute expensive metrics\n",
    "    triangles = nx.triangles(G)  # Dictionary {node: triangle_count}\n",
    "    core_numbers = nx.core_number(G)  # Dictionary {node: core_number}\n",
    "    degrees = dict(G.degree())  # Dictionary {node: degree}\n",
    "    \n",
    "    # Compute total triangles once\n",
    "    TC = sum(triangles.values()) // 3 if sum(triangles.values()) > 0 else 1  # Avoid division by zero\n",
    "    \n",
    "    def compute_TP(v_A):\n",
    "        return triangles[v_A] / TC\n",
    "    \n",
    "    def compute_InfE(v_A, v_B):\n",
    "        common_neighbors = set(G[v_A]) & set(G[v_B])\n",
    "        sum_DG_k = sum(degrees[k] for k in common_neighbors)\n",
    "        return (degrees[v_A] * degrees[v_B]) / (1 + sum_DG_k)\n",
    "\n",
    "    def compute_NIP(v_A):\n",
    "        return sum(np.sqrt(degrees[v_a] * core_numbers[v_a]) for v_a in G[v_A])\n",
    "\n",
    "    # Store edge weights in a dictionary to avoid recomputation\n",
    "    edge_weights = {}\n",
    "\n",
    "    def compute_EW(v_A, v_B):\n",
    "        if (v_A, v_B) in edge_weights:\n",
    "            return edge_weights[(v_A, v_B)]\n",
    "        \n",
    "        TP_v_A, TP_v_B = compute_TP(v_A), compute_TP(v_B)\n",
    "        KS_v_A, KS_v_B = core_numbers[v_A], core_numbers[v_B]\n",
    "        NIP_v_A, NIP_v_B = compute_NIP(v_A), compute_NIP(v_B)\n",
    "        ED_v_A_v_B = 1 / compute_InfE(v_A, v_B)\n",
    "        \n",
    "        EW_value = ((KS_v_A * (1 + TP_v_A) * NIP_v_A) / ED_v_A_v_B) + ((KS_v_B * (1 + TP_v_B) * NIP_v_B) / ED_v_A_v_B)\n",
    "        edge_weights[(v_A, v_B)] = EW_value  # Store result for future use\n",
    "        edge_weights[(v_B, v_A)] = EW_value  # Store symmetric value\n",
    "        return EW_value\n",
    "\n",
    "    def compute_IS(v_A):\n",
    "        return sum(compute_EW(v_A, v_B) for v_B in G[v_A])\n",
    "\n",
    "    # Compute potential edge weights once\n",
    "    for u, v in G.edges():\n",
    "        G[u][v]['weight'] = compute_EW(u, v)\n",
    "\n",
    "    # Compute IS values for all nodes\n",
    "    IS_values = {v: compute_IS(v) for v in G.nodes()}\n",
    "    \n",
    "    return IS_values\n",
    "\n",
    "# Method 21 (DKGM)\n",
    "\n",
    "def method21(G):\n",
    "    # Step 1: Efficient k-shell decomposition using batch removal\n",
    "    G_copy = G.copy()\n",
    "    k_shell = {}\n",
    "    k_star = {}\n",
    "    k = 1\n",
    "    iteration = 1\n",
    "\n",
    "    while G_copy.number_of_nodes() > 0:\n",
    "        # Identify nodes to remove at current k-level\n",
    "        nodes_to_remove = {node for node in G_copy.nodes if G_copy.degree[node] <= k}\n",
    "        iteration_nodes = []\n",
    "        \n",
    "        while nodes_to_remove:\n",
    "            iteration_nodes.extend([(node, iteration) for node in nodes_to_remove])\n",
    "            G_copy.remove_nodes_from(nodes_to_remove)\n",
    "            iteration += 1\n",
    "            nodes_to_remove = {node for node in G_copy.nodes if G_copy.degree[node] <= k}\n",
    "\n",
    "        # Assign k-shell and k-star values\n",
    "        if iteration_nodes:\n",
    "            m = iteration_nodes[-1][1]  # Last iteration number\n",
    "            for node, n in iteration_nodes:\n",
    "                k_shell[node] = k\n",
    "                k_star[node] = k_shell[node] + (n / (m + 1))\n",
    "\n",
    "        k += 1\n",
    "        iteration = 1  # Reset for next k-shell\n",
    "\n",
    "    # Step 2: Compute DK values\n",
    "    DK = {node: G.degree[node] + k_star.get(node, 0) for node in G.nodes()}\n",
    "\n",
    "    # Step 3: Precompute shortest path lengths (BFS for unweighted, Dijkstra for weighted)\n",
    "    R = 2  # Neighborhood radius\n",
    "    DKGM = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    for node in G.nodes():\n",
    "        # Get shortest path lengths up to radius R using BFS\n",
    "        sp_lengths = nx.single_source_shortest_path_length(G, node, cutoff=R)\n",
    "\n",
    "        for neighbor, d_ij in sp_lengths.items():\n",
    "            if node != neighbor and d_ij > 0:  # Avoid self-loops and division by zero\n",
    "                DKGM[node] += DK[node] * DK.get(neighbor, 0) / (d_ij ** 2)\n",
    "\n",
    "    return DKGM\n",
    "\n",
    "\n",
    "\n",
    "#method 22: modularity_vitality (MV)\n",
    "def method22(G):\n",
    "    \"\"\"Optimized version of method23 using Leiden algorithm for community detection.\"\"\"\n",
    "    \n",
    "    def detect_communities_leiden(G):\n",
    "        \"\"\"Detect communities using Leiden algorithm efficiently.\"\"\"\n",
    "        # Convert NetworkX graph to iGraph.\n",
    "        # Use to_numpy_array instead of the deprecated to_numpy_matrix.\n",
    "        ig_G = ig.Graph.Adjacency((nx.to_numpy_array(G) > 0).tolist(), mode=\"undirected\")\n",
    "        ig_G.vs[\"name\"] = list(G.nodes())\n",
    "        partition = la.find_partition(ig_G, la.ModularityVertexPartition)\n",
    "        return {ig_G.vs[i][\"name\"]: idx for idx, comm in enumerate(partition) for i in comm}\n",
    "\n",
    "    communities = detect_communities_leiden(G)\n",
    "\n",
    "    # Precompute degrees, edges per community, and modularity.\n",
    "    M = G.number_of_edges()\n",
    "    degree_cache = dict(G.degree())\n",
    "    d_c = {c: sum(degree_cache[n] for n in G.nodes() if communities[n] == c)\n",
    "           for c in set(communities.values())}\n",
    "\n",
    "    def modularity_vitality(G, node, communities):\n",
    "        \"\"\"Compute the modularity vitality of a node using optimized calculations.\"\"\"\n",
    "        if not G.has_node(node):\n",
    "            return 0\n",
    "\n",
    "        c_i = communities[node]\n",
    "        k_i = degree_cache[node]\n",
    "\n",
    "        # Compute h_i,c values.\n",
    "        hi_c = {c: sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == c)\n",
    "                for c in set(communities.values())}\n",
    "        hi_c[c_i] += k_i  # Add k_i when node belongs to c_i.\n",
    "\n",
    "        # Compute initial modularity.\n",
    "        community_nodes = [n for n in G.nodes() if communities[n] == c_i]\n",
    "        M_internal = sum(1 for n in community_nodes for neighbor in G.neighbors(n)\n",
    "                         if communities[neighbor] == c_i) // 2\n",
    "        k_i_internal = hi_c[c_i] - k_i  # Internal edges contribution.\n",
    "\n",
    "        sum_term = sum((d_c[c] - hi_c[c]) ** 2 for c in hi_c)\n",
    "        Q_updated = (M_internal - k_i_internal) / (M - k_i) - (1 / (4 * (M - k_i) ** 2)) * sum_term\n",
    "\n",
    "        return Q_updated\n",
    "\n",
    "    return {node: modularity_vitality(G, node, communities) for node in G.nodes()}\n",
    "\n",
    "\n",
    "#method 23: Global-and-Local Centrality (GLC)\n",
    "def method23(G, lam=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Global-and-Local Centrality (GLC) of each node in graph G.\n",
    "    Optimized version with improved time complexity.\n",
    "    \n",
    "    Parameters:\n",
    "        G   : networkx.Graph\n",
    "              The input graph.\n",
    "        lam : float (default=1.0)\n",
    "              The fraction (lambda) of nodes that must be covered by clusters.\n",
    "    \n",
    "    Returns:\n",
    "        GLC : dict\n",
    "              A dictionary mapping each node in G to its GLC centrality value.\n",
    "    \"\"\"\n",
    "    # Pre-compute and cache node degrees and neighbor sets\n",
    "    degrees = dict(G.degree())\n",
    "    neighbors = {node: set(G.neighbors(node)) for node in G.nodes()}\n",
    "    total_nodes = G.number_of_nodes()\n",
    "    \n",
    "    # --- Step 1: Clustering and Global Critical Node Selection ---\n",
    "    clusters = []\n",
    "    assigned = set()\n",
    "    \n",
    "    # Helper function: Compute the potential of a node more efficiently\n",
    "    def compute_potential(node):\n",
    "        nbrs = neighbors[node]\n",
    "        target_set = nbrs.union({node})\n",
    "        s = 0\n",
    "        for j in nbrs:\n",
    "            # Use set intersection for faster computation\n",
    "            s += len(neighbors[j].intersection(target_set))\n",
    "        return degrees[node] * s\n",
    "\n",
    "    # Pre-compute potentials for all nodes\n",
    "    potentials = {node: compute_potential(node) for node in G.nodes()}\n",
    "    \n",
    "    while len(assigned) < total_nodes * lam:\n",
    "        # Filter out assigned nodes from potentials\n",
    "        available_potentials = {node: pot for node, pot in potentials.items() if node not in assigned}\n",
    "        if not available_potentials:\n",
    "            break\n",
    "        \n",
    "        # Select seed node with maximum potential\n",
    "        seed = max(available_potentials, key=available_potentials.get)\n",
    "        pcmax = available_potentials[seed]\n",
    "        new_cluster = set([seed])\n",
    "        assigned.add(seed)\n",
    "        \n",
    "        # Include neighbors of the seed with potential >= pcmax/2\n",
    "        for neighbor in neighbors[seed]:\n",
    "            if neighbor not in assigned and potentials[neighbor] >= pcmax / 2:\n",
    "                new_cluster.add(neighbor)\n",
    "                assigned.add(neighbor)\n",
    "        \n",
    "        # Expand the cluster iteratively (three degrees of influence)\n",
    "        for _ in range(3):\n",
    "            # More efficiently gather neighbors of the current cluster\n",
    "            cluster_neighbors = set()\n",
    "            for node in new_cluster:\n",
    "                cluster_neighbors.update(neighbors[node] - new_cluster)\n",
    "            \n",
    "            # Pre-compute kin and kout for all candidates\n",
    "            neighbor_metrics = {}\n",
    "            for candidate in cluster_neighbors:\n",
    "                if candidate in assigned:\n",
    "                    continue\n",
    "                # Use set operations for more efficient counting\n",
    "                kin = len(neighbors[candidate].intersection(new_cluster))\n",
    "                kout = degrees[candidate] - kin\n",
    "                if kin >= kout:\n",
    "                    neighbor_metrics[candidate] = (kin, degrees[candidate])\n",
    "            \n",
    "            # Sort by degree to process in order of increasing degree\n",
    "            candidates = sorted(neighbor_metrics.keys(), key=lambda x: degrees[x])\n",
    "            \n",
    "            if not candidates:\n",
    "                break\n",
    "                \n",
    "            # Add nodes that meet the criteria\n",
    "            for candidate in candidates:\n",
    "                new_cluster.add(candidate)\n",
    "                assigned.add(candidate)\n",
    "            \n",
    "            if not candidates:  # No more nodes added\n",
    "                break\n",
    "        \n",
    "        # Mark all neighbors of the new cluster as assigned\n",
    "        for node in new_cluster:\n",
    "            assigned.update(neighbors[node])\n",
    "        \n",
    "        clusters.append(new_cluster)\n",
    "    \n",
    "    # From each cluster, select the global critical node\n",
    "    global_critical = [max(cluster, key=lambda node: degrees[node]) for cluster in clusters if cluster]\n",
    "    \n",
    "    # --- Step 2: Local Influence Calculation ---\n",
    "    # Compute the k-shell (core) numbers in a single call\n",
    "    core_numbers = nx.core_number(G)\n",
    "    \n",
    "    # More efficiently compute local influence\n",
    "    LI = {}\n",
    "    for node in G.nodes():\n",
    "        LI[node] = sum(core_numbers[nbr] for nbr in neighbors[node])\n",
    "\n",
    "    # --- Step 3: Overall GLC Centrality Calculation ---\n",
    "    # Pre-compute all shortest paths for efficiency if graph is small enough\n",
    "    # For large graphs, compute paths on-demand\n",
    "    use_precomputed_paths = total_nodes <= 1000  # Threshold based on graph size\n",
    "    \n",
    "    if use_precomputed_paths:\n",
    "        # Pre-compute shortest paths from all nodes to global critical nodes\n",
    "        shortest_paths = {}\n",
    "        for u in global_critical:\n",
    "            paths = nx.single_source_shortest_path_length(G, u)\n",
    "            for v, length in paths.items():\n",
    "                shortest_paths[(v, u)] = length\n",
    "    \n",
    "    GLC = {}\n",
    "    for v in G.nodes():\n",
    "        global_factor = 0\n",
    "        for u in global_critical:\n",
    "            if use_precomputed_paths:\n",
    "                try:\n",
    "                    d = shortest_paths.get((v, u), float('inf'))\n",
    "                except:\n",
    "                    d = float('inf')\n",
    "            else:\n",
    "                try:\n",
    "                    d = nx.shortest_path_length(G, source=v, target=u)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    d = float('inf')\n",
    "            d = max(d, 1)  # Avoid division by zero\n",
    "            global_factor += LI[u] / (2 * d)\n",
    "        GLC[v] = LI[v] * global_factor\n",
    "\n",
    "    return GLC\n",
    "\n",
    "# Method 24 (Weighted K-shell)\n",
    "def method24(G):\n",
    "    c1 = 0.1\n",
    "    c2 = 0.4\n",
    "    deg = dict(G.degree())\n",
    "    ks = nx.core_number(G)\n",
    "    ksdw = dict()\n",
    "\n",
    "    for vi in G.nodes():\n",
    "        ksdw[vi] = 0\n",
    "        for vj in G.neighbors(vi):\n",
    "            ksdw[vi] += (c1 * deg[vi] + c2 * ks[vi]) * (c1 * deg[vj] + c2 * ks[vj])\n",
    "\n",
    "    return ksdw\n",
    "\n",
    "# method 25:EFFD centrality\n",
    "def method25(G):\n",
    "    \"\"\"\n",
    "    Optimized version of the EffG centrality calculation.\n",
    "    \n",
    "    Parameters:\n",
    "        G : networkx.Graph\n",
    "            The input graph.\n",
    "            \n",
    "    Returns:\n",
    "        effg_centrality : dict\n",
    "            A dictionary mapping each node to its EffG centrality value.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    \n",
    "    # Cache node degrees to avoid repeated lookups\n",
    "    degree = dict(G.degree())\n",
    "    nodes = list(G.nodes())\n",
    "    n = len(nodes)\n",
    "    \n",
    "    # Initialize the result dictionary\n",
    "    effg_centrality = {node: 0 for node in nodes}\n",
    "    \n",
    "    # Compute all shortest paths once using Dijkstra\n",
    "    # This is more efficient than calling nx.all_pairs_dijkstra_path_length()\n",
    "    # as it uses a more optimized implementation\n",
    "    shortest_paths = dict(nx.all_pairs_dijkstra_path(G, weight=None))\n",
    "    \n",
    "    # Process each node pair once\n",
    "    for i in range(n):\n",
    "        node_i = nodes[i]\n",
    "        k_i = degree[node_i]\n",
    "        \n",
    "        # Get all paths from node_i\n",
    "        paths_from_i = shortest_paths[node_i]\n",
    "        \n",
    "        for j in range(i + 1, n):  # Only process each pair once (i,j) where i<j\n",
    "            node_j = nodes[j]\n",
    "            k_j = degree[node_j]\n",
    "            \n",
    "            # Get the path between i and j\n",
    "            if node_j in paths_from_i:\n",
    "                path = paths_from_i[node_j]\n",
    "                path_length = len(path) - 1  # Number of edges in the path\n",
    "                \n",
    "                # Calculate effective distance\n",
    "                if G.has_edge(node_i, node_j):\n",
    "                    # Directly connected\n",
    "                    p = 1 / degree[node_i]\n",
    "                    d = 1 - np.log2(p) if p > 0 else float('inf')\n",
    "                else:\n",
    "                    # Indirectly connected\n",
    "                    d = path_length\n",
    "                \n",
    "                # Calculate interaction score\n",
    "                if d != float('inf'):\n",
    "                    score = (k_i * k_j) / (d ** 2)\n",
    "                    \n",
    "                    # Update centrality values for both nodes\n",
    "                    effg_centrality[node_i] += score\n",
    "                    effg_centrality[node_j] += score\n",
    "    \n",
    "    return effg_centrality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#methods = {\n",
    "#    \"Degree Centrality\": method1,\n",
    "#    \"Betweenness Centrality\": method2,\n",
    "#    \"Closeness Centrality\": method3,\n",
    "#    \"K-shell\": method4,\n",
    "#    \"Cnc Plus\": method5,\n",
    "#    \"Local H-index\": method6,\n",
    "#    \"DNC\": method7,\n",
    "#    \"ECLGC\": method8,\n",
    "#    \"Centripetal Centrality\": method9,\n",
    "#    \"EDBC\" : method10,\n",
    "#    \"LGC\" : method11,\n",
    "#    \"SEGM\" : method12,\n",
    "#    \"MCGM\" : method13,\n",
    "#    \"HVGC\" : method14,\n",
    "#    \"BaseGM\" : method15,\n",
    "#    \"HKS\" : method16,\n",
    "#    \"SHKS\" : method17,\n",
    "#    \"KSGC\" : method18,\n",
    "#    \"Local Relative ASP\" : method19,\n",
    "#    \"Information Rank\" : method20,\n",
    "#    \"IS-PEW\" : method21,\n",
    "#    \"DKGM\" : method22,\n",
    "#    \"modularity vitality\":method23,\n",
    "#    \"Global-and-Local Centrality (GLC)\":method24,\n",
    "#    \"Weighted K-shell\" : method25,\n",
    "#    \"EFFD gravity\" : method26,\n",
    "# }\n",
    "\n",
    "# List of methods to be applied\n",
    "methods = [globals()[f'method{i}'] for i in range(1, 26)]\n",
    "\n",
    "dataset_folder = 'dataset'\n",
    "beta_values = [0.02, 0.05, 0.08, 0.10, 0.13, 0.15, 0.18, 0.20, 0.22, 0.25, 0.30]\n",
    "k_factors = [0.01, 0.03, 0.05, 0.07, 0.085, 0.10]\n",
    "f_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "for dataset_filename in os.listdir(dataset_folder):\n",
    "    if dataset_filename.endswith('.xlsx'):\n",
    "        dataset_path = os.path.join(dataset_folder, dataset_filename)\n",
    "        dataset_name = os.path.splitext(dataset_filename)[0]\n",
    "        print(f\"\\nProcessing dataset: {dataset_name}...\\n\")\n",
    "        \n",
    "        df_edges = pd.read_excel(dataset_path, sheet_name='Sheet1')\n",
    "        G = nx.from_pandas_edgelist(df_edges, source='source_column', target='target_column')\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "        \n",
    "        df_sir = pd.read_excel(dataset_path, sheet_name='SIR')\n",
    "        df_sir.set_index('Node', inplace=True)\n",
    "        \n",
    "        num_nodes = len(G.nodes())\n",
    "        results = {}\n",
    "        method_results = {}\n",
    "\n",
    "        for i, method in enumerate(methods, start=1):\n",
    "            method_name = f'method{i}'\n",
    "            print(f\"Applying {method_name} on {dataset_name}...\")\n",
    "            try:\n",
    "                ranked_nodes, exec_time = measure_execution_time(method, G)\n",
    "                monotonicity = compute_monotonicity(G, ranked_nodes)\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': ranked_nodes,\n",
    "                    'execution_time': exec_time,\n",
    "                    'monotonicity': monotonicity\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {method_name} for {dataset_name}: {e}\")\n",
    "                method_results[method_name] = {\n",
    "                    'ranked_nodes': [],\n",
    "                    'execution_time': None,\n",
    "                    'monotonicity': None,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "\n",
    "        for beta in beta_values:\n",
    "            beta_column = f'Beta_{beta:.2f}'\n",
    "            if beta_column not in df_sir.columns:\n",
    "                print(f\"Warning: {beta_column} not found in {dataset_name}. Skipping beta {beta}...\")\n",
    "                continue\n",
    "            \n",
    "            spread_power = df_sir[beta_column].to_dict()\n",
    "            sigma = sorted(spread_power.keys(), key=lambda x: spread_power[x], reverse=True)\n",
    "            \n",
    "            for method_name, method_data in method_results.items():\n",
    "                if 'error' in method_data:\n",
    "                    results[(method_name, beta)] = {'Beta': beta, 'Kendall Tau': None, 'P-Value': None}\n",
    "                    continue\n",
    "                \n",
    "                ranked_nodes = method_data['ranked_nodes']\n",
    "                tau, p_value = kendalltau(\n",
    "                    [spread_power[node] for node in sigma],\n",
    "                    [spread_power[node] for node, _ in ranked_nodes]\n",
    "                )\n",
    "                \n",
    "                R = [node for node, _ in ranked_nodes]\n",
    "                jaccard_scores = {}\n",
    "                si_scores = {}\n",
    "                rbo_scores = {}\n",
    "                mrr_scores = compute_mrr(sigma, R, k_factors)\n",
    "                \n",
    "                for k_factor in k_factors:\n",
    "                    k = max(1, int(k_factor * num_nodes))\n",
    "                    top_k_sigma = set(sigma[:k])\n",
    "                    top_k_R = set(R[:k])\n",
    "                    jaccard_scores[f'Jaccard k={k}'] = compute_jaccard_similarity(top_k_sigma, top_k_R)\n",
    "                \n",
    "                for f in f_values:\n",
    "                    f_count = max(1, int(f * num_nodes))\n",
    "                    top_f_nodes = [node for node, _ in ranked_nodes][:f_count]\n",
    "                    si_scores[f'SI {f:.2f}'] = sum(spread_power[node] for node in top_f_nodes) / (f * num_nodes)\n",
    "                    rbo_scores[f'RBO {f:.2f}'] = compute_rbo(sigma[:f_count], R[:f_count])\n",
    "                \n",
    "                results[(method_name, beta)] = {\n",
    "                    'Beta': beta,\n",
    "                    'Kendall Tau': tau,\n",
    "                    'P-Value': p_value,\n",
    "                    **jaccard_scores,\n",
    "                    **si_scores,\n",
    "                    **rbo_scores,\n",
    "                    **mrr_scores\n",
    "                }\n",
    "        \n",
    "        data = []\n",
    "        for (method_name, beta), result in results.items():\n",
    "            row = [\n",
    "                dataset_name,  # Include dataset name in output\n",
    "                method_name,\n",
    "                method_results[method_name]['execution_time'],\n",
    "                method_results[method_name].get('monotonicity', 'N/A'),\n",
    "                result.get('Beta'),\n",
    "                result.get('Kendall Tau'),\n",
    "                result.get('P-Value')\n",
    "            ] + [result.get(f'Jaccard k={int(k_factor * num_nodes)}') for k_factor in k_factors] + \\\n",
    "              [result.get(f'SI {f:.2f}') for f in f_values] + \\\n",
    "              [result.get(f'RBO {f:.2f}') for f in f_values] + \\\n",
    "              [result.get(f'MRR {k_factor:.2f}') for k_factor in k_factors]\n",
    "            \n",
    "            data.append(row)\n",
    "        \n",
    "        expected_columns = ['Dataset', 'Method', 'Execution Time', 'Monotonicity', 'Beta', 'Kendall Tau', 'P-Value'] + \\\n",
    "                           [f'Jaccard k={int(k_factor * num_nodes)}' for k_factor in k_factors] + \\\n",
    "                           [f'SI {f:.2f}' for f in f_values] + \\\n",
    "                           [f'RBO {f:.2f}' for f in f_values] + \\\n",
    "                           [f'MRR {k_factor:.2f}' for k_factor in k_factors]\n",
    "        \n",
    "        result_df = pd.DataFrame(data, columns=expected_columns)\n",
    "        \n",
    "        output_file = f'{dataset_name}_results.xlsx'\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            df_edges.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "            df_sir.to_excel(writer, sheet_name='SIR')\n",
    "            result_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        \n",
    "        print(f\"✅ Completed processing: {dataset_name}. Results saved in {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ac2f6-f115-4279-b538-48119179c75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
